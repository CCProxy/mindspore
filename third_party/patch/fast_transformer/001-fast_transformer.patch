diff --git a/.github/ISSUE_TEMPLATE/bug_report.yml b/.github/ISSUE_TEMPLATE/bug_report.yml
new file mode 100644
index 0000000..18054db
--- /dev/null
+++ b/.github/ISSUE_TEMPLATE/bug_report.yml
@@ -0,0 +1,32 @@
+name: "Bug Report"
+description: Submit a bug report
+labels: [ "bug" ]
+body:
+  - type: textarea
+    id: description
+    attributes:
+      label: Description
+      description: Please share your system info with us.
+      render: shell
+      placeholder: branch, docker version, GPU type
+    validations:
+      required: true
+      
+  - type: textarea
+    id: reproduced-steps
+    attributes:
+      label: Reproduced Steps
+      description: Please provide the step to reproduce the bugs
+      render: shell
+      placeholder: |
+        Steps to reproduce your bugs:
+        
+        1. docker run -ti --gpus all nvcr.io/nvidia/pytorch:22.03-py3 bash
+        2. git clone https://github.com/NVIDIA/FasterTransformer.git
+        3. cd FasterTransformer mkdir build && cd build
+        4. cmake -DSM=80 -DCMAKE_BUILD_TYPE=Release .. && make -j12
+        5. ./bin/bert_example 32 12 32 12 64 0 0
+        6. What error you see.
+  
+    validations:
+      required: true
diff --git a/.vscode/settings.json b/.vscode/settings.json
deleted file mode 100644
index 6f535da..0000000
--- a/.vscode/settings.json
+++ /dev/null
@@ -1,72 +0,0 @@
-{
-    "files.associations": {
-        "*.cuh": "cpp",
-        "stdexcept": "cpp",
-        "chrono": "cpp",
-        "cmath": "cpp",
-        "type_traits": "cpp",
-        "cctype": "cpp",
-        "clocale": "cpp",
-        "cstdarg": "cpp",
-        "cstddef": "cpp",
-        "cstdio": "cpp",
-        "cstdlib": "cpp",
-        "cstring": "cpp",
-        "ctime": "cpp",
-        "cwchar": "cpp",
-        "cwctype": "cpp",
-        "array": "cpp",
-        "atomic": "cpp",
-        "*.tcc": "cpp",
-        "condition_variable": "cpp",
-        "cstdint": "cpp",
-        "deque": "cpp",
-        "unordered_map": "cpp",
-        "vector": "cpp",
-        "exception": "cpp",
-        "algorithm": "cpp",
-        "functional": "cpp",
-        "iterator": "cpp",
-        "map": "cpp",
-        "memory": "cpp",
-        "memory_resource": "cpp",
-        "numeric": "cpp",
-        "optional": "cpp",
-        "random": "cpp",
-        "ratio": "cpp",
-        "set": "cpp",
-        "string": "cpp",
-        "string_view": "cpp",
-        "system_error": "cpp",
-        "tuple": "cpp",
-        "utility": "cpp",
-        "fstream": "cpp",
-        "initializer_list": "cpp",
-        "iomanip": "cpp",
-        "iosfwd": "cpp",
-        "iostream": "cpp",
-        "istream": "cpp",
-        "limits": "cpp",
-        "mutex": "cpp",
-        "new": "cpp",
-        "ostream": "cpp",
-        "sstream": "cpp",
-        "streambuf": "cpp",
-        "thread": "cpp",
-        "cinttypes": "cpp",
-        "typeinfo": "cpp",
-        "bitset": "cpp",
-        "hash_map": "cpp",
-        "hash_set": "cpp",
-        "slist": "cpp",
-        "regex": "cpp",
-        "strstream": "cpp",
-        "complex": "cpp",
-        "forward_list": "cpp",
-        "list": "cpp",
-        "unordered_set": "cpp",
-        "future": "cpp",
-        "cfenv": "cpp",
-        "typeindex": "cpp"
-    }
-}
\ No newline at end of file
diff --git a/3rdparty/trt_fused_multihead_attention/CMakeLists.txt b/3rdparty/trt_fused_multihead_attention/CMakeLists.txt
index 8707220..c9369e0 100644
--- a/3rdparty/trt_fused_multihead_attention/CMakeLists.txt
+++ b/3rdparty/trt_fused_multihead_attention/CMakeLists.txt
@@ -21,7 +21,10 @@ set(trt_fused_multi_head_attention_files
 )
 
 file(GLOB trt_fused_multi_head_attention_files ${trt_fused_multi_head_attention_files} *.sm*.cpp)
-
+if(${CUDA_VERSION_STRING}  VERSION_LESS_EQUAL "10.1.105" )
+#this cuda don't support sm80
+    list(REMOVE_ITEM  trt_fused_multi_head_attention_files fused_mha_with_relPosBias_fp16_64_32_kernel.sm80.cpp)
+endif()
 add_library(trt_fused_multi_head_attention STATIC ${trt_fused_multi_head_attention_files})
 target_link_libraries(trt_fused_multi_head_attention PUBLIC -lcublas -lcudart)
 set_property(TARGET trt_fused_multi_head_attention PROPERTY POSITION_INDEPENDENT_CODE  ON)
diff --git a/CMakeLists.txt b/CMakeLists.txt
index ea21014..f9e08b8 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -14,7 +14,9 @@
 cmake_minimum_required(VERSION 3.8 FATAL_ERROR) # for PyTorch extensions, version should be greater than 3.13
 project(FasterTransformer LANGUAGES CXX CUDA)
 
-find_package(CUDA 10.2 REQUIRED)
+find_package(CUDA 10.1 REQUIRED)
+
+option(EXAMPLES "build examples" on)
 
 if(${CUDA_VERSION_MAJOR} VERSION_GREATER_EQUAL "11")
   add_definitions("-DENABLE_BF16")
@@ -125,8 +127,6 @@ if(NOT (FIND_SM STREQUAL True))
   set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS}  \
                         -gencode=arch=compute_70,code=\\\"sm_70,compute_70\\\" \
                         -gencode=arch=compute_75,code=\\\"sm_75,compute_75\\\" \
-                        -gencode=arch=compute_80,code=\\\"sm_80,compute_80\\\" \
-                        -gencode=arch=compute_86,code=\\\"sm_86,compute_86\\\" \
                         ")
   #                      -rdc=true")
   set(CMAKE_C_FLAGS    "${CMAKE_C_FLAGS}    -DWMMA")
@@ -136,7 +136,12 @@ if(NOT (FIND_SM STREQUAL True))
     set(ENV{TORCH_CUDA_ARCH_LIST} "7.0;7.5;8.0;8.6")
   endif()
   set(CMAKE_CUDA_ARCHITECTURES 70 75 80 86)
-  message("-- Assign GPU architecture (sm=70,75,80,86)")
+if(${CUDA_VERSION_STRING}  VERSION_LESS_EQUAL "10.1" )
+  message("${CUDA_VERSION_STRING} removing unsupported sm 80 & 86")
+  list(REMOVE_ITEM CMAKE_CUDA_ARCHITECTURES 80 86)
+endif()
+  message("-- Assign GPU architecture (sm=${CMAKE_CUDA_ARCHITECTURES})")
+  set(SM 70)
 endif()
 
 if(BUILD_PYT)
@@ -152,8 +157,9 @@ set(CMAKE_CXX_STANDARD "${CXX_STD}")
 set(CMAKE_CXX_STANDARD_REQUIRED ON)
 set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --expt-extended-lambda")
 set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr")
-set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --std=c++${CXX_STD}")
-
+if(${CUDA_VERSION_STRING}  VERSION_GREATER "10.1.105" )
+  set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --std=c++${CXX_STD}")
+endif()
 set(CMAKE_CXX_FLAGS_RELEASE "${CMAKE_CXX_FLAGS_RELEASE} -O3")
 # set(CMAKE_CUDA_FLAGS_RELEASE "${CMAKE_CUDA_FLAGS_RELEASE} -Xcompiler -O3 --ptxas-options=--verbose")
 set(CMAKE_CUDA_FLAGS_RELEASE "${CMAKE_CUDA_FLAGS_RELEASE} -Xcompiler -O3")
@@ -230,9 +236,10 @@ link_directories(
 
 add_subdirectory(3rdparty)
 add_subdirectory(src)
-add_subdirectory(examples)
-add_subdirectory(tests)
-
+if(EXAMPLES)
+  add_subdirectory(examples)
+  add_subdirectory(tests)
+endif()
 ########################################
 
 if(BUILD_MULTI_GPU)
@@ -249,6 +256,7 @@ add_library(transformer-static STATIC
   $<TARGET_OBJECTS:FfnLayer>
   $<TARGET_OBJECTS:FusedAttentionLayer>
   $<TARGET_OBJECTS:GptContextAttentionLayer>
+  $<TARGET_OBJECTS:EncoderLayer>
   $<TARGET_OBJECTS:GptJ>
   $<TARGET_OBJECTS:GptJContextDecoder>
   $<TARGET_OBJECTS:GptJDecoder>
@@ -313,8 +321,9 @@ add_library(transformer-static STATIC
 set_property(TARGET transformer-static PROPERTY POSITION_INDEPENDENT_CODE ON)
 set_property(TARGET transformer-static PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)
 target_link_libraries(transformer-static PUBLIC -lcudart -lnccl -lmpi -lcublas -lcublasLt -lcurand)
+endif()
 
-add_library(transformer-shared SHARED
+set(transformer_objects  
   $<TARGET_OBJECTS:BaseBeamSearchLayer>
   $<TARGET_OBJECTS:BaseSamplingLayer>
   $<TARGET_OBJECTS:BeamSearchLayer>
@@ -324,29 +333,10 @@ add_library(transformer-shared SHARED
   $<TARGET_OBJECTS:FfnLayer>
   $<TARGET_OBJECTS:FusedAttentionLayer>
   $<TARGET_OBJECTS:GptContextAttentionLayer>
-  $<TARGET_OBJECTS:GptJ>
-  $<TARGET_OBJECTS:GptJContextDecoder>
-  $<TARGET_OBJECTS:GptJDecoder>
+  $<TARGET_OBJECTS:EncoderLayer>
   $<TARGET_OBJECTS:GptJDecoderLayerWeight>
-  $<TARGET_OBJECTS:GptJTritonBackend>
   $<TARGET_OBJECTS:GptJWeight>
   $<TARGET_OBJECTS:OnlineBeamSearchLayer>
-  $<TARGET_OBJECTS:ParallelGpt>
-  $<TARGET_OBJECTS:ParallelGptContextDecoder>
-  $<TARGET_OBJECTS:ParallelGptDecoder>
-  $<TARGET_OBJECTS:ParallelGptDecoderLayerWeight>
-  $<TARGET_OBJECTS:ParallelGptTritonBackend>
-  $<TARGET_OBJECTS:ParallelGptWeight>
-  $<TARGET_OBJECTS:T5Decoder>
-  $<TARGET_OBJECTS:T5Decoding>
-  $<TARGET_OBJECTS:T5Encoder>
-  $<TARGET_OBJECTS:T5TritonBackend>
-  $<TARGET_OBJECTS:TensorParallelDecoderCrossAttentionLayer>
-  $<TARGET_OBJECTS:TensorParallelDecoderSelfAttentionLayer>
-  $<TARGET_OBJECTS:TensorParallelGeluFfnLayer>
-  $<TARGET_OBJECTS:TensorParallelGptContextAttentionLayer>
-  $<TARGET_OBJECTS:TensorParallelReluFfnLayer>
-  $<TARGET_OBJECTS:TensorParallelUnfusedAttentionLayer>
   $<TARGET_OBJECTS:TopKSamplingLayer>
   $<TARGET_OBJECTS:TopKTopPSamplingLayer>
   $<TARGET_OBJECTS:TopPSamplingLayer>
@@ -373,9 +363,7 @@ add_library(transformer-shared SHARED
   $<TARGET_OBJECTS:matrix_transpose_kernels>
   $<TARGET_OBJECTS:matrix_vector_multiplication>
   $<TARGET_OBJECTS:memory_utils>
-  $<TARGET_OBJECTS:nccl_utils>
   $<TARGET_OBJECTS:custom_ar_comm>
-  $<TARGET_OBJECTS:custom_ar_kernels>
   $<TARGET_OBJECTS:word_list>
   $<TARGET_OBJECTS:online_softmax_beamsearch_kernels>
   $<TARGET_OBJECTS:quantization_int8_kernels>
@@ -387,14 +375,22 @@ add_library(transformer-shared SHARED
   $<TARGET_OBJECTS:trt_fused_multi_head_attention>
   $<TARGET_OBJECTS:unfused_attention_kernels>
   $<TARGET_OBJECTS:logprob_kernels>)
+
+if(${SM} GREATER_EQUAL 70)
+  set(transformer_objects  ${transformer_objects} $<TARGET_OBJECTS:custom_ar_kernels>)
+endif()
+
+add_library(transformer-shared SHARED ${transformer_objects})
 set_target_properties(transformer-shared PROPERTIES POSITION_INDEPENDENT_CODE ON)
 set_target_properties(transformer-shared PROPERTIES CUDA_RESOLVE_DEVICE_SYMBOLS ON)
 set_target_properties(transformer-shared PROPERTIES LINKER_LANGUAGE CXX)
-target_link_libraries(transformer-shared PUBLIC -lcudart -lnccl -lmpi -lcublas -lcublasLt -lcurand)
+target_link_libraries(transformer-shared PUBLIC -lcudart -lcublas -lcublasLt -lcurand)
 
-include(GNUInstallDirs)
+#include(GNUInstallDirs)
 set(INSTALL_CONFIGDIR ${CMAKE_INSTALL_LIBDIR}/cmake/FasterTransformer)
 
+
+
 include(CMakePackageConfigHelpers)
 configure_package_config_file(
   ${CMAKE_CURRENT_LIST_DIR}/cmake/FasterTransformerConfig.cmake.in
@@ -402,52 +398,23 @@ configure_package_config_file(
   INSTALL_DESTINATION ${INSTALL_CONFIGDIR}
 )
 
-install(
-  FILES
-  ${CMAKE_CURRENT_BINARY_DIR}/FasterTransformerConfig.cmake
-  DESTINATION ${INSTALL_CONFIGDIR}
-)
 
 install(
   TARGETS
     transformer-shared
   EXPORT
     transformer-shared-targets
-  LIBRARY DESTINATION ${CMAKE_INSTALL_PREFIX}/backends/fastertransformer
-  ARCHIVE DESTINATION ${CMAKE_INSTALL_PREFIX}/backends/fastertransformer
-)
-
-install(
-  EXPORT
-    transformer-shared-targets
-  FILE
-    FasterTransformerTargets.cmake
-  DESTINATION
-    ${INSTALL_CONFIGDIR}
+  LIBRARY DESTINATION ${CMAKE_INSTALL_PREFIX}/lib
+  ARCHIVE DESTINATION ${CMAKE_INSTALL_PREFIX}/lib
 )
 
 file(GLOB_RECURSE HEADER_FILES "*.h" "*.hpp" "*.cuh")
 foreach ( file ${HEADER_FILES} )
     file( RELATIVE_PATH rfile ${CMAKE_CURRENT_SOURCE_DIR} ${file} )
     get_filename_component( dir ${rfile} DIRECTORY )
-    install( FILES ${file} DESTINATION  ${CMAKE_INSTALL_PREFIX}/include/${dir} )
+    install( FILES ${file} DESTINATION  ${CMAKE_INSTALL_PREFIX}/include/${dir})
 endforeach()
 
 
-################################################################################
-# add_executable(gpt sample/cpp/gpt_sample.cc )
-# target_link_libraries(gpt PUBLIC -lcublas -lcublasLt -lcudart -lcurand -lnccl -lmpi transformer-static)
-# target_link_libraries(gpt PUBLIC -lcublas -lcublasLt -lcudart -lcurand -lnccl -lmpi decoder decoding)
-
-export(
-  EXPORT
-    transformer-shared-targets
-  FILE
-    ${CMAKE_CURRENT_BINARY_DIR}/FasterTransformerTargets.cmake
-  NAMESPACE
-    TritonCore::
-)
 
-export(PACKAGE FasterTransformer)
 
-endif() # BUILD_MULTI_GPU
diff --git a/README.md b/README.md
index a60983c..45b5374 100644
--- a/README.md
+++ b/README.md
@@ -52,7 +52,7 @@ FasterTransformer is built on top of CUDA, cuBLAS, cuBLASLt and C++. We provide
 | Swin Transformer | PyTorch        | Yes  | Yes                 | -                       | -               | -                 |
 | Swin Transformer | TensorRT       | Yes  | Yes                 | -                       | -               | -                 |
 | ViT              | PyTorch        | Yes  | Yes                 | -                       | -               | -                 |
-| ViT              | TensorRT       | Yes  | Yes                 | -                       | -               | -                 |
+| ViT              | TensorRT       | Yes  | -                   | -                       | -               | -                 |
 
 * Note that the FasterTransformer supports the models above on C++ because all source codes are built on C++.
 
diff --git a/deploy.sh b/deploy.sh
new file mode 100755
index 0000000..ac54401
--- /dev/null
+++ b/deploy.sh
@@ -0,0 +1,32 @@
+#copy cuda folder (once)
+base=`git rev-parse --show-toplevel`
+server=10.10.10.174
+while getopts "d" opt
+do
+case "${opt}" in 
+  "d" ) 
+   debug=1
+   shift
+   ;;
+esac
+done
+file=`realpath $1`
+shift
+rsync -v ${file} ${server}:${file}
+echo "file=${file}"
+rsync -v ${base}/../mindspore/trc/transformer/*.fp32 ${server}:${base}/build/bin
+rsync -v ${base}/build/lib/*.so ${server}:${base}/build/lib
+# echo "cd ${base}/build/bin/"
+command=$(cat <<-ENDM
+<<<<<<< HEAD
+  CUDA_VISIBLE_DEVICES=0 \
+  NVIDIA_TF32_OVERRIDE=0 \
+=======
+  CUDA_VISIBLE_DEVICES=3 \
+>>>>>>> origin/bert1
+  LD_LIBRARY_PATH=${base}/../FasterTransformer:/usr/local/cuda-11.7/lib64 \
+  ${file} $@ 
+ENDM
+)
+echo "command=${command}"
+ssh ${server} "cd ${base}/build/bin ;${command}"
diff --git a/docs/gpt_guide.md b/docs/gpt_guide.md
index afcba9a..71c4fab 100644
--- a/docs/gpt_guide.md
+++ b/docs/gpt_guide.md
@@ -312,7 +312,7 @@ python tools/checkpoint_util.py --model-type GPT --loader megatron --saver faste
 To convert the Megatron GPT model to binary, FasterTransformer provides a tool `examples/onnx/multi_gpu_gpt/onnx_ckpt_convert.py` to convert the checkpoint.
 
 ```bash
-wget https://github.com/onnx/models/raw/master/text/machine_comprehension/gpt-2/model/gpt2-10.onnx
+wget https://github.com/onnx/models/raw/main/text/machine_comprehension/gpt-2/model/gpt2-10.onnx
 python ../examples/onnx/multi_gpu_gpt/onnx_ckpt_convert.py -i gpt2-10.onnx -o ../models/onnx-models/c-model/124m/ -i_g 1
 python ../examples/onnx/multi_gpu_gpt/onnx_ckpt_convert.py -i gpt2-10.onnx -o ../models/onnx-models/c-model/124m/ -i_g 4
 ```
diff --git a/examples/cpp/CMakeLists.txt b/examples/cpp/CMakeLists.txt
index b67cd01..3cc4155 100644
--- a/examples/cpp/CMakeLists.txt
+++ b/examples/cpp/CMakeLists.txt
@@ -13,6 +13,7 @@
 # limitations under the License.
 
 add_subdirectory(bert)
+add_subdirectory(ms)
 add_subdirectory(bert_int8)
 add_subdirectory(decoding)
 add_subdirectory(gpt)
diff --git a/examples/cpp/gpt/gpt_example.cc b/examples/cpp/gpt/gpt_example.cc
index cacb09e..5fec0c9 100644
--- a/examples/cpp/gpt/gpt_example.cc
+++ b/examples/cpp/gpt/gpt_example.cc
@@ -236,7 +236,7 @@ void gpt_example(const INIReader reader)
 #endif
 
     if (std::is_same<T, half>::value) {
-        cublas_wrapper.setGemmConfig(CUDA_R_16F, CUDA_R_16F, CUDA_R_16F, CUDA_R_32F);
+        cublas_wrapper.setGemmConfig(CUDA_R_16F, CUDA_R_16F, CUDA_R_16F, CUBLAS_COMPUTE_32F_FAST_TF32);
     }
 #ifdef ENABLE_BF16
     else if (std::is_same<T, __nv_bfloat16>::value) {
diff --git a/examples/cpp/ms/CMakeLists.txt b/examples/cpp/ms/CMakeLists.txt
new file mode 100644
index 0000000..eb47b5c
--- /dev/null
+++ b/examples/cpp/ms/CMakeLists.txt
@@ -0,0 +1,22 @@
+# Copyright (c) 2019-2022, NVIDIA CORPORATION.  All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+add_executable(ms_benchmark ms.cc)
+if (SPARSITY_SUPPORT)
+# target_link_libraries(ms_benchmark PUBLIC -lcublas -lcublasLt -lcudart -lcusparse -lcusparseLt transformer-shared) 
+target_link_libraries(ms_benchmark PUBLIC -lcublas -lcublasLt -lcudart -lcusparse -lcusparseLt GptContextAttentionLayer EncoderLayer)
+else()
+# target_link_libraries(ms_benchmark PUBLIC -lcublas -lcublasLt -lcudart transformer-shared)
+target_link_libraries(ms_benchmark PUBLIC -lcublas -lcublasLt -lcudart GptContextAttentionLayer EncoderLayer)
+endif()
diff --git a/examples/cpp/ms/initialize.h b/examples/cpp/ms/initialize.h
new file mode 100644
index 0000000..9bcf4eb
--- /dev/null
+++ b/examples/cpp/ms/initialize.h
@@ -0,0 +1,643 @@
+#pragma once
+
+#include "src/fastertransformer/layers/attention_layers/AttentionWeight.h"
+#include "src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.h"
+#include "src/fastertransformer/layers/encoder_layers/EncoderLayerWeight.h"
+#include "src/fastertransformer/layers/encoder_layers/MSEncoderLayer.h"
+using namespace fastertransformer;
+struct opt_arg {
+    size_t batch_size;
+    size_t num_layers;
+    size_t seq_len;  // source seq len
+    size_t tgt_seq_len;
+    size_t head_num;
+    size_t hidden_size;
+    size_t size_per_head;
+    float eps1;
+    float eps2;
+    bool post_layernorm_residual;
+    bool is_remove_padding;
+    std::string model_name;
+    std::string compute_type;
+    std::string w_compute_type;
+    std::string s_compute_type;
+    size_t ffn_hidden_size;
+};
+template<typename T, typename U = T, typename S = T>
+struct DecriptorTest {
+    std::vector<Tensor> input_tensors;          // GPU
+    std::vector<Tensor> input_python_tensors;   // CPU
+    std::vector<Tensor> output_tensors;         // GPU
+    std::vector<Tensor> output_python_tensors;  // CPU
+    std::vector<Tensor> w_tensors;
+    BaseAttentionLayer<T, U, S>* Attn;
+    //
+};
+template<typename T, typename U = T, typename S = T>
+struct DecriptorEncoderLayer {
+    std::vector<Tensor> input_tensors;          // GPU
+    std::vector<Tensor> input_python_tensors;   // CPU
+    std::vector<Tensor> output_tensors;         // GPU
+    std::vector<Tensor> output_python_tensors;  // CPU
+    std::vector<Tensor> w_tensors;
+    BaseEncoderLayer<T, U, S>* Encoder;
+    //
+};
+
+typedef enum {
+    MHA_X1 = 1,     // AttnIn + AttnMask
+    MHA_X2,         // AttnIn + EncOut -- same seq size + AttnMask
+    MHA_CROSS,      // AttnIn + EncOut + AttnMAsk
+    MHA_T5,         // AttnIn + EncOut + AttnMAsk + position_bias
+    MHA_T5_CROSS,   // AttnIn + EncOut + AttnMAsk + position_bias
+    TEL,            // transformer encoder layer
+} MODEL_TEST_ID_E;
+
+int ModelNum(std::string model_name)
+{
+    if (model_name == "mha_x1") {
+        return MHA_X1;
+    }
+    else if (model_name == "mha_x2") {
+        return MHA_X2;
+    }
+    else if (model_name == "mha_cross") {
+        return MHA_CROSS;
+    }
+    else if (model_name == "mha_T5") {
+        return MHA_T5;
+    } else if (model_name == "mha_T5_cross") { 
+        return MHA_T5_CROSS;
+    } else if (model_name == "transformer_encoder_layer") { 
+        return TEL;
+    } else {
+        return -1;
+    }
+}
+
+template<typename T, typename U = T, typename S = T>
+void InitializeAttn(opt_arg* opt_a,
+                    DecriptorTest<T, U, S>& desc,
+                    cudaStream_t stream,
+                    cublasMMWrapper* cublas_wrapper,
+                    Allocator<AllocatorType::CUDA>* allocator)
+{
+    const size_t hidden_units = opt_a->head_num * opt_a->size_per_head;
+
+    // TODO Nizzan - check if need to be <T,U>
+    desc.Attn = new MSMHALayer<T, U, S>(opt_a->batch_size,
+                                    opt_a->seq_len,
+                                    opt_a->tgt_seq_len,
+                                    opt_a->head_num,
+                                    opt_a->size_per_head,
+                                    stream,
+                                    cublas_wrapper,
+                                    allocator,
+                                    false,      // free buffer after fwd
+                                    true,       // is_qk_buf_float_
+                                    false,      //is_cross
+                                    false,      // sparse
+                                    false);     // is_position_bias
+
+    desc.input_tensors.push_back(Tensor{MEMORY_GPU,
+                                    getTensorType<T>(),
+                                    std::vector<size_t>{opt_a->batch_size * opt_a->seq_len,hidden_units},
+                                    0});
+    desc.input_tensors.push_back(Tensor{MEMORY_GPU,
+                                    getTensorType<U>(),
+                                    std::vector<size_t>{opt_a->batch_size, 1, opt_a->seq_len, opt_a->seq_len},
+                                    0});
+
+    desc.input_python_tensors.push_back(Tensor{MEMORY_CPU,
+                                    getTensorType<T>(),
+                                    std::vector<size_t>{opt_a->batch_size * opt_a->seq_len,hidden_units},
+                                    0});
+									
+    desc.input_python_tensors.push_back(Tensor{MEMORY_CPU,
+                                    getTensorType<U>(),
+                                    std::vector<size_t>{opt_a->batch_size, 1, opt_a->seq_len, opt_a->seq_len},
+                                    0});
+    // desc.input_python_tensors.push_back(Tensor{MEMORY_CPU,
+    //                                 getTensorType<T>(),
+    //                                 std::vector<size_t>{opt_a->batch_size * opt_a->seq_len,hidden_units},
+    //                                 0});
+    // desc.input_python_tensors.push_back(Tensor{MEMORY_CPU,
+    //                                 getTensorType<U>(),
+    //                                 std::vector<size_t>{opt_a->batch_size, 1, opt_a->seq_len, opt_a->seq_len},
+    //                                 0});
+    // GPU RESULTS
+    desc.output_tensors.push_back(Tensor{
+        MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, hidden_units}, 0});
+    // desc.output_tensors.push_back(
+    //     Tensor{MEMORY_GPU,
+    //            getTensorType<T>(),
+    //            std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->size_per_head},
+    //            0});
+    // desc.output_tensors.push_back(
+    //     Tensor{MEMORY_GPU,
+    //            getTensorType<T>(),
+    //            std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->size_per_head},
+    //            0});
+
+    desc.output_python_tensors.push_back(Tensor{
+        MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, hidden_units}, 0});
+    // desc.output_python_tensors.push_back(
+    //     Tensor{MEMORY_CPU,
+    //            getTensorType<T>(),
+    //            std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->size_per_head},
+    //            0});
+    // desc.output_python_tensors.push_back(
+    //     Tensor{MEMORY_CPU,
+    //            getTensorType<T>(),
+    //            std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->size_per_head},
+    //            0});
+
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, 3 * hidden_units}, 0});
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, hidden_units}, 0});
+    desc.w_tensors.push_back(Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{3 * hidden_units}, 0});
+    desc.w_tensors.push_back(Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units}, 0});
+}
+template<typename T, typename U = T, typename S = T>
+void InitializeAttnX2(opt_arg* opt_a,
+                      DecriptorTest<T, U, S>& desc,
+                      cudaStream_t stream,
+                      cublasMMWrapper* cublas_wrapper,
+                      Allocator<AllocatorType::CUDA>* allocator)
+{
+    const size_t hidden_units = opt_a->head_num * opt_a->size_per_head;
+
+    desc.Attn = new MSMHALayer<T, U, S>(opt_a->batch_size,
+                                    opt_a->seq_len,
+                                    opt_a->tgt_seq_len,
+                                    opt_a->head_num,
+                                    opt_a->size_per_head,
+                                    stream,
+                                    cublas_wrapper,
+                                    allocator,
+                                    false,      // free buffer after fwd
+                                    true,       // is_qk_buf_float_
+                                    false,      //is_cross
+                                    false,      // sparse
+                                    false);     // is_position_bias
+
+    desc.input_tensors.push_back(Tensor{MEMORY_GPU,
+                                    getTensorType<T>(),
+                                    std::vector<size_t>{opt_a->batch_size * opt_a->seq_len, hidden_units},
+                                    0});
+
+    // GPU RESULTS
+    desc.output_tensors.push_back(Tensor{
+        MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, hidden_units}, 0});
+    desc.output_tensors.push_back(
+        Tensor{MEMORY_GPU,
+               getTensorType<T>(),
+               std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->size_per_head},
+               0});
+    desc.output_tensors.push_back(
+        Tensor{MEMORY_GPU,
+               getTensorType<T>(),
+               std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->size_per_head},
+               0});
+
+    desc.output_python_tensors.push_back(Tensor{
+        MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, hidden_units}, 0});
+    desc.output_python_tensors.push_back(
+        Tensor{MEMORY_CPU,
+               getTensorType<T>(),
+               std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->size_per_head},
+               0});
+    desc.output_python_tensors.push_back(
+        Tensor{MEMORY_CPU,
+               getTensorType<T>(),
+               std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->size_per_head},
+               0});
+
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, hidden_units}, 0});
+    desc.w_tensors.push_back(Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{3 * hidden_units}, 0});
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, 2 * hidden_units}, 0});
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, hidden_units}, 0});
+    desc.w_tensors.push_back(Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units}, 0});
+}
+
+template<typename T, typename U = T, typename S = T>
+void InitializeAttnCross(opt_arg* opt_a,
+                         DecriptorTest<T, U, S>& desc,
+                         cudaStream_t stream,
+                         cublasMMWrapper* cublas_wrapper,
+                         Allocator<AllocatorType::CUDA>* allocator)
+{
+    const size_t hidden_units = opt_a->head_num * opt_a->size_per_head;
+
+    desc.Attn = new MSMHALayer<T, U, S>(opt_a->batch_size,
+                                    opt_a->seq_len,
+                                    opt_a->tgt_seq_len,
+                                    opt_a->head_num,
+                                    opt_a->size_per_head,
+                                    stream,
+                                    cublas_wrapper,
+                                    allocator,
+                                    false,      // free buffer after fwd
+                                    true,       // is_qk_buf_float_
+                                    true,       //is_cross
+                                    false,      // sparse
+                                    false);     // is_position_bias
+
+    desc.input_tensors.push_back(Tensor{MEMORY_GPU,
+                                        getTensorType<T>(),
+                                        std::vector<size_t>{opt_a->batch_size*opt_a->seq_len, hidden_units},
+                                        0});
+    desc.input_tensors.push_back(Tensor{
+        MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size* opt_a->tgt_seq_len, hidden_units}, 0});
+
+    desc.input_tensors.push_back(Tensor{
+        MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, opt_a->tgt_seq_len}, 0});
+ desc.input_python_tensors.push_back(Tensor{MEMORY_CPU,
+                                        getTensorType<T>(),
+                                        std::vector<size_t>{opt_a->batch_size*opt_a->seq_len, hidden_units},
+                                        0});
+    desc.input_python_tensors.push_back(Tensor{
+        MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size* opt_a->tgt_seq_len, hidden_units}, 0});
+
+    desc.input_python_tensors.push_back(Tensor{
+        MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, opt_a->tgt_seq_len}, 0});
+
+   
+    // GPU RESULTS
+
+    desc.output_tensors.push_back(Tensor{
+        MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, hidden_units}, 0});
+    // desc.output_tensors.push_back(Tensor{
+    //     MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->tgt_seq_len, opt_a->size_per_head}, 0});
+    // desc.output_tensors.push_back(Tensor{
+    //     MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->tgt_seq_len, opt_a->size_per_head}, 0});
+
+    desc.output_python_tensors.push_back(Tensor{
+        MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, hidden_units}, 0});
+    // desc.output_python_tensors.push_back(Tensor{
+    //     MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->tgt_seq_len, opt_a->size_per_head}, 0});
+    // desc.output_python_tensors.push_back(Tensor{
+    //     MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->tgt_seq_len, opt_a->size_per_head}, 0});
+
+
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, hidden_units}, 0});
+    desc.w_tensors.push_back(Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{3 * hidden_units}, 0});
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, 2 * hidden_units}, 0});
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, hidden_units}, 0});
+    desc.w_tensors.push_back(Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units}, 0});
+}
+template<typename T, typename U = T, typename S = T>
+void InitializeAttnT5(opt_arg* opt_a,
+                      DecriptorTest<T, U, S>& desc,
+                      cudaStream_t stream,
+                      cublasMMWrapper* cublas_wrapper,
+                      Allocator<AllocatorType::CUDA>* allocator)
+{
+    const size_t hidden_units = opt_a->head_num * opt_a->size_per_head;
+
+    desc.Attn = new MSMHALayer<T, U, S>(opt_a->batch_size,
+                                    opt_a->seq_len,
+                                    opt_a->tgt_seq_len,
+                                    opt_a->head_num,
+                                    opt_a->size_per_head,
+                                    stream,
+                                    cublas_wrapper,
+                                    allocator,
+                                    false,      // free buffer after fwd
+                                    true,       // is_qk_buf_float_
+                                    false,      //is_cross
+                                    false,      // sparse
+                                    true);     // is_position_bias
+
+    desc.input_tensors.push_back(Tensor{MEMORY_GPU,
+                                    getTensorType<T>(),
+                                    std::vector<size_t>{opt_a->batch_size * opt_a->seq_len,hidden_units},
+                                    0});
+    desc.input_tensors.push_back(Tensor{MEMORY_GPU,
+                                    getTensorType<U>(),
+                                    std::vector<size_t>{opt_a->batch_size, 1, opt_a->seq_len, opt_a->seq_len},
+                                    0});
+    
+    desc.input_tensors.push_back(Tensor{MEMORY_GPU,
+                                    getTensorType<T>(),
+                                    std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->tgt_seq_len},
+                                    0});
+
+    desc.input_python_tensors.push_back(Tensor{MEMORY_CPU,
+                                    getTensorType<T>(),
+                                    std::vector<size_t>{opt_a->batch_size * opt_a->seq_len,hidden_units},
+                                    0});
+									
+    desc.input_python_tensors.push_back(Tensor{MEMORY_CPU,
+                                    getTensorType<U>(),
+                                    std::vector<size_t>{opt_a->batch_size, 1, opt_a->seq_len, opt_a->seq_len},
+                                    0});
+
+    desc.input_python_tensors.push_back(Tensor{MEMORY_CPU,
+                                    getTensorType<T>(),
+                                    std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->tgt_seq_len},
+                                    0});
+
+
+    // GPU RESULTS
+
+    desc.output_tensors.push_back(Tensor{
+        MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, hidden_units}, 0});
+    // desc.output_tensors.push_back(Tensor{
+    //     MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->tgt_seq_len, opt_a->size_per_head}, 0});
+    // desc.output_tensors.push_back(Tensor{
+    //     MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->tgt_seq_len, opt_a->size_per_head}, 0});
+    // desc.output_tensors.push_back(Tensor{
+    //   MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->tgt_seq_len},0});
+
+    desc.output_python_tensors.push_back(Tensor{
+        MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, hidden_units}, 0});
+    // desc.output_python_tensors.push_back(Tensor{
+    //     MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->tgt_seq_len, opt_a->size_per_head}, 0});
+    // desc.output_python_tensors.push_back(Tensor{
+    //     MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->tgt_seq_len, opt_a->size_per_head}, 0});
+    // desc.output_python_tensors.push_back(Tensor{
+    //     MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->tgt_seq_len}, 0});
+
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, 3 * hidden_units}, 0});
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, hidden_units}, 0});
+}
+
+template<typename T, typename U=T, typename S=T>
+void InitializeAttnT5Cross(opt_arg* opt_a,
+                        DecriptorTest<T, U, S> &desc,
+                        cudaStream_t stream,
+                        cublasMMWrapper* cublas_wrapper,
+                        Allocator<AllocatorType::CUDA>* allocator) {
+    const size_t hidden_units = opt_a->head_num * opt_a->size_per_head;
+
+    desc.Attn = new MSMHALayer<T, U, S>(opt_a->batch_size,
+                                    opt_a->seq_len,
+                                    opt_a->tgt_seq_len,
+                                    opt_a->head_num,
+                                    opt_a->size_per_head,
+                                    stream,
+                                    cublas_wrapper,
+                                    allocator,
+                                    false,      // free buffer after fwd
+                                    true,       // is_qk_buf_float_
+                                    true,       //is_cross
+                                    false,      // sparse
+                                    true);     // is_position_bias
+
+    desc.input_tensors.push_back(Tensor{MEMORY_GPU,
+                                    getTensorType<T>(),
+                                    std::vector<size_t>{opt_a->batch_size * opt_a->seq_len,hidden_units},
+                                    0});
+    
+    desc.input_tensors.push_back(Tensor{MEMORY_GPU,
+                                    getTensorType<T>(),
+                                    std::vector<size_t>{opt_a->batch_size * opt_a->tgt_seq_len, hidden_units},
+                                    0});
+
+    desc.input_tensors.push_back(Tensor{MEMORY_GPU,
+                                    getTensorType<U>(),
+                                    std::vector<size_t>{opt_a->batch_size, 1, opt_a->seq_len, opt_a->tgt_seq_len},
+                                    0});
+    
+    desc.input_tensors.push_back(Tensor{MEMORY_GPU,
+                                    getTensorType<T>(),
+                                    std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->tgt_seq_len},
+                                    0});
+
+    desc.input_python_tensors.push_back(Tensor{MEMORY_CPU,
+                                    getTensorType<T>(),
+                                    std::vector<size_t>{opt_a->batch_size * opt_a->seq_len,hidden_units},
+                                    0});
+    
+    desc.input_python_tensors.push_back(Tensor{MEMORY_CPU,
+                                    getTensorType<T>(),
+                                    std::vector<size_t>{opt_a->batch_size * opt_a->tgt_seq_len, hidden_units},
+                                    0});
+									
+    desc.input_python_tensors.push_back(Tensor{MEMORY_CPU,
+                                    getTensorType<U>(),
+                                    std::vector<size_t>{opt_a->batch_size, 1, opt_a->seq_len, opt_a->tgt_seq_len},
+                                    0});
+
+    desc.input_python_tensors.push_back(Tensor{MEMORY_CPU,
+                                    getTensorType<T>(),
+                                    std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->tgt_seq_len},
+                                    0});
+
+
+    // GPU RESULTS
+
+    desc.output_tensors.push_back(Tensor{
+        MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, hidden_units}, 0});
+    // desc.output_tensors.push_back(Tensor{
+    //     MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->tgt_seq_len, opt_a->size_per_head}, 0});
+    // desc.output_tensors.push_back(Tensor{
+    //     MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->tgt_seq_len, opt_a->size_per_head}, 0});
+    // desc.output_tensors.push_back(Tensor{
+    //   MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->tgt_seq_len},0});
+
+    desc.output_python_tensors.push_back(Tensor{
+        MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, hidden_units}, 0});
+    // desc.output_python_tensors.push_back(Tensor{
+    //     MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->tgt_seq_len, opt_a->size_per_head}, 0});
+    // desc.output_python_tensors.push_back(Tensor{
+    //     MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->tgt_seq_len, opt_a->size_per_head}, 0});
+    // desc.output_python_tensors.push_back(Tensor{
+    //     MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->head_num, opt_a->seq_len, opt_a->tgt_seq_len}, 0});
+
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, hidden_units}, 0});
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, 2 * hidden_units}, 0});
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, hidden_units}, 0});
+}
+
+template<typename T, typename U = T, typename S = T>
+void InitializeEncoder(opt_arg* opt_a,
+                       DecriptorEncoderLayer<T, U, S>& desc,
+                       cudaStream_t stream,
+                       cublasMMWrapper* cublas_wrapper,
+                       cublasHandle_t* cublas_handle,
+                       Allocator<AllocatorType::CUDA>* allocator)
+{
+    // const size_t hidden_units = opt_a->head_num * opt_a->size_per_head;
+    const size_t hidden_units = opt_a->hidden_size;
+    // TODO Nizzan - check if need to be <T,U>
+    desc.Encoder = new MSELayer<T, U, S>(opt_a->batch_size,
+                                         opt_a->seq_len,
+                                         opt_a->tgt_seq_len,
+                                         opt_a->head_num,
+                                         opt_a->size_per_head,
+                                         opt_a->ffn_hidden_size,
+                                         opt_a->eps1,
+                                         opt_a->eps2,
+                                         opt_a->post_layernorm_residual,
+                                         stream,
+                                         cublas_wrapper,
+                                         cublas_handle,
+                                         allocator,
+                                         false,   // free buffer after fwd
+                                         true,    // is_qk_buf_float_
+                                         false);  // sparse
+    desc.input_tensors.push_back(Tensor{
+        MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, opt_a->hidden_size}, 0});
+    desc.input_tensors.push_back(Tensor{
+        MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, opt_a->seq_len}, 0});
+    desc.input_python_tensors.push_back(Tensor{
+        MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, opt_a->hidden_size}, 0});
+    desc.input_python_tensors.push_back(Tensor{
+        MEMORY_CPU, getTensorType<U>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, opt_a->seq_len}, 0});
+
+    desc.output_tensors.push_back(Tensor{
+        MEMORY_GPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, opt_a->hidden_size}, 0});
+
+    desc.output_python_tensors.push_back(Tensor{
+        MEMORY_CPU, getTensorType<T>(), std::vector<size_t>{opt_a->batch_size, opt_a->seq_len, opt_a->hidden_size}, 0});
+
+    desc.w_tensors.push_back(Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{opt_a->hidden_size}, 0});
+    desc.w_tensors.push_back(Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{opt_a->hidden_size}, 0});
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, 3 * hidden_units}, 0});
+    desc.w_tensors.push_back(Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{3 * hidden_units}, 0});
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units, hidden_units}, 0});
+    desc.w_tensors.push_back(Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{hidden_units}, 0});
+    desc.w_tensors.push_back(Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{opt_a->hidden_size}, 0});
+
+    desc.w_tensors.push_back(Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{opt_a->hidden_size}, 0});
+
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{opt_a->hidden_size, opt_a->ffn_hidden_size}, 0});
+    desc.w_tensors.push_back(Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{opt_a->ffn_hidden_size}, 0});
+    desc.w_tensors.push_back(
+        Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{opt_a->ffn_hidden_size, opt_a->hidden_size}, 0});
+    desc.w_tensors.push_back(Tensor{MEMORY_GPU, getTensorType<U>(), std::vector<size_t>{opt_a->hidden_size}, 0});
+}
+
+template<typename T, typename U = T, typename S = T>
+void Init(opt_arg* opt_a,
+          DecriptorTest<T, U, S>& desc,
+          cudaStream_t stream,
+          cublasMMWrapper* cublas_wrapper,
+          Allocator<AllocatorType::CUDA>* allocator)
+{
+    int model_num = ModelNum(opt_a->model_name);
+    switch (model_num) {
+        case MHA_X1:
+            InitializeAttn<T, U, S>(opt_a, desc, stream, cublas_wrapper, allocator);
+            break;
+        case MHA_X2:
+            InitializeAttnX2<T, U, S>(opt_a,
+                              desc,
+                              stream,
+                              cublas_wrapper,
+                              allocator);
+            break;
+        case MHA_CROSS:
+            InitializeAttnCross<T, U, S>(opt_a,
+                              desc,
+                              stream,
+                              cublas_wrapper,
+                              allocator);
+            break;
+        case MHA_T5:
+            InitializeAttnT5<T, U, S>(opt_a,
+                            desc,
+                            stream,
+                            cublas_wrapper,
+                            allocator);
+            break;
+        case MHA_T5_CROSS:
+            InitializeAttnT5Cross<T, U, S>(opt_a,
+                                desc,
+                                stream,
+                                cublas_wrapper,
+                                allocator);
+            break;
+        default:
+            break;
+    }
+}
+template<typename T, typename U = T, typename S = T>
+void InitE(opt_arg* opt_a,
+           DecriptorEncoderLayer<T, U, S>& desc,
+           cudaStream_t stream,
+           cublasMMWrapper* cublas_wrapper,
+           cublasHandle_t* cublas_handle,
+           Allocator<AllocatorType::CUDA>* allocator)
+{
+    int model_num = ModelNum(opt_a->model_name);
+    switch (model_num) {
+        case TEL:
+            InitializeEncoder<T, U, S>(opt_a, desc, stream, cublas_wrapper, cublas_handle, allocator);
+            break;
+        default:
+            break;
+    }
+}
+
+template<typename T>
+void InitWeight(opt_arg* opt_a, AttentionWeight<T>& attn_weights, std::vector<Tensor> w_tensors)
+{
+    int modelId = ModelNum(opt_a->model_name);
+    if (modelId == MHA_X1) {
+        attn_weights.query_weight.kernel = (const T*)w_tensors[0].data;
+        attn_weights.attention_output_weight.kernel = (const T*)w_tensors[1].data;
+        attn_weights.query_weight.bias = (const T*)w_tensors[2].data;
+        attn_weights.attention_output_weight.bias = (const T*)w_tensors[3].data;
+    }
+    else if (modelId == MHA_X2 || modelId == MHA_CROSS) {
+        attn_weights.query_weight.kernel = (const T*)w_tensors[0].data;
+        attn_weights.query_weight.bias = (const T*)w_tensors[1].data;
+        attn_weights.key_weight.kernel = (const T*)w_tensors[2].data;
+        attn_weights.attention_output_weight.kernel = (const T*)w_tensors[3].data;
+        attn_weights.attention_output_weight.bias = (const T*)w_tensors[4].data;
+    } else if (modelId==MHA_T5) {
+        attn_weights.query_weight.kernel = (const T*)w_tensors[0].data;
+        attn_weights.query_weight.bias = nullptr;
+        attn_weights.attention_output_weight.kernel = (const T*)w_tensors[1].data;
+        attn_weights.attention_output_weight.bias = nullptr;
+    } else if (modelId==MHA_T5_CROSS) {
+        attn_weights.query_weight.kernel = (const T*)w_tensors[0].data;
+        attn_weights.query_weight.bias = nullptr;
+        attn_weights.key_weight.kernel = (const T*)w_tensors[1].data;
+        attn_weights.attention_output_weight.kernel = (const T*)w_tensors[2].data;
+        attn_weights.attention_output_weight.bias = nullptr;
+    } else {
+        // return ERROR illegal model !
+    }
+}
+
+template<typename T>
+void InitWeightEncoder(opt_arg* opt_a, EncoderLayerWeight<T>& encoder_weights, std::vector<Tensor> w_tensors)
+{
+    int modelId = ModelNum(opt_a->model_name);
+    if (modelId == TEL) {
+        encoder_weights.qkv_weight.kernel = (const T*)w_tensors[2].data;
+        encoder_weights.qkv_weight.bias = (const T*)w_tensors[3].data;
+        encoder_weights.attention_layer_output_weight.kernel = (const T*)w_tensors[4].data;
+        encoder_weights.attention_layer_output_weight.bias = (const T*)w_tensors[5].data;
+        encoder_weights.layernorm1.gamma = (const T*)w_tensors[0].data;
+        encoder_weights.layernorm1.beta = (const T*)w_tensors[1].data;
+        encoder_weights.layernorm2.gamma = (const T*)w_tensors[6].data;
+        encoder_weights.layernorm2.beta = (const T*)w_tensors[7].data;
+        encoder_weights.encoder_output_mapping.kernel = (const T*)w_tensors[8].data;
+        encoder_weights.encoder_output_projection.kernel = (const T*)w_tensors[10].data;
+        encoder_weights.encoder_output_mapping.bias = (const T*)w_tensors[9].data;
+        encoder_weights.encoder_output_projection.bias = (const T*)w_tensors[11].data;
+    }
+    else {
+        // return ERROR illegal model !
+    }
+}
diff --git a/examples/cpp/ms/ms.cc b/examples/cpp/ms/ms.cc
new file mode 100644
index 0000000..2b12bd5
--- /dev/null
+++ b/examples/cpp/ms/ms.cc
@@ -0,0 +1,591 @@
+/*
+ * Copyright (c) 2019-2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include "examples/cpp/ms/initialize.h"
+#include "src/fastertransformer/layers/attention_layers/AttentionWeight.h"
+#include "src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.h"
+#include "src/fastertransformer/layers/encoder_layers/EncoderLayerWeight.h"
+#include "src/fastertransformer/layers/encoder_layers/MSEncoderLayer.h"
+#include "src/fastertransformer/utils/logger.h"
+#include <cfloat>
+#include <getopt.h>
+#include <vector>
+using namespace fastertransformer;
+
+template<typename T, typename U = T, typename S = T>
+int MsExample(opt_arg* opt_a);
+void usage()
+{
+    std::cout << "Usage: ms_benchmark -b<batch_size> -l <num_layers>"
+              << "-s <seq_len> -H <head_num> -S <hidden_size> -p <is_remove_padding>"
+              << "-T <compute_type> -W <weights_compute_type> -F <softmax_compute_type>"
+              << "-m <model_name>  \n";
+}
+
+bool read_args(int argc, char* argv[], opt_arg* opt_a)
+{
+    int opt;
+    while ((opt = getopt(argc, argv, "b:l:s:t:H:S:p:m:T:W:F:i:w:f:P:e1:e2")) != -1) {
+        switch (opt) {
+            case 'b':
+                opt_a->batch_size = atoi(optarg);
+                break;
+            case 'l':
+                opt_a->num_layers = atoi(optarg);
+                break;
+            case 's':
+                opt_a->seq_len = atoi(optarg);
+                break;
+            case 't':
+                opt_a->tgt_seq_len = atoi(optarg);
+                break;
+            case 'H':
+                opt_a->head_num = atoi(optarg);
+                break;
+            case 'S':
+                opt_a->hidden_size = atoi(optarg);
+                break;
+            case 'm':
+                opt_a->model_name = std::string(optarg);
+                break;
+            case 'T':
+                opt_a->compute_type = std::string(optarg);
+                break;
+            case 'W':
+                opt_a->w_compute_type = std::string(optarg);
+                break;
+            case 'F':
+                opt_a->s_compute_type = std::string(optarg);
+                break;
+            case 'f':
+                opt_a->ffn_hidden_size = atoi(optarg);
+                break;
+            case '1':
+                opt_a->eps1 = atoi(optarg);
+                break;
+            case '2':
+                opt_a->eps2 = atoi(optarg);
+                break;
+            case 'P':
+                if (atoi(optarg) == 1)
+                    opt_a->post_layernorm_residual=true;
+                else if (atoi(optarg) == 0)
+                    opt_a->post_layernorm_residual=false;
+                break;
+            case 'p':
+                opt_a->is_remove_padding = bool(optarg);
+                break;
+            case 'i':
+            case 'w':
+                break;
+            case 'h':
+            default:
+                usage();
+                return false;
+        }
+    }
+    opt_a->size_per_head = opt_a->hidden_size / opt_a->head_num;
+    opt_a->tgt_seq_len = (opt_a->tgt_seq_len == -1) ? opt_a->seq_len : opt_a->tgt_seq_len;
+    if (opt_a->ffn_hidden_size == -1) {
+        opt_a->ffn_hidden_size = opt_a->hidden_size * 4;
+    }
+    return true;
+}
+
+int main(int argc, char** argv)
+{
+    opt_arg opt_a;
+    opt_a.batch_size = 1;
+    opt_a.num_layers = 1;
+    opt_a.seq_len = 1;
+    opt_a.tgt_seq_len = -1;
+    opt_a.head_num = 1;
+    opt_a.hidden_size = 1;
+    opt_a.size_per_head = 1;
+    opt_a.ffn_hidden_size = -1;
+    opt_a.eps1 = 1e-6f;
+    opt_a.eps2 = 1e-6f;
+    opt_a.post_layernorm_residual = true;
+    opt_a.is_remove_padding = false;
+    opt_a.model_name = "";
+    opt_a.compute_type = "fp32";
+    opt_a.w_compute_type = "fp32";
+    opt_a.s_compute_type = "fp32";
+
+
+    if (read_args(argc, argv, &opt_a)) {
+        bool c_type_fp32 = (opt_a.compute_type.compare("fp32") == 0);
+        bool w_type_fp32 = (opt_a.w_compute_type.compare("fp32") == 0);
+        bool s_type_fp32 = (opt_a.s_compute_type.compare("fp32") == 0);
+
+        s_type_fp32 = c_type_fp32;  // Do softmax compute type as compute type
+        if (c_type_fp32 && w_type_fp32 && s_type_fp32) {
+            return MsExample<float, float, float>(&opt_a);
+        }
+        else if (c_type_fp32 && w_type_fp32 && !s_type_fp32) {
+            return MsExample<float, float, half>(&opt_a);
+        }
+        else if (c_type_fp32 && !w_type_fp32 && s_type_fp32) {
+            return MsExample<float, half, float>(&opt_a);
+        }
+        else if (c_type_fp32 && !w_type_fp32 && !s_type_fp32) {
+            return MsExample<float, half, half>(&opt_a);
+        }
+        else if (!c_type_fp32 && w_type_fp32 && s_type_fp32) {
+            return MsExample<half, float, float>(&opt_a);
+        }
+        else if (!c_type_fp32 && w_type_fp32 && !s_type_fp32) {
+            return MsExample<half, float, half>(&opt_a);
+        }
+        else if (!c_type_fp32 && !w_type_fp32 && s_type_fp32) {
+            return MsExample<half, half, float>(&opt_a);
+        }
+        else {  // (!c_type_fp32 && !w_type_fp32 && !s_type_fp32)
+            return MsExample<half, half, half>(&opt_a);
+        }
+    }
+}
+
+template<typename T>
+int ReadFileBuf(const std::string file, T* buf, size_t size_buff)
+{
+    if (file.empty()) {
+        FT_LOG_ERROR("file is nullptr\n");
+        return -1;
+    }
+
+    std::ifstream ifs(file);
+    if (!ifs.good()) {
+        FT_LOG_ERROR("file: %s does not exist\n", file.c_str());
+        return -1;
+    }
+
+    if (!ifs.is_open()) {
+        FT_LOG_ERROR("file: open failed\n");
+        return -1;
+    }
+
+    ifs.seekg(0, std::ios::end);
+    size_t file_size = ifs.tellg();
+    if (file_size != size_buff) {
+        ifs.close();
+        FT_LOG_ERROR("file: %s size is %d desc size is %d\n", file.c_str(), file_size, size_buff);
+        return -1;
+    }
+    // return 0;
+    ifs.seekg(0, std::ios::beg);
+    ifs.read(reinterpret_cast<char*>(buf), size_buff);
+    ifs.close();
+    return 0;
+}
+
+template<typename T>
+int CalcTensorsSize(std::vector<Tensor>& tensors)
+{
+    int total = 0;
+    for (size_t i = 0; i < tensors.size(); i++) {
+        float size = 1;
+        for (size_t j = 0; j < tensors[i].shape.size(); j++) {
+            size *= tensors[i].shape[j];
+        }
+        total += size;
+    }
+
+    return total * sizeof(T);
+}
+
+template<typename T>
+int ReadTensors(std::vector<Tensor>& tensors, std::string post, opt_arg* opt_a, bool cpy = true)
+{
+    for (size_t i = 0; i < tensors.size(); i++) {
+        // if (tensors[i].type != TYPE_FP32) {
+        //     FT_LOG_ERROR("Type not supported, exiting ");
+        //     return -1;
+        // }
+        float size = 1;
+        for (size_t j = 0; j < tensors[i].shape.size(); j++) {
+            size *= tensors[i].shape[j];
+        }
+        std::string suffix = post.compare("weight") == 0 ? opt_a->w_compute_type : opt_a->compute_type;
+        std::string fn = opt_a->model_name + "_" + post + std::to_string(i + 1) + "." + suffix;
+        T* input;
+        T* input_host = (T*)malloc(size * sizeof(T));
+        int res = ReadFileBuf(fn, input_host, size * sizeof(T));
+        if (res) {
+            fn = opt_a->model_name + "_" + post + std::to_string(i + 1) + "." + "fp16";
+            res = ReadFileBuf(fn, input_host, size * 2);
+        }
+        FT_CHECK(!res);
+        if (tensors[i].where == MEMORY_GPU) {
+            deviceMalloc(&input, size, false);
+            if (cpy)
+                cudaH2Dcpy(input, input_host, size);
+            else
+                deviceMemSetZero<T>(input, size);
+            tensors[i].data = input;
+            free(input_host);
+            input_host = 0;
+        }
+        else if (tensors[i].where == MEMORY_CPU) {
+            tensors[i].data = input_host;
+        }
+    }
+    return 0;
+}
+template<typename T>
+static float CompareData(const T* refOutput, int size, const T* msTensorData)
+{
+    constexpr float relativeTolerance = 1e-5;
+    constexpr float absoluteTolerance = 1e-8;
+    size_t errorCount = 0;
+    float meanError = 0;
+    std::cout << "Out tensor size is: " << size << std::endl;
+    std::cout << "Data of model output: ";
+    static int x = 0;
+    int s = std::min(10, size);
+    if (x == 0) {
+        for (int j = 0; j < s; j++) {  // std::min(50, size)
+            std::cout << static_cast<float>(msTensorData[j]) << " ";
+        }
+        std::cout << std::endl;
+        std::cout << "Data of Ref output  : ";
+        for (int j = 0; j < s; j++) {  // std::min(50, size)
+            std::cout << static_cast<float>(refOutput[j]) << " ";
+        }
+        std::cout << std::endl;
+    }
+    x++;
+    int nan_cnt = 0;
+    for (int j = 0; j < size; j++) {
+        if (std::isnan(msTensorData[j]) || std::isinf(msTensorData[j])) {
+            // std::cerr << "Output tensor has nan or inf data, compare fail" << std::endl;
+            // FT_LOG_ERROR("Output tensor has nan or inf data, compare fail\n");
+            // return RET_ERROR;
+            // return -1;
+            nan_cnt++;
+            continue;
+        }
+
+        auto tolerance = absoluteTolerance + relativeTolerance * fabs(refOutput[j]);
+        auto absoluteError = std::fabs(static_cast<float>(msTensorData[j]) - static_cast<float>(refOutput[j]));
+        if (absoluteError > tolerance) {
+            if (fabs(refOutput[j]) == 0) {
+                if (absoluteError > 1e-5) {
+                    meanError += absoluteError;
+                    errorCount++;
+                }
+                else {
+                    continue;
+                }
+            }
+            else {
+                //if (absoluteError > 1e-2) std::cout << "idx=" <<j << std::endl;
+                // just assume that atol = rtol
+                meanError += absoluteError / (fabs(refOutput[j]) + FLT_MIN);
+                errorCount++;
+            }
+        }
+    }
+    std::cout << "nan_cnt is " << nan_cnt << " size " << size <<std::endl;
+    if (meanError > 0.0f) {
+        meanError /= errorCount;
+    }
+    if (meanError <= 0.0000001) {
+        std::cout << "Mean bias of tensor: 0%" << std::endl;
+    }
+    else {
+        std::cout << "Mean bias of tensor: " << meanError * 100 << "%" << std::endl;
+    }
+    std::cout << std::endl;
+    return meanError;
+}
+template<typename T>
+int CompareOutput(std::vector<Tensor> output_python_tensors, std::vector<Tensor> output_tensors)
+{
+    float total_bias = 0;
+    int total_size = 0;
+    float accuracy_threshold_ = 0.5f;
+    bool has_error = false;
+    for (size_t i = 0; i < output_tensors.size(); i++) {
+        float size = 1;
+        for (size_t j = 0; j < output_tensors[i].shape.size(); j++) {
+            size *= output_tensors[i].shape[j];
+        }
+        T* output_device = (T*)output_tensors[i].data;
+        T* output_host = (T*)malloc(size * sizeof(T));
+        cudaD2Hcpy(output_host, output_device, size);
+        float bias = CompareData((T*)output_python_tensors[i].data, size, output_host);
+        free(output_host);
+        if (bias >= 0) {
+            total_bias += bias;
+            total_size++;
+        }
+        else {
+            has_error = true;
+            break;
+        }
+    }
+    if (!has_error) {
+        float mean_bias;
+        if (total_size != 0) {
+            mean_bias = total_bias / total_size * 100;
+        }
+        else {
+            mean_bias = 0;
+        }
+
+        std::cout << "Mean bias of all nodes/tensors: " << mean_bias << "%"
+                  << " threshold is:" << accuracy_threshold_ << std::endl;
+        std::cout << "=======================================================" << std::endl << std::endl;
+
+        if (mean_bias > accuracy_threshold_) {
+            FT_LOG_INFO("Mean bias of all nodes/tensors is too big: %f %", mean_bias);
+            std::cout << "Mean bias of all nodes/tensors is too big: " << mean_bias << "%" << std::endl;
+            return -9;
+        }
+        else {
+            return 0;
+        }
+    }
+    else {
+        FT_LOG_ERROR("Error in CompareData");
+        std::cerr << "Error in CompareData" << std::endl;
+        std::cout << "=======================================================" << std::endl << std::endl;
+        return -1;
+    }
+}
+
+void FreeDesc(std::vector<Tensor>& desc)
+{
+    for (size_t i = 0; i < desc.size(); i++) {
+        if (desc[i].where == MEMORY_GPU) {
+            cudaFree((float*)desc[i].data);
+        }
+        else if (desc[i].where == MEMORY_CPU) {
+            free((float*)desc[i].data);
+        }
+    }
+}
+
+uint64_t GetTimeUs()
+{
+    const int USEC = 1000000;
+    const int MSEC = 1000;
+    struct timespec ts = {0, 0};
+    if (clock_gettime(CLOCK_MONOTONIC, &ts) != 0) {
+        return 0;
+    }
+    uint64_t retval = (uint64_t)((ts.tv_sec * USEC) + (ts.tv_nsec / MSEC));
+    return retval;
+}
+
+template<typename T, typename U = T, typename S = T>
+int MsExample(opt_arg* opt_a)
+{
+    printf("[INFO] Device: %s \n", getDeviceName().c_str());
+
+    cudaStream_t stream;
+    cublasHandle_t cublas_handle;
+    cublasLtHandle_t cublaslt_handle;
+    cudaStreamCreate(&stream);
+    cublasCreate(&cublas_handle);
+    cublasLtCreate(&cublaslt_handle);
+#ifdef SPARSITY_ENABLED
+    cusparseLtHandle_t cusparselt_handle;
+    CHECK_CUSPARSE(cusparseLtInit(&cusparselt_handle));
+#endif
+    cublasSetStream(cublas_handle, stream);
+    cublasAlgoMap* cublas_algo_map = new cublasAlgoMap("gemm_config.in", "");
+
+    Allocator<AllocatorType::CUDA> allocator(getDevice());
+
+    std::mutex* cublas_wrapper_mutex = new std::mutex();
+#ifdef SPARSITY_ENABLED
+    cublasMMWrapper cublas_wrapper = cublasMMWrapper(
+        cublas_handle, cublaslt_handle, cusparselt_handle, stream, cublas_algo_map, cublas_wrapper_mutex, &allocator);
+#else
+    cublasMMWrapper cublas_wrapper =
+        cublasMMWrapper(cublas_handle, cublaslt_handle, stream, cublas_algo_map, cublas_wrapper_mutex, &allocator);
+#endif
+    if (std::is_same<T, half>::value) {
+        if (std::is_same<U, float>::value) {
+            cublas_wrapper.setFP16MixedGemmConfig();
+        }
+        else {
+            cublas_wrapper.setFP16GemmConfig();
+        }
+    }
+    else if (std::is_same<T, float>::value) {
+        if (std::is_same<U, half>::value) {
+            cublas_wrapper.setFP32MixedGemmConfig();
+        }
+        else {
+            cublas_wrapper.setFP32GemmConfig();
+        }
+    }
+
+    if (opt_a->model_name != "transformer_encoder_layer") {
+        DecriptorTest<T, U, S> desc;
+        Init<T, U, S>(opt_a, desc, stream, &cublas_wrapper, &allocator);
+        int res = ReadTensors<T>(desc.input_tensors, std::string("input"), opt_a);
+        FT_CHECK(!res);
+        res = ReadTensors<T>(desc.input_python_tensors, std::string("input"), opt_a);
+        FT_CHECK(!res);
+
+        res = ReadTensors<T>(desc.output_tensors, std::string("output"), opt_a, false);
+        FT_CHECK(!res);
+
+        res = ReadTensors<T>(desc.output_python_tensors, std::string("output"), opt_a);
+        FT_CHECK(!res);
+
+        res = ReadTensors<U>(desc.w_tensors, std::string("weight"), opt_a);
+        FT_CHECK(!res);
+
+        std::cout << "inputs size not encoder: " << CalcTensorsSize<T>(desc.input_tensors) << std::endl;
+        std::cout << "weights size not encoder: " << CalcTensorsSize<T>(desc.w_tensors) << std::endl;
+        std::cout << "ouputs size not encoder: " << CalcTensorsSize<T>(desc.output_tensors) << std::endl;
+
+        AttentionWeight<U> attn_weights;
+        InitWeight<U>(opt_a, attn_weights, desc.w_tensors);
+
+        // test for BE !!
+        desc.Attn->forward(&desc.output_tensors, &desc.input_tensors, &attn_weights);
+
+
+        CompareOutput<T>(desc.output_python_tensors, desc.output_tensors);
+        
+// #define DO_TIME
+// #ifdef DO_TIME
+//         // warmup
+//         for (int i = 0; i < 10; i++) {
+//             desc.Attn->forward(&desc.output_tensors, &desc.input_tensors, &attn_weights);
+//         }
+//         // profile time
+//         const int ite = 1000;
+//         CudaTimer cuda_timer(stream);
+//         cuda_timer.start();
+
+//         for (int i = 0; i < ite; i++) {
+//             for (int i = 0; i < desc.input_tensors.size(); i++) {
+//                 int size = desc.input_tensors[i].size();
+//                 cudaH2Dcpy(const_cast<T*>(reinterpret_cast<const T*>(desc.input_tensors[i].data)),
+//                            const_cast<T*>(reinterpret_cast<const T*>(desc.input_python_tensors[i].data)),
+//                            size);
+//             }
+
+//             desc.Attn->forward(&desc.output_tensors, &desc.input_tensors, &attn_weights);
+//             for (int i = 0; i < desc.output_tensors.size(); i++) {
+//                 int size = desc.output_tensors[i].size();
+//                 cudaD2Hcpy(const_cast<T*>(reinterpret_cast<const T*>(desc.output_python_tensors[i].data)),
+//                            const_cast<T*>(reinterpret_cast<const T*>(desc.output_tensors[i].data)),
+//                            size);
+//             }
+//         }
+//         float total_time = cuda_timer.stop();
+//         printf("batch_size %ld seq_len %ld layer %ld "
+//                "AVG FT-CPP-time %.2f ms (%d iterations) "
+//                "Total Time %.2f ms\n",
+//                opt_a->batch_size,
+//                opt_a->seq_len,
+//                opt_a->num_layers,
+//                total_time / ite,
+//                ite,
+//                total_time);
+// #endif
+
+#ifdef SPARSITY_ENABLED
+        cusparseLtDestroy(&cusparselt_handle);
+#endif
+        delete cublas_algo_map;
+        delete cublas_wrapper_mutex;
+        FreeDesc(desc.output_tensors);
+        FreeDesc(desc.input_tensors);
+        FreeDesc(desc.output_python_tensors);
+        FreeDesc(desc.w_tensors);
+    }
+    else {
+        DecriptorEncoderLayer<T, U, S> desc;
+        InitE<T, U, S>(opt_a, desc, stream, &cublas_wrapper, &cublas_handle, &allocator);
+        int res = ReadTensors<T>(desc.input_tensors, std::string("input"), opt_a);
+        FT_CHECK(!res);
+        res = ReadTensors<T>(desc.input_python_tensors, std::string("input"), opt_a);
+        FT_CHECK(!res);
+
+        res = ReadTensors<T>(desc.output_tensors, std::string("output"), opt_a, false);
+        FT_CHECK(!res);
+
+        res = ReadTensors<T>(desc.output_python_tensors, std::string("output"), opt_a);
+        FT_CHECK(!res);
+
+        res = ReadTensors<U>(desc.w_tensors, std::string("weight"), opt_a);
+        FT_CHECK(!res);
+        EncoderLayerWeight<U> encoder_weights;
+        InitWeightEncoder<U>(opt_a, encoder_weights, desc.w_tensors);
+        // test for BE !!
+        desc.Encoder->forward(&desc.output_tensors, &desc.input_tensors, &encoder_weights);
+
+        CompareOutput<T>(desc.output_python_tensors, desc.output_tensors);
+// #define DO_TIME
+#ifdef DO_TIME
+        // warmup
+        for (int i = 0; i < 10; i++) {
+            desc.Encoder->forward(&desc.output_tensors, &desc.input_tensors, &encoder_weights);
+        }
+        // profile time
+        const int ite = 1000;
+        CudaTimer cuda_timer(stream);
+        cuda_timer.start();
+
+        for (int i = 0; i < ite; i++) {
+            // for (int i = 0; i < desc.input_tensors.size(); i++) {
+            //     int size = desc.input_tensors[i].size();
+            //     cudaH2Dcpy(const_cast<T*>(reinterpret_cast<const T*>(desc.input_tensors[i].data)),
+            //                const_cast<T*>(reinterpret_cast<const T*>(desc.input_python_tensors[i].data)),
+            //                size);
+            // }
+
+            desc.Encoder->forward(&desc.output_tensors, &desc.input_tensors, &encoder_weights);
+            // for (int i = 0; i < desc.output_tensors.size(); i++) {
+            //     int size = desc.output_tensors[i].size();
+            //     cudaD2Hcpy(const_cast<T*>(reinterpret_cast<const T*>(desc.output_python_tensors[i].data)),
+            //                const_cast<T*>(reinterpret_cast<const T*>(desc.output_tensors[i].data)),
+            //                size);
+            // }
+        }
+        float total_time = cuda_timer.stop();
+
+        printf("batch_size %ld seq_len %ld layer %ld "
+               "AVG FT-CPP-time %.2f ms (%d iterations) "
+               "Total Time %.2f ms\n",
+               opt_a->batch_size,
+               opt_a->seq_len,
+               opt_a->num_layers,
+               total_time / ite,
+               ite,
+               total_time);
+#endif
+
+#ifdef SPARSITY_ENABLED
+        cusparseLtDestroy(&cusparselt_handle);
+#endif
+        delete cublas_algo_map;
+        delete cublas_wrapper_mutex;
+        FreeDesc(desc.output_tensors);
+        FreeDesc(desc.input_tensors);
+        FreeDesc(desc.output_python_tensors);
+        FreeDesc(desc.w_tensors);
+    }
+    return 0;
+}
diff --git a/examples/pytorch/swin/Swin-Transformer-Quantization/SwinTransformer b/examples/pytorch/swin/Swin-Transformer-Quantization/SwinTransformer
new file mode 160000
index 0000000..cbaa0d8
--- /dev/null
+++ b/examples/pytorch/swin/Swin-Transformer-Quantization/SwinTransformer
@@ -0,0 +1 @@
+Subproject commit cbaa0d8707db403d85ad0e13c59f2f71cd6db425
diff --git a/examples/pytorch/vit/ViT-quantization/ViT-pytorch b/examples/pytorch/vit/ViT-quantization/ViT-pytorch
new file mode 160000
index 0000000..460a162
--- /dev/null
+++ b/examples/pytorch/vit/ViT-quantization/ViT-pytorch
@@ -0,0 +1 @@
+Subproject commit 460a162767de1722a014ed2261463dbbc01196b6
diff --git a/path.sh b/path.sh
new file mode 100755
index 0000000..53f5ca6
--- /dev/null
+++ b/path.sh
@@ -0,0 +1 @@
+export PATH=/usr/local/cuda-11/bin:/home/yoni/.vscode-server/bin/4af164ea3a06f701fe3e89a2bcbb421d2026b68f/bin/remote-cli:/home/yoni/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
diff --git a/src/fastertransformer/kernels/CMakeLists.txt b/src/fastertransformer/kernels/CMakeLists.txt
index 3db0830..3dd4210 100644
--- a/src/fastertransformer/kernels/CMakeLists.txt
+++ b/src/fastertransformer/kernels/CMakeLists.txt
@@ -159,9 +159,12 @@ add_library(matrix_vector_multiplication STATIC matrix_vector_multiplication.cu)
 set_property(TARGET matrix_vector_multiplication PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET matrix_vector_multiplication PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
 
-add_library(custom_ar_kernels STATIC custom_ar_kernels.cu)
-set_property(TARGET custom_ar_kernels PROPERTY POSITION_INDEPENDENT_CODE  ON)
-set_property(TARGET custom_ar_kernels PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
+if(${SM} GREATER_EQUAL 70)
+    message("-- Making custom kernels")
+    add_library(custom_ar_kernels STATIC custom_ar_kernels.cu)
+    set_property(TARGET custom_ar_kernels PROPERTY POSITION_INDEPENDENT_CODE  ON)
+    set_property(TARGET custom_ar_kernels PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
+endif()
 
 add_library(vit_kernels STATIC vit_kernels.cu)
 set_property(TARGET vit_kernels PROPERTY POSITION_INDEPENDENT_CODE  ON)
diff --git a/src/fastertransformer/kernels/activation_kernels.cu b/src/fastertransformer/kernels/activation_kernels.cu
index 7ff8e0f..e1be64c 100644
--- a/src/fastertransformer/kernels/activation_kernels.cu
+++ b/src/fastertransformer/kernels/activation_kernels.cu
@@ -201,12 +201,21 @@ __global__ void add_bias(H_T* out, const B_T* __restrict bias, int m, int n)
     }
 }
 
+template<typename H_T, typename B_T>
+__global__ void add_bias_basic(H_T* out, const B_T* __restrict bias, int m, int n)
+{
+    for (int id = blockIdx.x * blockDim.x + threadIdx.x; id < m * n; id += blockDim.x * gridDim.x) {
+        out[id] = out[id] + (H_T)ldg(&bias[id % n]);
+    }
+}
+
 template<>
 __global__ void add_bias(half* out, const half* __restrict bias, int m, int n)
 {
     half2* out_ptr = (half2*)out;
     const half2* bias_ptr = (half2*)bias;
-    for (int id = blockIdx.x * blockDim.x + threadIdx.x; id < m * n; id += blockDim.x * gridDim.x) {
+    int id = blockIdx.x * blockDim.x + threadIdx.x;
+    for (; id < m * n; id += blockDim.x * gridDim.x) {
         out_ptr[id] = out_ptr[id] + __ldg(&bias_ptr[id % n]);
     }
 }
@@ -228,15 +237,29 @@ void invokeAddBias(H_T* out, const B_T* bias, const int m, const int n, cudaStre
 {
     const int data_type_factor = 4 / sizeof(H_T);  // 1 for fp32, 2 for fp16 and bf16
     dim3 block, grid;
-    if (n / 4 / data_type_factor <= 1024) {
-        block.x = n / 4 / data_type_factor;
-        grid.x = m;
-    }
-    else {
-        block.x = 1024;
-        grid.x = ceil(m * n / 1024.);
+
+    bool reminder = (data_type_factor != 1) ? (n % data_type_factor) : false;
+    if (reminder) {
+        if (n / 4 <= 1024) {
+            block.x = n / 4;
+            grid.x = m;
+        }
+        else {
+            block.x = 1024;
+            grid.x = ceil(m * n / 1024.);
+        }
+        add_bias_basic<<<grid, block, 0, stream>>>(out, bias, m, n);
+    } else {
+        if (n / 4 / data_type_factor <= 1024) {
+            block.x = n / 4 / data_type_factor;
+            grid.x = m;
+        }
+        else {
+            block.x = 1024;
+            grid.x = ceil(m * n / 1024.);
+        }
+        add_bias<<<grid, block, 0, stream>>>(out, bias, m, (n / data_type_factor));
     }
-    add_bias<<<grid, block, 0, stream>>>(out, bias, m, n / data_type_factor);
 }
 
 template void invokeAddBias(float* out, const float* bias, const int m, const int n, cudaStream_t stream);
diff --git a/src/fastertransformer/kernels/add_residual_kernels.cu b/src/fastertransformer/kernels/add_residual_kernels.cu
index 4cd9f0f..1bf2be3 100644
--- a/src/fastertransformer/kernels/add_residual_kernels.cu
+++ b/src/fastertransformer/kernels/add_residual_kernels.cu
@@ -29,6 +29,18 @@ __global__ void addBiasResidual(T* output, const T* input, const T* bias, const
     }
 }
 
+template<typename T, typename S, typename U>
+__global__ void addBiasResidualCast(U* output, const T* input, T* out, const T* bias, const int m, const int n)
+{
+    S *out_cast = (S*)out;
+    const int col_index = blockIdx.y * blockDim.x + threadIdx.x;
+    if (col_index < n) {
+        T bias_val = (bias == nullptr) ? (T)(0.0f) : bias[col_index];
+        out_cast[blockIdx.x * n + col_index] =
+            (S)((T)output[blockIdx.x * n + col_index] + (T)input[blockIdx.x * n + col_index] + bias_val);
+    }
+}
+
 template<typename T>
 void invokeAddBiasResidual(T* output, const T* input, const T* bias, const int m, const int n, cudaStream_t stream)
 {
@@ -38,6 +50,20 @@ void invokeAddBiasResidual(T* output, const T* input, const T* bias, const int m
     addBiasResidual<<<grid, block, 0, stream>>>(output, input, bias, m, n);
 }
 
+template<typename T, typename S, typename U>
+void invokeAddBiasResidualCast(U* output, const T* input, T* out, const T* bias, const int m, const int n, cudaStream_t stream)
+{
+    int blocks_per_row = ceil(float(n) / 1024);
+    dim3 grid(m, blocks_per_row);
+    dim3 block(min(n, 1024));
+    addBiasResidualCast<T, S, U><<<grid, block, 0, stream>>>(output, input, out, bias, m, n);
+}
+
+template void invokeAddBiasResidualCast<float, half, half>(half* output, const float* input, float* out, const float* bias, const int m, const int n, cudaStream_t stream);
+template void invokeAddBiasResidualCast<float, half, float>(float* output, const float* input, float* out, const float* bias, const int m, const int n, cudaStream_t stream);
+template void invokeAddBiasResidualCast<float, float, float>(float* output, const float* input, float* out, const float* bias, const int m, const int n, cudaStream_t stream);
+template void invokeAddBiasResidualCast<float, float, half>(half* output, const float* input, float* out, const float* bias, const int m, const int n, cudaStream_t stream);
+
 template<typename T>
 __global__ void addBiasAttentionFfnResidual(T* block_output,
                                             const T* ffn_output,
@@ -88,11 +114,9 @@ void invokeAddBiasAttentionFfnResidual(T* block_output,
     }
 }
 
-template void invokeAddBiasResidual(
-    float* output, const float* input, const float* bias, const int m, const int n, cudaStream_t stream);
+template void invokeAddBiasResidual<float>(float *output, const float *input, const float *bias, int m, int n, cudaStream_t stream);
+template void invokeAddBiasResidual<half>(half *output, const half *input, const half *bias, int m, int n, cudaStream_t stream);
 
-template void
-invokeAddBiasResidual(half* output, const half* input, const half* bias, const int m, const int n, cudaStream_t stream);
 
 #ifdef ENABLE_BF16
 template void invokeAddBiasResidual(__nv_bfloat16* output,
diff --git a/src/fastertransformer/kernels/add_residual_kernels.h b/src/fastertransformer/kernels/add_residual_kernels.h
index edd8179..7ab8eb4 100644
--- a/src/fastertransformer/kernels/add_residual_kernels.h
+++ b/src/fastertransformer/kernels/add_residual_kernels.h
@@ -27,6 +27,9 @@ namespace fastertransformer {
 template<typename T>
 void invokeAddBiasResidual(T* output, const T* input, const T* bias, const int m, const int n, cudaStream_t stream);
 
+template<typename T>
+void invokeAddBiasResidual(T* output, const T* input, const T* bias, const int m, const int n, int max_seq, const int *sequent_len, cudaStream_t stream);
+
 template<typename T>
 void invokeT5AddResidual(T* output, const T* input, const int m, const int n, cudaStream_t stream);
 
@@ -65,4 +68,8 @@ void invokeAddBiasResidualCol32(T* output,
                                 const float* input1_amax_ptr,
                                 const int scale_is_vector = 0);
 
+template<typename T, typename S, typename U>
+void invokeAddBiasResidualCast(U* output, const T* input, T* out, const T* bias, const int m, const int n, cudaStream_t stream);
+
 }  // namespace fastertransformer
+
diff --git a/src/fastertransformer/kernels/bert_preprocess_kernels.cu b/src/fastertransformer/kernels/bert_preprocess_kernels.cu
index c855fa1..9976d50 100644
--- a/src/fastertransformer/kernels/bert_preprocess_kernels.cu
+++ b/src/fastertransformer/kernels/bert_preprocess_kernels.cu
@@ -14,10 +14,12 @@
  * limitations under the License.
  */
 
+#include "reduce_kernel_utils.cuh"
 #include "bert_preprocess_kernels.h"
 
 namespace fastertransformer {
 
+
 __global__ void getPaddingOffsetKernel(size_t* valid_word_num,
                                        int* tmp_mask_offset,
                                        const int* sequence_length,
@@ -55,6 +57,31 @@ void invokeGetPaddingOffset(size_t* h_token_num,
     sync_check_cuda_error();
 }
 
+template<typename T>
+__global__ void buildSequnceLength(const T * input, int *sequnce_length, const int max_seq_length, const int hidden_size) {
+     __shared__ int s_max_val;
+    int bid = blockIdx.x;
+    const T * seq_base = input + bid* max_seq_length * hidden_size;
+    const T zero = static_cast<T>(0.f);
+    int last = -max_seq_length;
+    for (int i=max_seq_length - 1 - threadIdx.x; i >= 0; i -= blockDim.x) {
+        const T * seq_ptr = seq_base + i * hidden_size;
+        if ((seq_ptr[0] == zero) && (seq_ptr[1] == zero)) {
+            last = -i;
+        }
+    }
+    int max_val = blockReduceMax<int>(last);
+    if (threadIdx.x == 0) {
+            s_max_val = max_val;
+    }
+    __syncthreads();
+    sequnce_length[bid] = -s_max_val;
+}
+
+
+
+
+
 template<typename T>
 __global__ void buildEncoderAttentionMaskKernel(T* attention_mask, const int* sequence_lengths, const int max_seq_len)
 {
@@ -67,15 +94,15 @@ __global__ void buildEncoderAttentionMaskKernel(T* attention_mask, const int* se
         int col_id = i % max_seq_len;
         // if (row_id < length && col_id < length) {
         // TODO (bhsueh) check this modification is ok or not on other rmodel
-        if (col_id < length) {
-            attention_mask[i] = (T)(1.0f);
-        }
-        else {
+        if (col_id >= length)  {
             attention_mask[i] = (T)(0.0f);
         }
     }
 }
 
+
+
+
 template<typename T>
 void invokeBuildEncoderAttentionMask(
     T* attention_mask, const int* sequence_lengths, const int batch_size, const int max_seq_len, cudaStream_t stream)
@@ -113,6 +140,18 @@ __global__ void getTrtPaddingOffsetKernel(int* trt_mha_padding_offset, const int
     }
 }
 
+
+
+template <typename T>
+void invokeBuildSequnceLength(const T * input, int batch_size, int *sequnce_length, int max_seq_length, int hidden_size,cudaStream_t stream) {
+    buildSequnceLength<<<batch_size,256,0,stream>>>(input,sequnce_length, max_seq_length,hidden_size);
+}
+
+
+
+
+
+
 void invokeGetTrtPaddingOffset(int* trt_mha_padding_offset,
                                const int* sequence_length,
                                const int batch_size,
@@ -300,6 +339,9 @@ void invokeBuildRelativeAttentionBias(T* relative_attention_bias,
                                                            is_bidirectional,
                                                            max_distance);
 }
+template void invokeBuildSequnceLength(const float * input, int batch_size, int *sequnce_length, int max_seq_length, int hidden_size,cudaStream_t stream);
+template void invokeBuildSequnceLength(const half * input, int batch_size, int *sequnce_length, int max_seq_length, int hidden_size,cudaStream_t stream);
+
 
 template void invokeBuildRelativeAttentionBias(float* relative_attention_bias,
                                                const float* relative_attention_bias_table,
diff --git a/src/fastertransformer/kernels/bert_preprocess_kernels.h b/src/fastertransformer/kernels/bert_preprocess_kernels.h
index dcb8f85..dca4ef5 100644
--- a/src/fastertransformer/kernels/bert_preprocess_kernels.h
+++ b/src/fastertransformer/kernels/bert_preprocess_kernels.h
@@ -34,6 +34,9 @@ template<typename T>
 void invokeBuildEncoderAttentionMask(
     T* attention_mask, const int* sequence_lengths, const int batch_size, const int max_seq_len, cudaStream_t stream);
 
+template <typename T>
+void invokeBuildSequnceLength(const T * input, int batch_size, int *sequnce_length, int max_seq_length, int hidden_size,cudaStream_t stream);
+
 void invokeGetTrtPaddingOffset(int* trt_mha_padding_offset,
                                const int* sequence_length,
                                const int request_batch_size,
diff --git a/src/fastertransformer/kernels/layernorm_kernels.cu b/src/fastertransformer/kernels/layernorm_kernels.cu
index 96a090e..1076baf 100644
--- a/src/fastertransformer/kernels/layernorm_kernels.cu
+++ b/src/fastertransformer/kernels/layernorm_kernels.cu
@@ -29,7 +29,8 @@ __global__ void generalAddBiasResidualLayerNormOpt(T* normed_output,
                                                    const T* __restrict gamma,
                                                    const T* __restrict beta,
                                                    int m,
-                                                   int n)
+                                                   int n,
+                                                   float eps)
 {
     __shared__ float s_mean;
     __shared__ float s_variance;
@@ -74,7 +75,7 @@ __global__ void generalAddBiasResidualLayerNormOpt(T* normed_output,
     variance = blockReduceSum(local_var_sum);
 
     if (threadIdx.x == 0) {
-        s_variance = rsqrtf(variance / n / 2 + 1e-6f);
+        s_variance = rsqrtf(variance / n / 2 + eps);
     }
     __syncthreads();
 
@@ -100,7 +101,8 @@ __global__ void generalAddBiasResidualLayerNormOpt2(T* normed_output,
                                                     const T* __restrict gamma,
                                                     const T* __restrict beta,
                                                     int m,
-                                                    int n)
+                                                    int n,
+                                                    float eps)
 {
     __shared__ float s_mean;
     __shared__ float s_variance;
@@ -145,7 +147,7 @@ __global__ void generalAddBiasResidualLayerNormOpt2(T* normed_output,
 
     if (threadIdx.x == 0) {
         s_mean = sums[0] / n / 2;
-        s_variance = rsqrtf(sums[1] / n / 2 - s_mean * s_mean + 1e-6f);
+        s_variance = rsqrtf(sums[1] / n / 2 - s_mean * s_mean + eps);
     }
     __syncthreads();
 
@@ -166,7 +168,7 @@ __global__ void generalAddBiasResidualLayerNormOpt2(T* normed_output,
 // TODO(bhsueh) add half2 implementation
 template<typename T, int N>
 __global__ void
-addBiasResidualPostLayerNorm(T* out, const T* input, const T* bias, const T* gamma, const T* beta, int m, int n)
+addBiasResidualPostLayerNorm(T* out, const T* input, const T* bias, const T* gamma, const T* beta, int m, int n, float eps)
 {
     __shared__ float s_mean;
     __shared__ float s_variance;
@@ -197,7 +199,7 @@ addBiasResidualPostLayerNorm(T* out, const T* input, const T* bias, const T* gam
     }
     variance = blockReduceSum<float>(variance);
     if (threadIdx.x == 0) {
-        s_variance = variance / n + 1e-6f;
+        s_variance = variance / n + eps;
     }
     __syncthreads();
 
@@ -210,9 +212,63 @@ addBiasResidualPostLayerNorm(T* out, const T* input, const T* bias, const T* gam
     }
 }
 
+template<typename T, typename S, typename D, int N>
+__global__ void addBiasResidualPostLayerNormCast(S* attn_output,
+                                                 D* norm_attn_out,
+                                                 const S* __restrict input,
+                                                 const T* __restrict bias,
+                                                 const T* __restrict gamma,
+                                                 const T* __restrict beta,
+                                                 int m, 
+                                                 int n,
+                                                 float eps)
+{
+    __shared__ float s_mean;
+    __shared__ float s_variance;
+    float mean = 0.0f;
+    float variance = 0.0f;
+    float local_out_cache[N];
+
+#pragma unroll N
+    for (int idx = threadIdx.x, i = 0; idx < n && i < N; ++i) {
+        float local_out = (float)((T)attn_output[blockIdx.x * n + idx] + (T)input[blockIdx.x * n + idx] + (T)__ldg(&bias[idx]));
+        mean += local_out;
+        // save local_out to local_out_cache to save some recompute
+        local_out_cache[i] = local_out;
+        idx += blockDim.x;
+    }
+
+    mean = blockReduceSum<float>(mean);
+    if (threadIdx.x == 0) {
+        s_mean = mean / n;
+    }
+    __syncthreads();
+
+#pragma unroll N
+    for (int idx = threadIdx.x, i = 0; idx < n && i < N; ++i) {
+        float local_out = local_out_cache[i];
+        variance += (local_out - s_mean) * (local_out - s_mean);
+        idx += blockDim.x;
+    }
+    variance = blockReduceSum<float>(variance);
+    if (threadIdx.x == 0) {
+        s_variance = variance / n + eps;
+    }
+    __syncthreads();
+
+#pragma unroll N
+    for (int idx = threadIdx.x, i = 0; idx < n && i < N; ++i) {
+        float local_out = local_out_cache[i];
+        norm_attn_out[blockIdx.x * n + idx] =
+            (D)(((local_out - s_mean) * rsqrtf(s_variance)) * (float)(__ldg(&gamma[idx])) + (float)(__ldg(&beta[idx])));
+        idx += blockDim.x;
+    }
+}
+
+
 template<int N>
 __global__ void addBiasResidualPostLayerNormHalf(
-    half* out, const half* input, const half* bias, const half* gamma, const half* beta, int m, int n)
+    half* out, const half* input, const half* bias, const half* gamma, const half* beta, int m, int n, float eps)
 {
     __shared__ float s_mean;
     __shared__ float s_variance;
@@ -255,7 +311,7 @@ __global__ void addBiasResidualPostLayerNormHalf(
 
     variance = blockReduceSum<float>(variance);
     if (threadIdx.x == 0) {
-        s_variance = rsqrtf(variance / n + 1e-6f);
+        s_variance = rsqrtf(variance / n + eps);
     }
     __syncthreads();
 
@@ -274,7 +330,7 @@ __global__ void addBiasResidualPostLayerNormHalf(
 
 template<typename T>
 __global__ void
-generalAddBiasResidualPostLayerNorm(T* out, const T* input, const T* bias, const T* gamma, const T* beta, int m, int n)
+generalAddBiasResidualPostLayerNorm(T* out, const T* input, const T* bias, const T* gamma, const T* beta, int m, int n, float eps)
 {
     __shared__ float s_mean;
     __shared__ float s_variance;
@@ -300,7 +356,7 @@ generalAddBiasResidualPostLayerNorm(T* out, const T* input, const T* bias, const
     }
     variance = blockReduceSum<float>(variance);
     if (threadIdx.x == 0) {
-        s_variance = variance / n + 1e-6f;
+        s_variance = variance / n + eps;
     }
     __syncthreads();
 
@@ -311,9 +367,55 @@ generalAddBiasResidualPostLayerNorm(T* out, const T* input, const T* bias, const
     }
 }
 
+template<typename T, typename S, typename D>
+__global__ void generalAddBiasResidualPostLayerNormCast(S* attn_output,
+                                                        D* norm_attn_out,
+                                                        const S* __restrict input,
+                                                        const T* __restrict bias,
+                                                        const T* __restrict gamma,
+                                                        const T* __restrict beta,
+                                                        int m,
+                                                        int n, 
+                                                        float eps)
+{
+    __shared__ float s_mean;
+    __shared__ float s_variance;
+    float mean = 0.0f;
+    float variance = 0.0f;
+
+    for (int idx = threadIdx.x; idx < n; idx += blockDim.x) {
+        float local_out = (float)((T)attn_output[blockIdx.x * n + idx] + (T)input[blockIdx.x * n + idx] + (T)__ldg(&bias[idx]));
+        mean += local_out;
+        // save local_out to out to save some recompute
+        attn_output[blockIdx.x * n + idx] = (T)local_out;
+    }
+
+    mean = blockReduceSum<float>(mean);
+    if (threadIdx.x == 0) {
+        s_mean = mean / n;
+    }
+    __syncthreads();
+
+    for (int idx = threadIdx.x; idx < n; idx += blockDim.x) {
+        float local_out = (T)attn_output[blockIdx.x * n + idx];
+        variance += (local_out - s_mean) * (local_out - s_mean);
+    }
+    variance = blockReduceSum<float>(variance);
+    if (threadIdx.x == 0) {
+        s_variance = variance / n + eps;
+    }
+    __syncthreads();
+
+    for (int idx = threadIdx.x; idx < n; idx += blockDim.x) {
+        float local_out = attn_output[blockIdx.x * n + idx];
+        norm_attn_out[blockIdx.x * n + idx] =
+            (D)(((local_out - s_mean) * rsqrtf(s_variance)) * (float)(__ldg(&gamma[idx])) + (float)(__ldg(&beta[idx])));
+    }
+}
+
 template<>
 __global__ void generalAddBiasResidualPostLayerNorm(
-    half* out, const half* input, const half* bias, const half* gamma, const half* beta, int m, int n)
+    half* out, const half* input, const half* bias, const half* gamma, const half* beta, int m, int n, float eps)
 {
     __shared__ float s_mean;
     __shared__ float s_variance;
@@ -352,7 +454,7 @@ __global__ void generalAddBiasResidualPostLayerNorm(
 
     variance = blockReduceSum<float>(variance);
     if (threadIdx.x == 0) {
-        s_variance = rsqrtf(variance / n + 1e-6f);
+        s_variance = rsqrtf(variance / n + eps);
     }
     __syncthreads();
 
@@ -373,7 +475,8 @@ __global__ void addBiasResidualPostLayerNormV2(T* out,
                                                const T* __restrict bias,
                                                const T* __restrict gamma,
                                                const T* __restrict beta,
-                                               int n)
+                                               int n,
+                                               float eps)
 {
     const int ite = 4;
     const int tid = threadIdx.x;
@@ -409,7 +512,7 @@ __global__ void addBiasResidualPostLayerNormV2(T* out,
 
     variance = blockReduceSum<float>(var);
     if (tid == 0) {
-        s_variance = rsqrtf(variance / n + 1e-6f);
+        s_variance = rsqrtf(variance / n + eps);
     }
     __syncthreads();
 
@@ -428,7 +531,8 @@ __global__ void addBiasResidualPostLayerNormV2(half* out,
                                                const half* __restrict bias,
                                                const half* __restrict gamma,
                                                const half* __restrict beta,
-                                               int n)
+                                               int n,
+                                               float eps)
 {
     const int ite = 4;
     const int tid = threadIdx.x;
@@ -473,7 +577,7 @@ __global__ void addBiasResidualPostLayerNormV2(half* out,
 
     variance = blockReduceSum<float>(var);
     if (threadIdx.x == 0) {
-        s_variance = rsqrtf(variance / n + 1e-6f);
+        s_variance = rsqrtf(variance / n + eps);
     }
     __syncthreads();
 
@@ -486,26 +590,140 @@ __global__ void addBiasResidualPostLayerNormV2(half* out,
     }
 }
 
+template<typename T, typename S, typename D>
+__global__ void addBiasResidualPostLayerNormV2Cast(S* attn_output,
+                                                   D* norm_attn_out,
+                                                   const S* __restrict input,
+                                                   const T* __restrict bias,
+                                                   const T* __restrict gamma,
+                                                   const T* __restrict beta,
+                                                   int n, 
+                                                   float eps)
+{
+    const int ite = 4;
+    const int tid = threadIdx.x;
+    const int bid = blockIdx.x;
+
+    __shared__ float s_mean;
+    __shared__ float s_variance;
+    float mean = 0.0f;
+    float variance = 0.0f;
+    float local_out[ite];
+
+    float sum = 0.0f;
+#pragma unroll
+    for (int i = 0; i < ite; i++) {
+        int col_id = i * blockDim.x + tid;
+        int id = bid * n + col_id;
+        local_out[i] = (float)((T)(attn_output[id]) + (T)__ldg(&input[id]) + (T)__ldg(&bias[col_id]));
+        sum += local_out[i];
+    }
+
+    mean = blockReduceSum<float>(sum);
+    if (tid == 0) {
+        s_mean = mean / n;
+    }
+    __syncthreads();
+
+    float var = 0.0f;
+#pragma unroll
+    for (int i = 0; i < ite; i++) {
+        float diff = local_out[i] - s_mean;
+        var += diff * diff;
+    }
+
+    variance = blockReduceSum<float>(var);
+    if (tid == 0) {
+        s_variance = rsqrtf(variance / n + eps);
+    }
+    __syncthreads();
+
+#pragma unroll
+    for (int i = 0; i < ite; i++) {
+        int col_id = i * blockDim.x + tid;
+        int id = bid * n + col_id;
+        norm_attn_out[id] =
+            (D)((local_out[i] - s_mean) * s_variance * (float)__ldg(&gamma[col_id]) + (float)__ldg(&beta[col_id]));
+    }
+}
+
+template<typename T, typename S, typename D>
+void invokeAddBiasResidualLayerNormCast(
+   S* attn_output, D* norm_attn_out, const S* input, const T* bias, const T* gamma, const T* beta, int m, int n, cudaStream_t stream, float eps)
+{
+    dim3 grid(m);
+    dim3 block(std::min(n, 1024));
+    if (n == 768 || n == 1024) {
+        addBiasResidualPostLayerNormV2Cast<T, S, D><<<grid, n / 4, 0, stream>>>(attn_output, norm_attn_out, input, bias, gamma, beta, n, eps);
+    }
+    else {
+        block.x = std::min(n, 1024);
+        int num_trips = (n + block.x - 1) / block.x;
+        if (num_trips == 1) {
+            addBiasResidualPostLayerNormCast<T, S, D, 1><<<grid, block, 0, stream>>>(attn_output, norm_attn_out, input, bias, gamma, beta, m, n, eps);
+        }
+        else if (num_trips == 2) {
+            addBiasResidualPostLayerNormCast<T, S, D, 2><<<grid, block, 0, stream>>>(attn_output, norm_attn_out, input, bias, gamma, beta, m, n, eps);
+        }
+        else {
+            generalAddBiasResidualPostLayerNormCast<T, S, D><<<grid, block, 0, stream>>>(attn_output, norm_attn_out, input, bias, gamma, beta, m, n, eps);
+        }
+    }
+}
+
+template void invokeAddBiasResidualLayerNormCast<float, float, half>(float* out, half* norm_attn_out,
+                                             const float* input,
+                                             const float* bias,
+                                             const float* gamma,
+                                             const float* beta,
+                                             int m,
+                                             int n,
+                                             cudaStream_t stream,
+                                             float eps);
+
+template void invokeAddBiasResidualLayerNormCast<float, half, float>(half* out, float* norm_attn_out,
+                                             const half* input,
+                                             const float* bias,
+                                             const float* gamma,
+                                             const float* beta,
+                                             int m,
+                                             int n,
+                                             cudaStream_t stream,
+                                             float eps);                                            
+
+template void invokeGeneralAddBiasResidualPreLayerNormCast<float, half>(
+                                              float* attn_output,
+                                              half* norm_output,
+                                              const float* from_tensor,
+                                              const float* gamma,
+                                              const float* beta,
+                                              const float* bias,
+                                              int m,
+                                              int n,
+                                              cudaStream_t stream,
+                                              float eps,
+                                              int opt_version);
+
 template<typename T>
 void invokeAddBiasResidualLayerNorm(
-    T* out, const T* input, const T* bias, const T* gamma, const T* beta, int m, int n, cudaStream_t stream)
+    T* out, const T* input, const T* bias, const T* gamma, const T* beta, int m, int n, cudaStream_t stream, float eps)
 {
     dim3 grid(m);
     dim3 block(std::min(n, 1024));
     if (n == 768 || n == 1024) {
-        addBiasResidualPostLayerNormV2<T><<<grid, n / 4, 0, stream>>>(out, input, bias, gamma, beta, n);
+        addBiasResidualPostLayerNormV2<T><<<grid, n / 4, 0, stream>>>(out, input, bias, gamma, beta, n, eps);
     }
     else {
         block.x = std::min(n, 1024);
         int num_trips = (n + block.x - 1) / block.x;
         if (num_trips == 1) {
-            addBiasResidualPostLayerNorm<T, 1><<<grid, block, 0, stream>>>(out, input, bias, gamma, beta, m, n);
+            addBiasResidualPostLayerNorm<T, 1><<<grid, block, 0, stream>>>(out, input, bias, gamma, beta, m, n, eps);
         }
         else if (num_trips == 2) {
-            addBiasResidualPostLayerNorm<T, 2><<<grid, block, 0, stream>>>(out, input, bias, gamma, beta, m, n);
+            addBiasResidualPostLayerNorm<T, 2><<<grid, block, 0, stream>>>(out, input, bias, gamma, beta, m, n, eps);
         }
         else {
-            generalAddBiasResidualPostLayerNorm<T><<<grid, block, 0, stream>>>(out, input, bias, gamma, beta, m, n);
+            generalAddBiasResidualPostLayerNorm<T><<<grid, block, 0, stream>>>(out, input, bias, gamma, beta, m, n, eps);
         }
     }
 }
@@ -518,25 +736,26 @@ void invokeAddBiasResidualLayerNorm(half* out,
                                     const half* beta,
                                     int m,
                                     int n,
-                                    cudaStream_t stream)
+                                    cudaStream_t stream,
+                                    float eps)
 {
     dim3 grid(m);
     dim3 block(std::min(n, 1024));
 
     if (m >= 512 && (n == 768 || n == 1024)) {
-        addBiasResidualPostLayerNormV2<half><<<grid, n / 8, 0, stream>>>(out, input, bias, gamma, beta, n);
+        addBiasResidualPostLayerNormV2<half><<<grid, n / 8, 0, stream>>>(out, input, bias, gamma, beta, n, eps);
     }
     else {
         block.x = std::min(n, 1024);
         int num_trips = (n + block.x - 1) / block.x;
         if (num_trips == 1) {
-            addBiasResidualPostLayerNorm<half, 1><<<grid, block, 0, stream>>>(out, input, bias, gamma, beta, m, n);
+            addBiasResidualPostLayerNorm<half, 1><<<grid, block, 0, stream>>>(out, input, bias, gamma, beta, m, n, eps);
         }
         else if (num_trips == 2) {
-            addBiasResidualPostLayerNorm<half, 2><<<grid, block, 0, stream>>>(out, input, bias, gamma, beta, m, n);
+            addBiasResidualPostLayerNorm<half, 2><<<grid, block, 0, stream>>>(out, input, bias, gamma, beta, m, n, eps);
         }
         else {
-            generalAddBiasResidualPostLayerNorm<half><<<grid, block, 0, stream>>>(out, input, bias, gamma, beta, m, n);
+            generalAddBiasResidualPostLayerNorm<half><<<grid, block, 0, stream>>>(out, input, bias, gamma, beta, m, n, eps);
         }
     }
 }
@@ -548,7 +767,8 @@ template void invokeAddBiasResidualLayerNorm(float* out,
                                              const float* beta,
                                              int m,
                                              int n,
-                                             cudaStream_t stream);
+                                             cudaStream_t stream,
+                                             float eps);
 template void invokeAddBiasResidualLayerNorm(half* out,
                                              const half* input,
                                              const half* bias,
@@ -556,7 +776,8 @@ template void invokeAddBiasResidualLayerNorm(half* out,
                                              const half* beta,
                                              int m,
                                              int n,
-                                             cudaStream_t stream);
+                                             cudaStream_t stream,
+                                             float eps);
 
 template<typename T>
 __global__ void generalAddBiasResidualLayerNorm(const T* __restrict input,
@@ -566,7 +787,8 @@ __global__ void generalAddBiasResidualLayerNorm(const T* __restrict input,
                                                 T* output,
                                                 T* norm_output,
                                                 int m,
-                                                int n)
+                                                int n,
+                                                float eps)
 {
     int tid = threadIdx.x;
 
@@ -601,7 +823,7 @@ __global__ void generalAddBiasResidualLayerNorm(const T* __restrict input,
     variance = blockReduceSum(local_var_sum);
 
     if (threadIdx.x == 0) {
-        s_variance = rsqrtf(variance / n + 1e-6f);
+        s_variance = rsqrtf(variance / n + eps);
     }
     __syncthreads();
 
@@ -612,6 +834,61 @@ __global__ void generalAddBiasResidualLayerNorm(const T* __restrict input,
     }
 }
 
+template<typename T, typename S>
+__global__ void generalAddBiasResidualLayerNormCast(const T* __restrict input,
+                                                const T* __restrict gamma,
+                                                const T* __restrict beta,
+                                                const T* __restrict bias,
+                                                T* output,
+                                                S* norm_output,
+                                                int m,
+                                                int n,
+                                                float eps)
+{
+    int tid = threadIdx.x;
+
+    __shared__ float s_mean;
+    __shared__ float s_variance;
+    float mean = 0.0f;
+    float variance = 0.0f;
+
+    float local_sum = 0.0f;
+    for (int i = tid; i < n; i += blockDim.x) {
+        float local_out = (float)(ldg(&input[blockIdx.x * n + i]));
+        local_out += (float)((T)output[blockIdx.x * n + i]);
+        if (bias != nullptr) {
+            local_out += (float)(ldg(&bias[i]));
+        }
+        output[blockIdx.x * n + i] = (T)local_out;
+        local_sum += local_out;
+    }
+
+    mean = blockReduceSum(local_sum);
+
+    if (threadIdx.x == 0) {
+        s_mean = mean / n;
+    }
+    __syncthreads();
+
+    float local_var_sum = 0.0f;
+    for (int i = tid; i < n; i += blockDim.x) {
+        float diff = (float)(output[blockIdx.x * n + i]) - s_mean;
+        local_var_sum += diff * diff;
+    }
+    variance = blockReduceSum(local_var_sum);
+
+    if (threadIdx.x == 0) {
+        s_variance = rsqrtf(variance / n + eps);
+    }
+    __syncthreads();
+
+    for (int i = tid; i < n; i += blockDim.x) {
+        float beta_val = (beta == nullptr) ? 0.0f : (float)(ldg(&beta[i]));
+        norm_output[blockIdx.x * n + i] =
+            (S)((((float)output[blockIdx.x * n + i] - s_mean) * s_variance) * (float)(ldg(&gamma[i])) + beta_val);
+    }
+}
+
 #define HALF_LAYERNORM_OPT(UNROLL_FACTOR)                                                                              \
     generalAddBiasResidualLayerNormOpt<T2, true, true, true, true, UNROLL_FACTOR>                                      \
         <<<grid, block, 0, stream>>>((T2*)norm_output,                                                                 \
@@ -621,7 +898,8 @@ __global__ void generalAddBiasResidualLayerNorm(const T* __restrict input,
                                      (const T2*)gamma,                                                                 \
                                      (const T2*)beta,                                                                  \
                                      m,                                                                                \
-                                     half_n);
+                                     half_n,                                                                           \
+                                     eps);
 
 #define HALF_LAYERNORM_OPT2(UNROLL_FACTOR)                                                                             \
     generalAddBiasResidualLayerNormOpt2<T2, true, true, true, true, UNROLL_FACTOR>                                     \
@@ -632,7 +910,8 @@ __global__ void generalAddBiasResidualLayerNorm(const T* __restrict input,
                                      (const T2*)gamma,                                                                 \
                                      (const T2*)beta,                                                                  \
                                      m,                                                                                \
-                                     half_n);
+                                     half_n,                                                                           \
+                                     eps);
 
 template<typename T>
 void invokeGeneralAddBiasResidualPreLayerNorm(T* output,
@@ -644,6 +923,7 @@ void invokeGeneralAddBiasResidualPreLayerNorm(T* output,
                                               int m,
                                               int n,
                                               cudaStream_t stream,
+                                              float eps,
                                               int opt_version)
 {
     if (opt_version > 0 && sizeof(T) == 2 && n % 2 == 0) {
@@ -709,10 +989,41 @@ void invokeGeneralAddBiasResidualPreLayerNorm(T* output,
 
         /* should pay attention to the rsqrt precision*/
         generalAddBiasResidualLayerNorm<T>
-            <<<grid, block, 0, stream>>>(input, gamma, beta, bias, output, norm_output, m, n);  // For gpt-3
+            <<<grid, block, 0, stream>>>(input, gamma, beta, bias, output, norm_output, m, n, eps);  // For gpt-3
     }
 }
 
+template<typename T, typename S>
+void invokeGeneralAddBiasResidualPreLayerNormCast(T* attn_output,
+                                              S* norm_output,
+                                              const T* from_tensor,
+                                              const T* gamma,
+                                              const T* beta,
+                                              const T* bias,
+                                              int m,
+                                              int n,
+                                              cudaStream_t stream,
+                                              float eps,
+                                              int opt_version)
+{ 
+    dim3 grid(m);
+    dim3 block(min(n, 1024));
+
+    /* For general cases, n is equal to hidden_units, e.g., 512/1024.
+    Since we have warp shuffle inside the code, block.x % 32 should be 0.
+    */
+
+    if (n % 32 != 0) {
+        block.x = 1024;
+    }
+
+    // block.x = block.x / (4 / sizeof(T));  // if using half, only need half of block.x
+
+    /* should pay attention to the rsqrt precision*/
+    generalAddBiasResidualLayerNormCast<T, S>
+        <<<grid, block, 0, stream>>>(from_tensor, gamma, beta, bias, attn_output, norm_output, m, n, eps);  // For gpt-3
+}
+
 #undef HALF_LAYERNORM_OPT
 #undef HALF_LAYERNORM_OPT2
 
@@ -725,6 +1036,7 @@ template void invokeGeneralAddBiasResidualPreLayerNorm(float* output,
                                                        int m,
                                                        int n,
                                                        cudaStream_t stream,
+                                                       float eps,
                                                        int opt_version);
 
 template void invokeGeneralAddBiasResidualPreLayerNorm(half* output,
@@ -736,6 +1048,7 @@ template void invokeGeneralAddBiasResidualPreLayerNorm(half* output,
                                                        int m,
                                                        int n,
                                                        cudaStream_t stream,
+                                                       float eps,
                                                        int opt_version);
 
 #ifdef ENABLE_BF16
@@ -748,12 +1061,13 @@ template void invokeGeneralAddBiasResidualPreLayerNorm(__nv_bfloat16* output,
                                                        int m,
                                                        int n,
                                                        cudaStream_t stream,
+                                                       float eps,
                                                        int opt_version);
 #endif
 
 template<typename T>
 __global__ void generalAddResidualT5LayerNorm(
-    const T* __restrict input, const T* __restrict gamma, T* output, T* norm_output, int m, int n)
+    const T* __restrict input, const T* __restrict gamma, T* output, T* norm_output, int m, int n, float eps)
 {
     // layernorm module in the T5 style No bias and no subtraction of mean.
     __shared__ float s_variance;
@@ -770,7 +1084,7 @@ __global__ void generalAddResidualT5LayerNorm(
     variance = blockReduceSum(local_var_sum);
 
     if (threadIdx.x == 0) {
-        s_variance = rsqrtf(variance / n + 1e-6f);
+        s_variance = rsqrtf(variance / n + eps);
     }
     __syncthreads();
 
@@ -783,7 +1097,7 @@ __global__ void generalAddResidualT5LayerNorm(
 
 template<typename T>
 void invokeGeneralAddResidualT5PreLayerNorm(
-    T* output, T* norm_output, const T* input, const T* gamma, int m, int n, cudaStream_t stream)
+    T* output, T* norm_output, const T* input, const T* gamma, int m, int n, cudaStream_t stream, float eps)
 {
     dim3 grid(m);
     dim3 block(min(n, 1024));
@@ -799,14 +1113,14 @@ void invokeGeneralAddResidualT5PreLayerNorm(
     block.x = block.x / (4 / sizeof(T));  // if using half, only need half of block.x
 
     /* should pay attention to the rsqrt precision*/
-    generalAddResidualT5LayerNorm<T><<<grid, block, 0, stream>>>(input, gamma, output, norm_output, m, n);
+    generalAddResidualT5LayerNorm<T><<<grid, block, 0, stream>>>(input, gamma, output, norm_output, m, n, eps);
 }
 
 template void invokeGeneralAddResidualT5PreLayerNorm(
-    float* output, float* norm_output, const float* input, const float* gamma, int m, int n, cudaStream_t stream);
+    float* output, float* norm_output, const float* input, const float* gamma, int m, int n, cudaStream_t stream, float eps);
 
 template void invokeGeneralAddResidualT5PreLayerNorm(
-    half* output, half* norm_output, const half* input, const half* gamma, int m, int n, cudaStream_t stream);
+    half* output, half* norm_output, const half* input, const half* gamma, int m, int n, cudaStream_t stream, float eps);
 
 template<typename T>
 void invokeGeneralAddBiasResidualT5PreLayerNorm(T* output,
@@ -817,13 +1131,14 @@ void invokeGeneralAddBiasResidualT5PreLayerNorm(T* output,
                                                 const T* bias,
                                                 int m,
                                                 int n,
-                                                cudaStream_t stream)
+                                                cudaStream_t stream,
+                                                float eps)
 {
     if (beta != nullptr && bias != nullptr) {
-        invokeGeneralAddBiasResidualPreLayerNorm(output, norm_output, input, gamma, beta, bias, m, n, stream);
+        invokeGeneralAddBiasResidualPreLayerNorm(output, norm_output, input, gamma, beta, bias, m, n, stream, eps);
     }
     else {
-        invokeGeneralAddResidualT5PreLayerNorm(output, norm_output, input, gamma, m, n, stream);
+        invokeGeneralAddResidualT5PreLayerNorm(output, norm_output, input, gamma, m, n, stream, eps);
     }
     return;
 }
@@ -836,7 +1151,8 @@ template void invokeGeneralAddBiasResidualT5PreLayerNorm(float* output,
                                                          const float* bias,
                                                          int m,
                                                          int n,
-                                                         cudaStream_t stream);
+                                                         cudaStream_t stream,
+                                                         float eps);
 
 template void invokeGeneralAddBiasResidualT5PreLayerNorm(half* output,
                                                          half* norm_output,
@@ -846,11 +1162,12 @@ template void invokeGeneralAddBiasResidualT5PreLayerNorm(half* output,
                                                          const half* bias,
                                                          int m,
                                                          int n,
-                                                         cudaStream_t stream);
+                                                         cudaStream_t stream,
+                                                         float eps);
 
 template<typename T>
 __global__ void generalLayerNorm(
-    const T* __restrict input, const T* __restrict gamma, const T* __restrict beta, T* output, int m, int n)
+    const T* __restrict input, const T* __restrict gamma, const T* __restrict beta, T* output, int m, int n, float eps)
 {
     const int tid = threadIdx.x;
 
@@ -879,7 +1196,7 @@ __global__ void generalLayerNorm(
     variance = blockReduceSum(local_var_sum);
 
     if (threadIdx.x == 0) {
-        s_variance = rsqrtf(variance / n + 1e-6f);
+        s_variance = rsqrtf(variance / n + eps);
     }
     __syncthreads();
 
@@ -892,11 +1209,11 @@ __global__ void generalLayerNorm(
 
 #define HALF_LAYERNORM_OPT(UNROLL_FACTOR)                                                                              \
     generalAddBiasResidualLayerNormOpt<T2, false, false, true, true, UNROLL_FACTOR><<<grid, block, 0, stream>>>(       \
-        (T2*)out, (T2*)out, nullptr, (const T2*)input, (const T2*)gamma, (const T2*)beta, m, half_n);
+        (T2*)out, (T2*)out, nullptr, (const T2*)input, (const T2*)gamma, (const T2*)beta, m, half_n, eps);
 
 #define HALF_LAYERNORM_OPT2(UNROLL_FACTOR)                                                                             \
     generalAddBiasResidualLayerNormOpt2<T2, false, false, true, true, UNROLL_FACTOR><<<grid, block, 0, stream>>>(      \
-        (T2*)out, (T2*)out, nullptr, (const T2*)input, (const T2*)gamma, (const T2*)beta, m, half_n);
+        (T2*)out, (T2*)out, nullptr, (const T2*)input, (const T2*)gamma, (const T2*)beta, m, half_n, eps);
 
 template<typename T>
 void invokeGeneralLayerNorm(T* out,
@@ -906,6 +1223,7 @@ void invokeGeneralLayerNorm(T* out,
                             const int m,
                             const int n,
                             cudaStream_t stream,
+                            float eps,
                             int opt_version)
 {
     dim3 grid(m);
@@ -965,7 +1283,7 @@ void invokeGeneralLayerNorm(T* out,
         }
 
         /* should pay attention to the rsqrt precision*/
-        generalLayerNorm<T><<<grid, block, 0, stream>>>(input, gamma, beta, out, m, n);  // For gpt-3
+        generalLayerNorm<T><<<grid, block, 0, stream>>>(input, gamma, beta, out, m, n, eps);  // For gpt-3
     }
 }
 
@@ -979,6 +1297,7 @@ template void invokeGeneralLayerNorm(float* out,
                                      const int m,
                                      const int n,
                                      cudaStream_t stream,
+                                     float eps,
                                      int opt_version);
 template void invokeGeneralLayerNorm(half* out,
                                      const half* input,
@@ -987,6 +1306,7 @@ template void invokeGeneralLayerNorm(half* out,
                                      const int m,
                                      const int n,
                                      cudaStream_t stream,
+                                     float eps,
                                      int opt_version);
 #ifdef ENABLE_BF16
 template void invokeGeneralLayerNorm(__nv_bfloat16* out,
@@ -996,11 +1316,12 @@ template void invokeGeneralLayerNorm(__nv_bfloat16* out,
                                      const int m,
                                      const int n,
                                      cudaStream_t stream,
+                                     float eps,
                                      int opt_version);
 #endif
 
 template<typename T>
-__global__ void generalT5LayerNorm(const T* __restrict input, const T* __restrict gamma, T* output, int m, int n)
+__global__ void generalT5LayerNorm(const T* __restrict input, const T* __restrict gamma, T* output, int m, int n, float eps)
 {
     // layernorm module in the T5 style No bias and no subtraction of mean.
     const int tid = threadIdx.x;
@@ -1016,7 +1337,7 @@ __global__ void generalT5LayerNorm(const T* __restrict input, const T* __restric
     variance = blockReduceSum(local_var_sum);
 
     if (threadIdx.x == 0) {
-        s_variance = rsqrtf(variance / n + 1e-6f);
+        s_variance = rsqrtf(variance / n + eps);
     }
     __syncthreads();
 
@@ -1028,10 +1349,10 @@ __global__ void generalT5LayerNorm(const T* __restrict input, const T* __restric
 
 template<typename T>
 void invokeGeneralT5LayerNorm(
-    T* out, const T* input, const T* gamma, const T* beta, const int m, const int n, cudaStream_t stream)
+    T* out, const T* input, const T* gamma, const T* beta, const int m, const int n, cudaStream_t stream, float eps)
 {
     if (beta != nullptr) {
-        invokeGeneralLayerNorm(out, input, gamma, beta, m, n, stream);
+        invokeGeneralLayerNorm(out, input, gamma, beta, m, n, stream, eps);
         return;
     }
 
@@ -1048,7 +1369,7 @@ void invokeGeneralT5LayerNorm(
     block.x = block.x / (4 / sizeof(T));  // if using half, only need half of block.x
 
     /* should pay attention to the rsqrt precision*/
-    generalT5LayerNorm<T><<<grid, block, 0, stream>>>(input, gamma, out, m, n);  // For gpt-3
+    generalT5LayerNorm<T><<<grid, block, 0, stream>>>(input, gamma, out, m, n, eps);  // For gpt-3
 }
 
 template void invokeGeneralT5LayerNorm(float* out,
@@ -1057,9 +1378,10 @@ template void invokeGeneralT5LayerNorm(float* out,
                                        const float* beta,
                                        const int m,
                                        const int n,
-                                       cudaStream_t stream);
+                                       cudaStream_t stream,
+                                       float eps);
 template void invokeGeneralT5LayerNorm(
-    half* out, const half* input, const half* gamma, const half* beta, const int m, const int n, cudaStream_t stream);
+    half* out, const half* input, const half* gamma, const half* beta, const int m, const int n, cudaStream_t stream, float eps);
 
 /*******************  invokeLayernormShiftPartition  ***********************/
 
@@ -1073,7 +1395,8 @@ __global__ void layernorm_shift_partition(T* out,
                                           int W,
                                           int n,
                                           int shift_size,
-                                          int window_size)
+                                          int window_size,
+                                          float eps)
 {
     int tid = threadIdx.x;
     const int batch_offset = blockIdx.z * gridDim.y * gridDim.x;
@@ -1102,7 +1425,7 @@ __global__ void layernorm_shift_partition(T* out,
     float diff = (tid < n) ? (local_out - s_mean) : 0.0f;
     variance = blockReduceSum<float>(diff * diff);
     if (threadIdx.x == 0) {
-        s_variance = variance / n + 1e-6f;
+        s_variance = variance / n + eps;
     }
     __syncthreads();
 
@@ -1122,7 +1445,8 @@ __global__ void layernorm_shift_partition(half2* out_ptr,
                                           int W,
                                           int n,
                                           int shift_size,
-                                          int window_size)
+                                          int window_size,
+                                          float eps)
 {
     const int batch_offset = blockIdx.z * gridDim.y * gridDim.x;
     const int bid = batch_offset + blockIdx.y * gridDim.x + blockIdx.x;
@@ -1161,7 +1485,7 @@ __global__ void layernorm_shift_partition(half2* out_ptr,
     }
     variance = blockReduceSum<float>(variance);
     if (threadIdx.x == 0) {
-        s_variance = rsqrtf(variance / (n * 2) + 1e-6f);
+        s_variance = rsqrtf(variance / (n * 2) + eps);
     }
     __syncthreads();
 
@@ -1184,7 +1508,8 @@ __global__ void layernorm_shift_partition_v2(T* out,
                                              int W,
                                              int n,
                                              int shift_size,
-                                             int window_size)
+                                             int window_size,
+                                             float eps)
 {
     const int ite = 4;
     const int tid = threadIdx.x;
@@ -1236,7 +1561,7 @@ __global__ void layernorm_shift_partition_v2(T* out,
 
     variance = blockReduceSum<float>(var);
     if (tid == 0) {
-        s_variance = rsqrtf(variance / n + 1e-6f);
+        s_variance = rsqrtf(variance / n + eps);
     }
     __syncthreads();
 
@@ -1260,7 +1585,8 @@ __global__ void layernorm_shift_partition_v2(half2* out_ptr,
                                              int W,
                                              int n,
                                              int shift_size,
-                                             int window_size)
+                                             int window_size,
+                                             float eps)
 {
     const int ite = 4;
     const int tid = threadIdx.x;
@@ -1315,7 +1641,7 @@ __global__ void layernorm_shift_partition_v2(half2* out_ptr,
 
     variance = blockReduceSum<float>(var);
     if (threadIdx.x == 0) {
-        s_variance = rsqrtf(variance / (n * 2) + 1e-6f);
+        s_variance = rsqrtf(variance / (n * 2) + eps);
     }
     __syncthreads();
 
@@ -1341,18 +1667,19 @@ void invokeLayernormShiftPartition(T* out,
                                    int n,
                                    int shift_size,
                                    int window_size,
-                                   cudaStream_t stream)
+                                   cudaStream_t stream,
+                                   float eps)
 {
     dim3 grid(W, H, batch);
     int blockSize = (n + 31) / 32 * 32;
     if (blockSize >= 768) {
         blockSize = ((blockSize / 4) + 31) / 32 * 32;
         layernorm_shift_partition_v2<T>
-            <<<grid, blockSize, 0, stream>>>(out, input, gamma, beta, batch, H, W, n, shift_size, window_size);
+            <<<grid, blockSize, 0, stream>>>(out, input, gamma, beta, batch, H, W, n, shift_size, window_size, eps);
     }
     else {
         layernorm_shift_partition<T>
-            <<<grid, blockSize, 0, stream>>>(out, input, gamma, beta, batch, H, W, n, shift_size, window_size);
+            <<<grid, blockSize, 0, stream>>>(out, input, gamma, beta, batch, H, W, n, shift_size, window_size, eps);
     }
 }
 
@@ -1367,7 +1694,8 @@ void invokeLayernormShiftPartition(half* out,
                                    int n,
                                    int shift_size,
                                    int window_size,
-                                   cudaStream_t stream)
+                                   cudaStream_t stream,
+                                   float eps)
 {
     dim3 grid(W, H, batch);
     int blockSize = n / 2;
@@ -1384,7 +1712,8 @@ void invokeLayernormShiftPartition(half* out,
                                                                      W,
                                                                      n / 2,
                                                                      shift_size,
-                                                                     window_size);
+                                                                     window_size,
+                                                                     eps);
     }
     else {
         layernorm_shift_partition<<<grid, blockSize, 0, stream>>>((half2*)out,
@@ -1396,7 +1725,8 @@ void invokeLayernormShiftPartition(half* out,
                                                                   W,
                                                                   n / 2,
                                                                   shift_size,
-                                                                  window_size);
+                                                                  window_size,
+                                                                  eps);
     }
 }
 
@@ -1410,7 +1740,8 @@ template void invokeLayernormShiftPartition<float>(float* out,
                                                    int n,
                                                    int shift_size,
                                                    int window_size,
-                                                   cudaStream_t stream);
+                                                   cudaStream_t stream,
+                                                   float eps);
 
 template void invokeLayernormShiftPartition<half>(half* out,
                                                   const half* input,
@@ -1422,12 +1753,13 @@ template void invokeLayernormShiftPartition<half>(half* out,
                                                   int n,
                                                   int shift_size,
                                                   int window_size,
-                                                  cudaStream_t stream);
+                                                  cudaStream_t stream,
+                                                  float eps);
 
 /*******************  invokeAddBiasLayernorm  ***********************/
 
 template<typename T>
-__global__ void add_bias_layernorm(T* out, const T* bias, const T* gamma, const T* beta, int n)
+__global__ void add_bias_layernorm(T* out, const T* bias, const T* gamma, const T* beta, int n, float eps)
 {
     int tid = threadIdx.x;
     const int bid = blockIdx.x;
@@ -1447,7 +1779,7 @@ __global__ void add_bias_layernorm(T* out, const T* bias, const T* gamma, const
     float diff = (tid < n) ? (local_out - s_mean) : 0.0f;
     variance = blockReduceSum<float>(diff * diff);
     if (threadIdx.x == 0) {
-        s_variance = variance / n + 1e-6f;
+        s_variance = variance / n + eps;
     }
     __syncthreads();
 
@@ -1459,7 +1791,7 @@ __global__ void add_bias_layernorm(T* out, const T* bias, const T* gamma, const
 
 template<typename T>
 __global__ void
-add_bias_layernorm_v2(T* out, const T* __restrict bias, const T* __restrict gamma, const T* __restrict beta, int n)
+add_bias_layernorm_v2(T* out, const T* __restrict bias, const T* __restrict gamma, const T* __restrict beta, int n, float eps)
 {
     const int ite = 4;
     const int tid = threadIdx.x;
@@ -1496,7 +1828,7 @@ add_bias_layernorm_v2(T* out, const T* __restrict bias, const T* __restrict gamm
 
     variance = blockReduceSum<float>(var);
     if (tid == 0) {
-        s_variance = rsqrtf(variance / n + 1e-6f);
+        s_variance = rsqrtf(variance / n + eps);
     }
     __syncthreads();
 
@@ -1512,15 +1844,15 @@ add_bias_layernorm_v2(T* out, const T* __restrict bias, const T* __restrict gamm
 
 #define HALF_LAYERNORM_OPT(UNROLL_FACTOR)                                                                              \
     generalAddBiasResidualLayerNormOpt<T2, false, true, true, true, UNROLL_FACTOR><<<grid, block, 0, stream>>>(        \
-        (T2*)out, (T2*)out, (const T2*)bias, (const T2*)out, (const T2*)gamma, (const T2*)beta, m, half_n);
+        (T2*)out, (T2*)out, (const T2*)bias, (const T2*)out, (const T2*)gamma, (const T2*)beta, m, half_n, eps);
 
 #define HALF_LAYERNORM_OPT2(UNROLL_FACTOR)                                                                             \
     generalAddBiasResidualLayerNormOpt2<T2, false, true, true, true, UNROLL_FACTOR><<<grid, block, 0, stream>>>(       \
-        (T2*)out, (T2*)out, (const T2*)bias, (const T2*)out, (const T2*)gamma, (const T2*)beta, m, half_n);
+        (T2*)out, (T2*)out, (const T2*)bias, (const T2*)out, (const T2*)gamma, (const T2*)beta, m, half_n, eps);
 
 template<typename T>
 void invokeAddBiasLayernorm(
-    T* out, const T* bias, const T* gamma, const T* beta, int m, int n, cudaStream_t stream, int opt_version)
+    T* out, const T* bias, const T* gamma, const T* beta, int m, int n, cudaStream_t stream, float eps, int opt_version)
 {
     dim3 grid(m);
     if (n % 2 == 0 && std::is_same<T, half>::value && opt_version > 0) {
@@ -1572,10 +1904,10 @@ void invokeAddBiasLayernorm(
         int blockSize = (n + 31) / 32 * 32;
         if (blockSize >= 768) {
             blockSize = ((blockSize / 4) + 31) / 32 * 32;
-            add_bias_layernorm_v2<T><<<grid, blockSize, 0, stream>>>(out, bias, gamma, beta, n);
+            add_bias_layernorm_v2<T><<<grid, blockSize, 0, stream>>>(out, bias, gamma, beta, n, eps);
         }
         else {
-            add_bias_layernorm<T><<<grid, blockSize, 0, stream>>>(out, bias, gamma, beta, n);
+            add_bias_layernorm<T><<<grid, blockSize, 0, stream>>>(out, bias, gamma, beta, n, eps);
         }
     }
 }
@@ -1590,6 +1922,7 @@ template void invokeAddBiasLayernorm<float>(float* out,
                                             int m,
                                             int n,
                                             cudaStream_t stream,
+                                            float eps,
                                             int opt_version);
 
 template void invokeAddBiasLayernorm<half>(half* out,
@@ -1599,6 +1932,7 @@ template void invokeAddBiasLayernorm<half>(half* out,
                                            int m,
                                            int n,
                                            cudaStream_t stream,
+                                           float eps,
                                            int opt_version);
 #ifdef ENABLE_BF16
 template void invokeAddBiasLayernorm<__nv_bfloat16>(__nv_bfloat16* out,
@@ -1608,6 +1942,7 @@ template void invokeAddBiasLayernorm<__nv_bfloat16>(__nv_bfloat16* out,
                                                     int m,
                                                     int n,
                                                     cudaStream_t stream,
+                                                    float eps,
                                                     int opt_version);
 #endif
 
@@ -1625,7 +1960,8 @@ __global__ void merge_layernorm_v2(T* out,
                                    int batch,
                                    int H,
                                    int W,
-                                   int n)
+                                   int n,
+                                   float eps)
 {
     const int ite = 4;
     const int tid = threadIdx.x;
@@ -1675,7 +2011,7 @@ __global__ void merge_layernorm_v2(T* out,
 
     variance = blockReduceSum<float>(var);
     if (tid == 0) {
-        s_variance = rsqrtf(variance / n + 1e-6f);
+        s_variance = rsqrtf(variance / n + eps);
     }
     __syncthreads();
 
@@ -1693,7 +2029,7 @@ __global__ void merge_layernorm_v2(T* out,
 // TODO : accelerate with half2
 template<typename T>
 void invokeMergeLayernorm(
-    T* output, const T* input, const T* gamma, const T* beta, int batch, int H, int W, int n, cudaStream_t stream)
+    T* output, const T* input, const T* gamma, const T* beta, int batch, int H, int W, int n, cudaStream_t stream, float eps)
 {
     if ((W % 2 != 0) || (H % 2 != 0)) {
         printf("[ERROR][invokeMergeLayernorm] H(W) should be a multiple of 2.\n");
@@ -1706,7 +2042,7 @@ void invokeMergeLayernorm(
     // if (blockSize >= 768)
     {
         blockSize = ((blockSize / 4) + 31) / 32 * 32;
-        merge_layernorm_v2<T><<<grid, blockSize, 0, stream>>>(output, input, gamma, beta, batch, H / 2, W / 2, n * 4);
+        merge_layernorm_v2<T><<<grid, blockSize, 0, stream>>>(output, input, gamma, beta, batch, H / 2, W / 2, n * 4, eps);
     }
     /*
     else
@@ -1722,7 +2058,8 @@ template void invokeMergeLayernorm<float>(float* output,
                                           int H,
                                           int W,
                                           int n,
-                                          cudaStream_t stream);
+                                          cudaStream_t stream,
+                                          float eps);
 
 template void invokeMergeLayernorm<half>(half* output,
                                          const half* input,
@@ -1732,6 +2069,47 @@ template void invokeMergeLayernorm<half>(half* output,
                                          int H,
                                          int W,
                                          int n,
-                                         cudaStream_t stream);
+                                         cudaStream_t stream,
+                                         float eps);
+
+
+
+
+
+
+__global__ void ToFloat(half* src, float* dst, int element_cnt) {
+  for (int pos = blockIdx.x * blockDim.x + threadIdx.x; pos < element_cnt; pos += blockDim.x * gridDim.x) {
+    dst[pos] = (float)(src[pos]);
+  }
+}
+
+__global__ void ToHalf(float* src, half* dst, int element_cnt) {
+  for (int pos = blockIdx.x * blockDim.x + threadIdx.x; pos < element_cnt; pos += blockDim.x * gridDim.x) {
+    dst[pos] = (half)(src[pos]);
+  }
+}
+
+__global__ void ToFlaotFromFloat(float* src, float* dst, int element_cnt) {
+  for (int pos = blockIdx.x * blockDim.x + threadIdx.x; pos < element_cnt; pos += blockDim.x * gridDim.x) {
+    dst[pos] = (src[pos]);
+
+  }
+}
+
+void InvokeCast(void* src, void* dst, int element_cnt, int dir, cudaStream_t stream) {
+  dim3 block, grid;
+    
+  block.x = 1024;
+  grid.x = ceil(element_cnt / 1024.);
+//  ToFlaotFromFloat<<<grid, block, 0, stream>>>((float*)src, (float*)dst, element_cnt);
+//  return;
+  if (dir) {
+    ToFloat<<<grid, block, 0, stream>>>((half*)src, (float*)dst, element_cnt);
+  } else {
+    ToHalf<<<grid, block, 0, stream>>>((float*)src, (half*)dst, element_cnt);
+  }
+  return;
+}
+
 
 }  // namespace fastertransformer
\ No newline at end of file
diff --git a/src/fastertransformer/kernels/layernorm_kernels.h b/src/fastertransformer/kernels/layernorm_kernels.h
index e8319de..bfd6c1c 100644
--- a/src/fastertransformer/kernels/layernorm_kernels.h
+++ b/src/fastertransformer/kernels/layernorm_kernels.h
@@ -42,7 +42,8 @@ void invokeAddBiasResidualLayerNorm(T* out,
                                     const T* beta,
                                     const int m,
                                     const int n,
-                                    cudaStream_t stream);
+                                    cudaStream_t stream,
+                                    float eps = 1e-6f);
 
 template<typename T>
 void invokeGeneralAddBiasResidualPreLayerNorm(T* output,
@@ -54,6 +55,7 @@ void invokeGeneralAddBiasResidualPreLayerNorm(T* output,
                                               int m,
                                               int n,
                                               cudaStream_t stream,
+                                              float eps = 1e-6f,
                                               int opt_version = 2);
 
 template<typename T>
@@ -64,15 +66,16 @@ void invokeGeneralLayerNorm(T* out,
                             const int m,
                             const int n,
                             cudaStream_t stream,
+                            float eps = 1e-6f,
                             int opt_version = 2);
 
 template<typename T>
 void invokeGeneralT5LayerNorm(
-    T* out, const T* input, const T* gamma, const T* beta, const int m, const int n, cudaStream_t stream);
+    T* out, const T* input, const T* gamma, const T* beta, const int m, const int n, cudaStream_t stream, float eps = 1e-6f);
 
 template<typename T>
 void invokeGeneralAddResidualT5PreLayerNorm(
-    T* output, T* norm_output, const T* input, const T* gamma, int m, int n, cudaStream_t stream);
+    T* output, T* norm_output, const T* input, const T* gamma, int m, int n, cudaStream_t stream, float eps = 1e-6f);
 
 template<typename T>
 void invokeGeneralAddBiasResidualT5PreLayerNorm(T* output,
@@ -83,7 +86,8 @@ void invokeGeneralAddBiasResidualT5PreLayerNorm(T* output,
                                                 const T* bias,
                                                 int m,
                                                 int n,
-                                                cudaStream_t stream);
+                                                cudaStream_t stream,
+                                                float eps = 1e-6f);
 
 template<typename T>
 void invokeLayernormShiftPartition(T* out,
@@ -96,14 +100,33 @@ void invokeLayernormShiftPartition(T* out,
                                    int n,
                                    int shift_size,
                                    int window_size,
-                                   cudaStream_t stream);
+                                   cudaStream_t stream,
+                                   float eps = 1e-6f);
+
+template<typename T, typename S>
+void invokeGeneralAddBiasResidualPreLayerNormCast(T* attn_output,
+                                              S* norm_output,
+                                              const T* from_tensor,
+                                              const T* gamma,
+                                              const T* beta,
+                                              const T* bias,
+                                              int m,
+                                              int n,
+                                              cudaStream_t stream,
+                                              float eps = 1e-6f,
+                                              int opt_version = 2);
+
+template<typename T, typename S, typename D>
+void invokeAddBiasResidualLayerNormCast(
+    S* attn_output, D* norm_output, const S* input, const T* bias, const T* gamma, const T* beta, int m, int n, cudaStream_t stream, float eps = 1e-6f);
 
 template<typename T>
 void invokeAddBiasLayernorm(
-    T* out, const T* bias, const T* gamma, const T* beta, int m, int n, cudaStream_t stream, int opt_version = 2);
+    T* out, const T* bias, const T* gamma, const T* beta, int m, int n, cudaStream_t stream, float eps = 1e-6f, int opt_version = 2);
 
 template<typename T>
 void invokeMergeLayernorm(
-    T* output, const T* input, const T* gamma, const T* beta, int batch, int H, int W, int n, cudaStream_t stream);
+    T* output, const T* input, const T* gamma, const T* beta, int batch, int H, int W, int n, cudaStream_t stream, float eps = 1e-6f);
 
+void InvokeCast(void* src, void* dst, int element_cnt, int dir, cudaStream_t stream);
 }  // namespace fastertransformer
diff --git a/src/fastertransformer/kernels/unfused_attention_kernels.cu b/src/fastertransformer/kernels/unfused_attention_kernels.cu
index f951e71..07479d0 100644
--- a/src/fastertransformer/kernels/unfused_attention_kernels.cu
+++ b/src/fastertransformer/kernels/unfused_attention_kernels.cu
@@ -15,6 +15,14 @@
  * limitations under the License.
  */
 
+#ifndef CUDART_VERSION
+#error CUDART_VERSION Undefined!
+#elif (CUDART_VERSION >= 11050)
+#include <cub/cub.cuh>
+#else
+#include "3rdparty/cub/cub.cuh"
+#endif
+
 #include "src/fastertransformer/kernels/bfloat16_fallback_kenrels.cuh"
 #include "src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h"
 #include "src/fastertransformer/kernels/reduce_kernel_utils.cuh"
@@ -23,6 +31,25 @@
 
 namespace fastertransformer {
 
+
+const int WARP_SIZE = 32;
+const bool ATTENION_OPT = true;
+const int ATTENTION_BLOCK_SIZE = 256;
+
+///////////////////////////////////////////////////////////////////////////////////////////////////
+
+template<int HALF_ELEMENTS_PER_WARP_LOAD>
+using Copy_half_t = typename std::conditional<
+    HALF_ELEMENTS_PER_WARP_LOAD == 32,
+    half,
+    typename std::conditional<HALF_ELEMENTS_PER_WARP_LOAD == 64,
+                              int,
+                              typename std::conditional<HALF_ELEMENTS_PER_WARP_LOAD == 128, int2, int4>::type>::type>::
+    type;
+
+template<typename T, int ELEMENTS_PER_WARP_LOAD>
+using Copy_t = Copy_half_t<sizeof(T) / sizeof(half) * ELEMENTS_PER_WARP_LOAD>;
+
 __inline__ __device__ int target_index(int id1, int id2, int id3, int id4, int dim_1, int dim_2, int dim_3, int dim_4)
 {
     return id1 * (dim_2 * dim_3 * dim_4) + id3 * (dim_2 * dim_4) + id2 * dim_4 + id4;
@@ -243,6 +270,172 @@ __global__ void softmax_kernel_v4(T* qk_buf_,
     }
 }
 
+template<int ITEMS_PER_THREAD, typename T, typename T_IN>
+__global__ void softmax_cross_kernel_v4(T* qk_buf_,
+                                  const T_IN* qk_buf_src,
+                                  const T* attr_mask,
+                                  const int batch_size,
+                                  const int head_num,
+                                  const int seq_len,
+                                  const int trgt_seq_len,
+                                  const T scalar)
+{
+    for (int seq_id = blockIdx.x; seq_id < seq_len; seq_id += gridDim.x) {
+        float data[ITEMS_PER_THREAD];
+        int qk_offset;
+        __shared__ float s_mean, s_max;
+        float local_max = -1e20f;
+        for (int i = 0; blockDim.x * i + threadIdx.x < trgt_seq_len; i++) {
+            qk_offset =
+                ((blockIdx.y * head_num + blockIdx.z) * seq_len + seq_id) * trgt_seq_len + blockDim.x * i + threadIdx.x;
+            int mask_offset = (blockIdx.y * seq_len + seq_id) * trgt_seq_len + blockDim.x * i + threadIdx.x;
+
+            float qk = static_cast<float>(qk_buf_src[qk_offset]);
+            float mask_val = static_cast<float>(ldg(&attr_mask[mask_offset]));
+
+            mask_val = (1.0f - mask_val) * -10000.0f;
+
+            data[i] = qk * static_cast<float>(scalar) + mask_val;
+            local_max = fmax(local_max, data[i]);
+        }
+
+        float max_val = blockDim.x <= 32 ? warpReduceMax(local_max) : blockReduceMax<float>(local_max);
+        if (threadIdx.x == 0) {
+            s_max = max_val;
+        }
+        __syncthreads();
+
+        float local_sum = 0;
+        for (int i = 0; blockDim.x * i + threadIdx.x < trgt_seq_len; i++) {
+            data[i] = __expf(data[i] - s_max);
+            local_sum += data[i];
+        }
+        float sum_val = blockDim.x <= 32 ? warpReduceSum(local_sum) : blockReduceSum<float>(local_sum);
+        if (threadIdx.x == 0) {
+            s_mean = sum_val + 1e-6f;
+            s_mean = __fdividef(1.0f, s_mean);
+        }
+        __syncthreads();
+
+        for (int i = 0; blockDim.x * i + threadIdx.x < trgt_seq_len; i++) {
+            qk_offset =
+                ((blockIdx.y * head_num + blockIdx.z) * seq_len + seq_id) * trgt_seq_len + blockDim.x * i + threadIdx.x;
+            qk_buf_[qk_offset] = (T)(data[i] * s_mean);
+        }
+    }
+}
+
+template<int ITEMS_PER_THREAD, typename T, typename T_M>
+__global__ void softmax_mix_kernel_v4(T* qk_buf_,
+                                  const T_M* attr_mask,
+                                  const int batch_size,
+                                  const int head_num,
+                                  const int seq_len,
+                                  const int trgt_seq_len,
+                                  const T scalar)
+{
+    T* qk_buf_src = qk_buf_;
+    for (int seq_id = blockIdx.x; seq_id < seq_len; seq_id += gridDim.x) {
+        float data[ITEMS_PER_THREAD];
+        int qk_offset;
+        __shared__ float s_mean, s_max;
+        float local_max = -1e20f;
+        for (int i = 0; blockDim.x * i + threadIdx.x < trgt_seq_len; i++) {
+            qk_offset =
+                ((blockIdx.y * head_num + blockIdx.z) * seq_len + seq_id) * trgt_seq_len + blockDim.x * i + threadIdx.x;
+            int mask_offset = (blockIdx.y * seq_len + seq_id) * trgt_seq_len + blockDim.x * i + threadIdx.x;
+            float qk = static_cast<float>(qk_buf_src[qk_offset]);
+            float mask_val = static_cast<float>(ldg(&attr_mask[mask_offset]));
+
+            mask_val = (1.0f - mask_val) * -10000.0f;
+
+            data[i] = qk * static_cast<float>(scalar) + mask_val;
+            local_max = fmax(local_max, data[i]);
+        }
+
+        float max_val = blockDim.x <= 32 ? warpReduceMax(local_max) : blockReduceMax<float>(local_max);
+        if (threadIdx.x == 0) {
+            s_max = max_val;
+        }
+        __syncthreads();
+
+        float local_sum = 0;
+        for (int i = 0; blockDim.x * i + threadIdx.x < trgt_seq_len; i++) {
+            data[i] = __expf(data[i] - s_max);
+            local_sum += data[i];
+        }
+        float sum_val = blockDim.x <= 32 ? warpReduceSum(local_sum) : blockReduceSum<float>(local_sum);
+        if (threadIdx.x == 0) {
+            s_mean = sum_val + 1e-6f;
+            s_mean = __fdividef(1.0f, s_mean);
+        }
+        __syncthreads();
+
+        for (int i = 0; blockDim.x * i + threadIdx.x < trgt_seq_len; i++) {
+            qk_offset =
+                ((blockIdx.y * head_num + blockIdx.z) * seq_len + seq_id) * trgt_seq_len + blockDim.x * i + threadIdx.x;
+            qk_buf_[qk_offset] = (T)(data[i] * s_mean);
+        }
+    }
+}
+
+template<int ITEMS_PER_THREAD, typename T, typename T_M>
+__global__ void softmax_mix_kernel_bias_v4(T* qk_buf_,
+                                        const T_M* attr_mask,
+                                        const T* position_bias,
+                                        const int batch_size,
+                                        const int head_num,
+                                        const int seq_len,
+                                        const int trgt_seq_len,
+                                        const T scalar)
+{
+    T* qk_buf_src = qk_buf_;
+    for (int seq_id = blockIdx.x; seq_id < seq_len; seq_id += gridDim.x) {
+        float data[ITEMS_PER_THREAD];
+        int qk_offset;
+        __shared__ float s_mean, s_max;
+        float local_max = -1e20f;
+        for (int i = 0; blockDim.x * i + threadIdx.x < trgt_seq_len; i++) {
+            qk_offset =
+                ((blockIdx.y * head_num + blockIdx.z) * seq_len + seq_id) * trgt_seq_len + blockDim.x * i + threadIdx.x;
+            int mask_offset = (blockIdx.y * seq_len + seq_id) * trgt_seq_len + blockDim.x * i + threadIdx.x;
+            int bias_offset = qk_offset;
+            float qk = static_cast<float>(qk_buf_src[qk_offset]);
+            float mask_val = static_cast<float>(ldg(&attr_mask[mask_offset]));
+            float bias_val = static_cast<float>(ldg(&position_bias[bias_offset]));
+
+            mask_val = (1.0f - mask_val) * -10000.0f;
+
+            data[i] = qk * static_cast<float>(scalar) + mask_val + bias_val;
+            local_max = fmax(local_max, data[i]);
+        }
+
+        float max_val = blockDim.x <= 32 ? warpReduceMax(local_max) : blockReduceMax<float>(local_max);
+        if (threadIdx.x == 0) {
+            s_max = max_val;
+        }
+        __syncthreads();
+
+        float local_sum = 0;
+        for (int i = 0; blockDim.x * i + threadIdx.x < trgt_seq_len; i++) {
+            data[i] = __expf(data[i] - s_max);
+            local_sum += data[i];
+        }
+        float sum_val = blockDim.x <= 32 ? warpReduceSum(local_sum) : blockReduceSum<float>(local_sum);
+        if (threadIdx.x == 0) {
+            s_mean = sum_val + 1e-6f;
+            s_mean = __fdividef(1.0f, s_mean);
+        }
+        __syncthreads();
+
+        for (int i = 0; blockDim.x * i + threadIdx.x < trgt_seq_len; i++) {
+            qk_offset =
+                ((blockIdx.y * head_num + blockIdx.z) * seq_len + seq_id) * trgt_seq_len + blockDim.x * i + threadIdx.x;
+            qk_buf_[qk_offset] = (T)(data[i] * s_mean);
+        }
+    }
+}
+
 template<typename T, int ITEMS_PER_THREAD>
 __global__ void softmax_kernel_v4_half2(
     T* qk_buf_, const T* attr_mask, const int batch_size, const int head_num, const int seq_len, const T scalar)
@@ -298,6 +491,119 @@ __global__ void softmax_kernel_v4_half2(
     }
 }
 
+template<typename T, int ITEMS_PER_THREAD>
+__global__ void softmax_cross_kernel_v4_half2(
+    T* qk_buf_, const T* attr_mask, const int batch_size, const int head_num, const int seq_len, const int trgt_seq_len, const T scalar)
+{
+    using T2 = typename TypeConverter<T>::Type;
+    T2* qk_buf_half2 = (T2*)qk_buf_;
+    const T2* attr_mask_half2 = (const T2*)attr_mask;
+
+    for (int seq_id = blockIdx.x; seq_id < seq_len; seq_id += gridDim.x) {
+        T2 data[ITEMS_PER_THREAD];
+        int qk_offset;
+        __shared__ float s_mean, s_max;
+        float local_max = -1e20f;
+        for (int i = 0; blockDim.x * i + threadIdx.x < (trgt_seq_len / 2) && i < ITEMS_PER_THREAD; i++) {
+            qk_offset = ((blockIdx.y * head_num + blockIdx.z) * seq_len + seq_id) * (trgt_seq_len / 2) + blockDim.x * i
+                        + threadIdx.x;
+            int mask_offset = (blockIdx.y * seq_len + seq_id) * (trgt_seq_len / 2) + blockDim.x * i + threadIdx.x;
+
+            T2 qk = qk_buf_half2[qk_offset];
+            T2 mask_val = ldg(&attr_mask_half2[mask_offset]);
+            mask_val = hmul2<T2>(hsub2<T2>(float2type2<T2>(1.0f), mask_val), float2type2<T2>(-10000.0f));
+
+            data[i] = hadd2<T2>(hmul2<T2>(qk, type2type2<T, T2>(scalar)), mask_val);
+
+            local_max = fmax(local_max, fmax((float)data[i].x, (float)data[i].y));
+        }
+
+        float max_val = blockDim.x <= 32 ? warpReduceMax(local_max) : blockReduceMax<float>(local_max);
+        if (threadIdx.x == 0) {
+            s_max = max_val;
+        }
+        __syncthreads();
+
+        float local_sum = 0;
+        for (int i = 0; blockDim.x * i + threadIdx.x < (trgt_seq_len / 2) && i < ITEMS_PER_THREAD; i++) {
+            data[i] = hexp2<T2>(hsub2<T2>(data[i], float2type2<T2>(s_max)));
+            local_sum += (float)(data[i].x + data[i].y);
+        }
+
+        float sum_val = blockDim.x <= 32 ? warpReduceSum(local_sum) : blockReduceSum<float>(local_sum);
+
+        if (threadIdx.x == 0) {
+            s_mean = sum_val + 1e-6f;
+            s_mean = __fdividef(1.0f, s_mean);
+        }
+        __syncthreads();
+
+        for (int i = 0; blockDim.x * i + threadIdx.x < (trgt_seq_len / 2) && i < ITEMS_PER_THREAD; i++) {
+            qk_offset = ((blockIdx.y * head_num + blockIdx.z) * seq_len + seq_id) * (trgt_seq_len / 2) + blockDim.x * i
+                        + threadIdx.x;
+            qk_buf_half2[qk_offset] = hmul2<T2>(data[i], float2type2<T2>(s_mean));
+        }
+    }
+}
+
+template<typename T, int ITEMS_PER_THREAD>
+__global__ void softmax_cross_kernel_bias_v4_half2(
+    T* qk_buf_, const T* attr_mask, const T* position_bias, const int batch_size, const int head_num, const int seq_len, const int trgt_seq_len, const T scalar)
+{
+    using T2 = typename TypeConverter<T>::Type;
+    T2* qk_buf_half2 = (T2*)qk_buf_;
+    const T2* attr_mask_half2 = (const T2*)attr_mask;
+    const T2* position_bias_half2 = (const T2*)position_bias;
+
+    for (int seq_id = blockIdx.x; seq_id < seq_len; seq_id += gridDim.x) {
+        T2 data[ITEMS_PER_THREAD];
+        int qk_offset;
+        __shared__ float s_mean, s_max;
+        float local_max = -1e20f;
+        for (int i = 0; blockDim.x * i + threadIdx.x < (trgt_seq_len / 2) && i < ITEMS_PER_THREAD; i++) {
+            qk_offset = ((blockIdx.y * head_num + blockIdx.z) * seq_len + seq_id) * (trgt_seq_len / 2) + blockDim.x * i
+                        + threadIdx.x;
+            int mask_offset = (blockIdx.y * seq_len + seq_id) * (trgt_seq_len / 2) + blockDim.x * i + threadIdx.x;
+            int bias_offset = qk_offset;
+
+            T2 qk = qk_buf_half2[qk_offset];
+            T2 mask_val = ldg(&attr_mask_half2[mask_offset]);
+            mask_val = hmul2<T2>(hsub2<T2>(float2type2<T2>(1.0f), mask_val), float2type2<T2>(-10000.0f));
+            T2 bias_val = (ldg(&position_bias_half2[bias_offset]));
+
+            data[i] = hadd2(hadd2<T2>(hmul2<T2>(qk, type2type2<T, T2>(scalar)), mask_val), bias_val);
+
+            local_max = fmax(local_max, fmax((float)data[i].x, (float)data[i].y));
+        }
+
+        float max_val = blockDim.x <= 32 ? warpReduceMax(local_max) : blockReduceMax<float>(local_max);
+        if (threadIdx.x == 0) {
+            s_max = max_val;
+        }
+        __syncthreads();
+
+        float local_sum = 0;
+        for (int i = 0; blockDim.x * i + threadIdx.x < (trgt_seq_len / 2) && i < ITEMS_PER_THREAD; i++) {
+            data[i] = hexp2<T2>(hsub2<T2>(data[i], float2type2<T2>(s_max)));
+            local_sum += (float)(data[i].x + data[i].y);
+        }
+
+        float sum_val = blockDim.x <= 32 ? warpReduceSum(local_sum) : blockReduceSum<float>(local_sum);
+
+        if (threadIdx.x == 0) {
+            s_mean = sum_val + 1e-6f;
+            s_mean = __fdividef(1.0f, s_mean);
+        }
+        __syncthreads();
+
+        for (int i = 0; blockDim.x * i + threadIdx.x < (trgt_seq_len / 2) && i < ITEMS_PER_THREAD; i++) {
+            qk_offset = ((blockIdx.y * head_num + blockIdx.z) * seq_len + seq_id) * (trgt_seq_len / 2) + blockDim.x * i
+                        + threadIdx.x;
+            qk_buf_half2[qk_offset] = hmul2<T2>(data[i], float2type2<T2>(s_mean));
+        }
+    }
+}
+
 template<typename T, int ITEMS_PER_THREAD, int NUM>
 __global__ void softmax_kernel_v5_half2(
     T* qk_buf_, const T* attr_mask, const int batch_size, const int head_num, const int seq_len, const T scalar)
@@ -415,27 +721,337 @@ __global__ void softmax_kernel_v5_half2(
     }
 }
 
-#define SOFTMAX_KERNEL(ITEMS_PER_THREAD)                                                                               \
-    block.x /= ITEMS_PER_THREAD;                                                                                       \
-    assert(block.x <= 1024);                                                                                           \
-    if (is_half2) {                                                                                                    \
-        if (grid.x % 4 == 0) {                                                                                         \
-            grid.x /= 4;                                                                                               \
-            softmax_kernel_v5_half2<half, ITEMS_PER_THREAD, 4><<<grid, block, 0, stream>>>(                            \
-                (half*)buffer, (const half*)attr_mask, batch_size, head_num, seq_len, (const half)scalar);             \
-        }                                                                                                              \
-        else {                                                                                                         \
-            softmax_kernel_v4_half2<half, ITEMS_PER_THREAD><<<grid, block, 0, stream>>>(                               \
-                (half*)buffer, (const half*)attr_mask, batch_size, head_num, seq_len, (const half)scalar);             \
-        }                                                                                                              \
-    }                                                                                                                  \
-    else {                                                                                                             \
-        softmax_kernel_v4<ITEMS_PER_THREAD, T, T_IN>                                                                   \
-            <<<grid, block, 0, stream>>>(buffer, buffer_src, attr_mask, batch_size, head_num, seq_len, scalar);        \
-    }
+template<typename T, int ITEMS_PER_THREAD, int NUM>
+__global__ void softmax_cross_kernel_v5_half2(
+    T* qk_buf_, const T* attr_mask, const int batch_size, const int head_num, const int seq_len, const int trgt_seq_len, const T scalar)
+{
+    using T2 = typename TypeConverter<T>::Type;
+    T2* qk_buf_half2 = (T2*)qk_buf_;
+    const T2* attr_mask_half2 = (const T2*)attr_mask;
 
-#ifdef ENABLE_BF16
-#define SOFTMAX_KERNEL_BF16(ITEMS_PER_THREAD)                                                                          \
+    for (int seq_id = blockIdx.x; seq_id < seq_len; seq_id += gridDim.x * NUM) {
+        T2 data[NUM][ITEMS_PER_THREAD];
+
+        int qk_offset[NUM];
+
+        __shared__ float s_sum[NUM], s_max[NUM];
+        float local_max[NUM];
+#pragma unroll
+        for (int j = 0; j < NUM; j++) {
+            local_max[j] = -1e20f;
+        }
+
+        for (int i = 0; blockDim.x * i + threadIdx.x < (trgt_seq_len / 2) && i < ITEMS_PER_THREAD; i++) {
+            int mask_offset[NUM];
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                qk_offset[j] = ((blockIdx.y * head_num + blockIdx.z) * seq_len + seq_id + j * gridDim.x) * (trgt_seq_len / 2)
+                               + blockDim.x * i + threadIdx.x;
+                mask_offset[j] =
+                    (blockIdx.y * seq_len + seq_id + j * gridDim.x) * (trgt_seq_len / 2) + blockDim.x * i + threadIdx.x;
+            }
+
+            T2 mask_val[NUM];
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                mask_val[j] = ldg(&attr_mask_half2[mask_offset[j]]);
+            }
+
+            T2 qk[NUM];
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                qk[j] = qk_buf_half2[qk_offset[j]];
+            }
+
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                mask_val[j] = hmul2<T2>(hsub2<T2>(float2type2<T2>(1.0f), mask_val[j]), float2type2<T2>(-10000.0f));
+            }
+
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                data[j][i] = hadd2<T2>(hmul2<T2>(qk[j], type2type2<T, T2>(scalar)), mask_val[j]);
+                local_max[j] = fmax(local_max[j], fmax((float)data[j][i].x, (float)data[j][i].y));
+            }
+        }
+
+        if (blockDim.x <= 32) {
+            warpReduceMaxV2<float, NUM>(local_max);
+        }
+        else {
+            blockReduceMaxV2<float, NUM>(local_max);
+        }
+
+        if (threadIdx.x == 0) {
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                s_max[j] = local_max[j];
+            }
+        }
+        __syncthreads();
+
+        float local_sum[NUM];
+#pragma unroll
+        for (int j = 0; j < NUM; j++) {
+            local_sum[j] = {0.f};
+        }
+
+        for (int i = 0; blockDim.x * i + threadIdx.x < (trgt_seq_len / 2) && i < ITEMS_PER_THREAD; i++) {
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                data[j][i] = hexp2<T2>(hsub2<T2>(data[j][i], float2type2<T2>(s_max[j])));
+            }
+
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                local_sum[j] += (float)(data[j][i].x + data[j][i].y);
+            }
+        }
+
+        if (blockDim.x <= 32) {
+            warpReduceSumV2<float, NUM>(local_sum);
+        }
+        else {
+            blockReduceSumV2<float, NUM>(local_sum);
+        }
+
+        if (threadIdx.x == 0) {
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                s_sum[j] = __fdividef(1.0f, local_sum[j] + 1e-6f);
+            }
+        }
+        __syncthreads();
+
+        for (int i = 0; blockDim.x * i + threadIdx.x < (trgt_seq_len / 2) && i < ITEMS_PER_THREAD; i++) {
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                qk_offset[j] = ((blockIdx.y * head_num + blockIdx.z) * seq_len + seq_id + j * gridDim.x) * (trgt_seq_len / 2)
+                               + blockDim.x * i + threadIdx.x;
+            }
+
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                qk_buf_half2[qk_offset[j]] = hmul2<T2>(data[j][i], float2type2<T2>(s_sum[j]));
+            }
+        }
+    }
+}
+
+template<typename T, int ITEMS_PER_THREAD, int NUM>
+__global__ void softmax_cross_kernel_bias_v5_half2(
+    T* qk_buf_, const T* attr_mask, const T* position_bias, const int batch_size, const int head_num, const int seq_len, const int trgt_seq_len, const T scalar)
+{
+    using T2 = typename TypeConverter<T>::Type;
+    T2* qk_buf_half2 = (T2*)qk_buf_;
+    const T2* attr_mask_half2 = (const T2*)attr_mask;
+    const T2* position_bias_half2 = (const T2*)position_bias;
+
+    for (int seq_id = blockIdx.x; seq_id < seq_len; seq_id += gridDim.x * NUM) {
+        T2 data[NUM][ITEMS_PER_THREAD];
+
+        int qk_offset[NUM];
+        int pos_bias_offset[NUM];
+
+        __shared__ float s_sum[NUM], s_max[NUM];
+        float local_max[NUM];
+#pragma unroll
+        for (int j = 0; j < NUM; j++) {
+            local_max[j] = -1e20f;
+        }
+
+        for (int i = 0; blockDim.x * i + threadIdx.x < (trgt_seq_len / 2) && i < ITEMS_PER_THREAD; i++) {
+            int mask_offset[NUM];
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                qk_offset[j] = ((blockIdx.y * head_num + blockIdx.z) * seq_len + seq_id + j * gridDim.x) * (trgt_seq_len / 2)
+                               + blockDim.x * i + threadIdx.x;
+                mask_offset[j] =
+                    (blockIdx.y * seq_len + seq_id + j * gridDim.x) * (trgt_seq_len / 2) + blockDim.x * i + threadIdx.x;
+                pos_bias_offset[j] = qk_offset[j];
+            }
+
+            T2 mask_val[NUM];
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                mask_val[j] = ldg(&attr_mask_half2[mask_offset[j]]);
+            }
+
+            T2 qk[NUM];
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                qk[j] = qk_buf_half2[qk_offset[j]];
+            }
+            
+            T2 pos_bias_val[NUM];
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                pos_bias_val[j] = ldg(&position_bias_half2[pos_bias_offset[j]]);
+            }
+
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                mask_val[j] = hmul2<T2>(hsub2<T2>(float2type2<T2>(1.0f), mask_val[j]), float2type2<T2>(-10000.0f));
+            }
+
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                data[j][i] = hadd2<T2>(hadd2<T2>(hmul2<T2>(qk[j], type2type2<T, T2>(scalar)), mask_val[j]), pos_bias_val[j]);
+                local_max[j] = fmax(local_max[j], fmax((float)data[j][i].x, (float)data[j][i].y));
+            }
+        }
+
+        if (blockDim.x <= 32) {
+            warpReduceMaxV2<float, NUM>(local_max);
+        }
+        else {
+            blockReduceMaxV2<float, NUM>(local_max);
+        }
+
+        if (threadIdx.x == 0) {
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                s_max[j] = local_max[j];
+            }
+        }
+        __syncthreads();
+
+        float local_sum[NUM];
+#pragma unroll
+        for (int j = 0; j < NUM; j++) {
+            local_sum[j] = {0.f};
+        }
+
+        for (int i = 0; blockDim.x * i + threadIdx.x < (trgt_seq_len / 2) && i < ITEMS_PER_THREAD; i++) {
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                data[j][i] = hexp2<T2>(hsub2<T2>(data[j][i], float2type2<T2>(s_max[j])));
+            }
+
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                local_sum[j] += (float)(data[j][i].x + data[j][i].y);
+            }
+        }
+
+        if (blockDim.x <= 32) {
+            warpReduceSumV2<float, NUM>(local_sum);
+        }
+        else {
+            blockReduceSumV2<float, NUM>(local_sum);
+        }
+
+        if (threadIdx.x == 0) {
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                s_sum[j] = __fdividef(1.0f, local_sum[j] + 1e-6f);
+            }
+        }
+        __syncthreads();
+
+        for (int i = 0; blockDim.x * i + threadIdx.x < (trgt_seq_len / 2) && i < ITEMS_PER_THREAD; i++) {
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                qk_offset[j] = ((blockIdx.y * head_num + blockIdx.z) * seq_len + seq_id + j * gridDim.x) * (trgt_seq_len / 2)
+                               + blockDim.x * i + threadIdx.x;
+            }
+
+#pragma unroll
+            for (int j = 0; j < NUM; j++) {
+                qk_buf_half2[qk_offset[j]] = hmul2<T2>(data[j][i], float2type2<T2>(s_sum[j]));
+            }
+        }
+    }
+}
+
+
+#define SOFTMAX_KERNEL(ITEMS_PER_THREAD)                                                                               \
+    block.x /= ITEMS_PER_THREAD;                                                                                       \
+    assert(block.x <= 1024);                                                                                           \
+    if (is_half2) {                                                                                                    \
+        if (grid.x % 4 == 0) {                                                                                         \
+            grid.x /= 4;                                                                                               \
+            softmax_kernel_v5_half2<half, ITEMS_PER_THREAD, 4><<<grid, block, 0, stream>>>(                            \
+                (half*)buffer, (const half*)attr_mask, batch_size, head_num, seq_len, (const half)scalar);             \
+        }                                                                                                              \
+        else {                                                                                                         \
+            softmax_kernel_v4_half2<half, ITEMS_PER_THREAD><<<grid, block, 0, stream>>>(                               \
+                (half*)buffer, (const half*)attr_mask, batch_size, head_num, seq_len, (const half)scalar);             \
+        }                                                                                                              \
+    }                                                                                                                  \
+    else {                                                                                                             \
+        softmax_kernel_v4<ITEMS_PER_THREAD, T, T_IN>                                                                   \
+            <<<grid, block, 0, stream>>>(buffer, buffer_src, attr_mask, batch_size, head_num, seq_len, scalar);        \
+    }
+
+#define SOFTMAX_CROSS_KERNEL(ITEMS_PER_THREAD)                                                                               \
+    block.x /= ITEMS_PER_THREAD;                                                                                       \
+    assert(block.x <= 1024);                                                                                           \
+    if (is_half2) {                                                                                                    \
+        if (grid.x % 4 == 0) {                                                                                         \
+            grid.x /= 4;                                                                                               \
+            softmax_cross_kernel_v5_half2<half, ITEMS_PER_THREAD, 4><<<grid, block, 0, stream>>>(                      \
+                (half*)buffer, (const half*)attr_mask, batch_size, head_num, seq_len, trgt_seq_len,                    \
+                (const half)scalar);                                                                                   \
+        }                                                                                                              \
+        else {                                                                                                         \
+            softmax_cross_kernel_v4_half2<half, ITEMS_PER_THREAD><<<grid, block, 0, stream>>>(                         \
+                (half*)buffer, (const half*)attr_mask, batch_size, head_num, seq_len, trgt_seq_len,                    \
+                 (const half)scalar);                                                                                  \
+        }                                                                                                              \
+    }                                                                                                                  \
+    else {                                                                                                             \
+        softmax_cross_kernel_v4<ITEMS_PER_THREAD, T, T_IN>                                                             \
+        <<<grid, block, 0, stream>>>(buffer, buffer_src, attr_mask, batch_size, head_num, seq_len,                     \
+        trgt_seq_len, scalar);                                                                                         \
+    }
+
+#define SOFTMAX_MIX_KERNEL(ITEMS_PER_THREAD)                                                                               \
+    block.x /= ITEMS_PER_THREAD;                                                                                       \
+    assert(block.x <= 1024);                                                                                           \
+    if (is_half2) {                                                                                                    \
+        if (grid.x % 4 == 0) {                                                                                         \
+            grid.x /= 4;                                                                                               \
+            softmax_cross_kernel_v5_half2<half, ITEMS_PER_THREAD, 4><<<grid, block, 0, stream>>>(                      \
+                (half*)io_buffer, (const half*)attr_mask, batch_size, head_num, seq_len, trgt_seq_len,                    \
+                (const half)scalar);                                                                                   \
+        }                                                                                                              \
+        else {                                                                                                         \
+            softmax_cross_kernel_v4_half2<half, ITEMS_PER_THREAD><<<grid, block, 0, stream>>>(                         \
+                (half*)io_buffer, (const half*)attr_mask, batch_size, head_num, seq_len, trgt_seq_len,                    \
+                 (const half)scalar);                                                                                  \
+        }                                                                                                              \
+    }                                                                                                                  \
+    else {                                                                                                             \
+        softmax_mix_kernel_v4<ITEMS_PER_THREAD, T, T_M>                                                             \
+        <<<grid, block, 0, stream>>>(io_buffer, attr_mask, batch_size, head_num, seq_len,                     \
+        trgt_seq_len, scalar);                                                                                         \
+    }
+
+#define SOFTMAX_MIX_KERNEL_BIAS(ITEMS_PER_THREAD)                                                                               \
+    block.x /= ITEMS_PER_THREAD;                                                                                       \
+    assert(block.x <= 1024);                                                                                           \
+    if (is_half2) {                                                                                                      \
+        if (grid.x % 4 == 0) {                                                                                         \
+            grid.x /= 4;                                                                                               \
+            softmax_cross_kernel_bias_v5_half2<half, ITEMS_PER_THREAD, 4><<<grid, block, 0, stream>>>(                      \
+                (half*)io_buffer, (const half*)attr_mask, (const half*)position_bias, batch_size, head_num, seq_len, trgt_seq_len,                    \
+                (const half)scalar);                                                                                   \
+        }                                                                                                              \
+        else {                                                                                                         \
+            softmax_cross_kernel_bias_v4_half2<half, ITEMS_PER_THREAD><<<grid, block, 0, stream>>>(                         \
+                (half*)io_buffer, (const half*)attr_mask, (const half*)position_bias, batch_size, head_num, seq_len, trgt_seq_len,                    \
+                 (const half)scalar);                                                                                  \
+        }                                                                                                              \
+    }                                                                                                                  \
+    else {                                                                                                             \
+        softmax_mix_kernel_bias_v4<ITEMS_PER_THREAD, T, T_M>                                                             \
+        <<<grid, block, 0, stream>>>(io_buffer, attr_mask, position_bias, batch_size, head_num, seq_len,                     \
+        trgt_seq_len, scalar);                                                                                         \
+    }
+
+#ifdef ENABLE_BF16
+#define SOFTMAX_KERNEL_BF16(ITEMS_PER_THREAD)                                                                          \
     block.x /= ITEMS_PER_THREAD;                                                                                       \
     assert(block.x <= 1024);                                                                                           \
     if (is_half2) {                                                                                                    \
@@ -501,6 +1117,120 @@ void invokeMaskedSoftMax(T* buffer,
     }
 }
 
+template<typename T, typename T_IN>
+void invokeCrossMaskedSoftMax(T* buffer,
+                         const T_IN* buffer_src,
+                         const T* attr_mask,
+                         const int batch_size,
+                         const int seq_len,
+                         const int trgt_seq_len,
+                         const int head_num,
+                         const T scalar,
+                         cudaStream_t stream)
+{
+
+    dim3 grid(seq_len, batch_size, head_num);
+    if (batch_size * head_num > 360) {
+        grid.x = ceil(float(seq_len) / 32.0f);
+    }
+
+    bool is_half2 = sizeof(T) == 2 && sizeof(T_IN) == 2 && trgt_seq_len % 2 == 0;
+    dim3 block((trgt_seq_len / (is_half2 ? 2 : 1) + 31) / 32 * 32);
+
+    if (block.x > 3072 && block.x <= 4096) {
+        SOFTMAX_CROSS_KERNEL(4)
+    }
+    if (block.x > 2048) {
+        SOFTMAX_CROSS_KERNEL(3)
+    }
+    else if (block.x > 1024) {
+        SOFTMAX_CROSS_KERNEL(2)
+    }
+    else if (block.x > 0) {
+        SOFTMAX_CROSS_KERNEL(1)
+    }
+    else {
+        FT_CHECK(trgt_seq_len <= 4096 || seq_len <= 4096);
+    }
+}
+
+
+template<typename T, typename T_M>
+void invokeMixMaskedSoftMax(T* io_buffer,
+                         const T_M* attr_mask,
+                         const int batch_size,
+                         const int seq_len,
+                         const int trgt_seq_len,
+                         const int head_num,
+                         const T scalar,
+                         cudaStream_t stream)
+{
+
+    dim3 grid(seq_len, batch_size, head_num);
+    if (batch_size * head_num > 360) {
+        grid.x = ceil(float(seq_len) / 32.0f);
+    }
+
+    bool is_half2 = sizeof(T) == 2 && sizeof(T_M) == 2 && trgt_seq_len % 2 == 0;
+    dim3 block((trgt_seq_len / (is_half2 ? 2 : 1) + 31) / 32 * 32);
+
+    if (block.x > 3072 && block.x <= 4096) {
+        SOFTMAX_MIX_KERNEL(4)
+    }
+    if (block.x > 2048) {
+        SOFTMAX_MIX_KERNEL(3)
+    }
+    else if (block.x > 1024) {
+        SOFTMAX_MIX_KERNEL(2)
+    }
+    else if (block.x > 0) {
+        SOFTMAX_MIX_KERNEL(1)
+    }
+    else {
+        FT_CHECK(trgt_seq_len <= 4096 || seq_len <= 4096);
+    }
+}
+
+template<typename T, typename T_M>
+void invokeMixMaskedSoftMax(T* io_buffer,
+                         const T_M* attr_mask,
+                         const T* position_bias,
+                         const int batch_size,
+                         const int seq_len,
+                         const int trgt_seq_len,
+                         const int head_num,
+                         const T scalar,
+                         cudaStream_t stream)
+{
+    if (position_bias == nullptr) {
+        invokeMixMaskedSoftMax(io_buffer, attr_mask, batch_size, seq_len, trgt_seq_len, head_num, scalar, stream);
+    } else {
+        dim3 grid(seq_len, batch_size, head_num);
+        if (batch_size * head_num > 360) {
+            grid.x = ceil(float(seq_len) / 32.0f);
+        }
+
+        bool is_half2 = sizeof(T) == 2 && sizeof(T_M) == 2 && trgt_seq_len % 2 == 0;
+        dim3 block((trgt_seq_len / (is_half2 ? 2 : 1) + 31) / 32 * 32);
+
+        if (block.x > 3072 && block.x <= 4096) {
+            SOFTMAX_MIX_KERNEL_BIAS(4)
+        }
+        if (block.x > 2048) {
+            SOFTMAX_MIX_KERNEL_BIAS(3)
+        }
+        else if (block.x > 1024) {
+            SOFTMAX_MIX_KERNEL_BIAS(2)
+        }
+        else if (block.x > 0) {
+            SOFTMAX_MIX_KERNEL_BIAS(1)
+        }
+        else {
+            FT_CHECK(trgt_seq_len <= 4096 || seq_len <= 4096);
+        }
+    }
+}
+
 #ifdef ENABLE_BF16
 template<>
 void invokeMaskedSoftMax(__nv_bfloat16* buffer,
@@ -574,13 +1304,118 @@ void invokeMaskedSoftMax(__nv_bfloat16* buffer,
         FT_CHECK(seq_len <= 4096);
     }
 }
+
+template<>
+void invokeCrossMaskedSoftMax(__nv_bfloat16* buffer,
+                         const __nv_bfloat16* buffer_src,
+                         const __nv_bfloat16* attr_mask,
+                         const int batch_size,
+                         const int seq_len, const int trgt_seq_len,
+                         const int head_num,
+                         const __nv_bfloat16 scalar,
+                         cudaStream_t stream) {;}
+
+template<>
+void invokeCrossMaskedSoftMax(__nv_bfloat16* buffer,
+                         const float* buffer_src,
+                         const __nv_bfloat16* attr_mask,
+                         const int batch_size,
+                         const int seq_len, const int trgt_seq_len,
+                         const int head_num,
+                         const __nv_bfloat16 scalar,
+                         cudaStream_t stream) {;}
 #endif  // ENABLE_BF16
 
+template void invokeMixMaskedSoftMax(float* io_buffer,
+                            const float* attr_mask,
+                            const int batch_size,
+                            const int seq_len,
+                            const int tgt_seq_len,
+                            const int head_num,
+                            const float scalar,
+                            cudaStream_t stream);
+
+template void invokeMixMaskedSoftMax(half* io_buffer,
+                            const half* attr_mask,
+                            const int batch_size,
+                            const int seq_len,
+                            const int tgt_seq_len,
+                            const int head_num,
+                            const half scalar,
+                            cudaStream_t stream);
+
+template void invokeMixMaskedSoftMax(float* io_buffer,
+                            const half* attr_mask,
+                            const int batch_size,
+                            const int seq_len,
+                            const int tgt_seq_len,
+                            const int head_num,
+                            const float scalar,
+                            cudaStream_t stream);
+
+template void invokeMixMaskedSoftMax(half* io_buffer,
+                            const float* attr_mask,
+                            const int batch_size,
+                            const int seq_len,
+                            const int tgt_seq_len,
+                            const int head_num,
+                            const half scalar,
+                            cudaStream_t stream);
+
+template void invokeMixMaskedSoftMax(float* io_buffer,
+                            const float* attr_mask,
+                            const float* position_bias,
+                            const int batch_size,
+                            const int seq_len,
+                            const int tgt_seq_len,
+                            const int head_num,
+                            const float scalar,
+                            cudaStream_t stream);
+
+template void invokeMixMaskedSoftMax(half* io_buffer,
+                            const half* attr_mask,
+                            const half* position_bias,
+                            const int batch_size,
+                            const int seq_len,
+                            const int tgt_seq_len,
+                            const int head_num,
+                            const half scalar,
+                            cudaStream_t stream);
+
+template void invokeMixMaskedSoftMax(float* io_buffer,
+                            const half* attr_mask,
+                            const float* position_bias,
+                            const int batch_size,
+                            const int seq_len,
+                            const int tgt_seq_len,
+                            const int head_num,
+                            const float scalar,
+                            cudaStream_t stream);
+
+template void invokeMixMaskedSoftMax(half* io_buffer,
+                            const float* attr_mask,
+                            const half* position_bias,
+                            const int batch_size,
+                            const int seq_len,
+                            const int tgt_seq_len,
+                            const int head_num,
+                            const half scalar,
+                            cudaStream_t stream);
+
 template void invokeMaskedSoftMax(float* buffer,
+                                      const float* buffer_src,
+                                      const float* attr_mask,
+                                      const int batch_size,
+                                      const int seq_len,
+                                      const int head_num,
+                                      const float scalar,
+                                      cudaStream_t stream);
+
+template void invokeCrossMaskedSoftMax(float* buffer,
                                   const float* buffer_src,
                                   const float* attr_mask,
                                   const int batch_size,
-                                  const int seq_len,
+                                  const int seq_len, const int trgt_seq_len,
                                   const int head_num,
                                   const float scalar,
                                   cudaStream_t stream);
@@ -594,6 +1429,15 @@ template void invokeMaskedSoftMax(half* buffer,
                                   const half scalar,
                                   cudaStream_t stream);
 
+template void invokeCrossMaskedSoftMax(half* buffer,
+                                  const float* buffer_src,
+                                  const half* attr_mask,
+                                  const int batch_size,
+                                  const int seq_len, const int trgt_seq_len,
+                                  const int head_num,
+                                  const half scalar,
+                                  cudaStream_t stream);
+
 template void invokeMaskedSoftMax(half* buffer,
                                   const half* buffer_src,
                                   const half* attr_mask,
@@ -603,6 +1447,15 @@ template void invokeMaskedSoftMax(half* buffer,
                                   const half scalar,
                                   cudaStream_t stream);
 
+template void invokeCrossMaskedSoftMax(half* buffer,
+                                  const half* buffer_src,
+                                  const half* attr_mask,
+                                  const int batch_size,
+                                  const int seq_len, const int trgt_seq_len,
+                                  const int head_num,
+                                  const half scalar,
+                                  cudaStream_t stream);
+
 #ifdef ENABLE_BF16
 template void invokeMaskedSoftMax(__nv_bfloat16* buffer,
                                   const __nv_bfloat16* buffer_src,
@@ -621,6 +1474,25 @@ template void invokeMaskedSoftMax(__nv_bfloat16* buffer,
                                   const int head_num,
                                   const __nv_bfloat16 scalar,
                                   cudaStream_t stream);
+
+template void invokeCrossMaskedSoftMax(__nv_bfloat16* buffer,
+                                  const __nv_bfloat16* buffer_src,
+                                  const __nv_bfloat16* attr_mask,
+                                  const int batch_size,
+                                  const int seq_len,  const int trgt_seq_len,
+                                  const int head_num,
+                                  const __nv_bfloat16 scalar,
+                                  cudaStream_t stream);
+
+template void invokeCrossMaskedSoftMax(__nv_bfloat16* buffer,
+                                  const float* buffer_src,
+                                  const __nv_bfloat16* attr_mask,
+                                  const int batch_size,
+                                  const int seq_len,  const int trgt_seq_len,
+                                  const int head_num,
+                                  const __nv_bfloat16 scalar,
+                                  cudaStream_t stream);
+
 #endif  // ENABLE_BF16
 
 template<typename T>
@@ -726,9 +1598,10 @@ void invokeTransposeQKV(T* dst,
             seq_per_block *= 2;
         }
 
-        FT_CHECK(grid.x * seq_per_block == batch_size * head_num * seq_len);
+        FT_CHECK((int)(grid.x * seq_per_block) == batch_size * head_num * seq_len);
 
-        if (seq_per_block * size_per_head % 2 == 0) {
+        // if (seq_per_block * size_per_head % 2 == 0) {
+        if (size_per_head % 2 == 0) {            
             block.x = seq_per_block * size_per_head / 2;
             if (std::is_same<T, half>::value) {
                 transpose<half2><<<grid, block, 0, stream>>>(
@@ -993,13 +1866,16 @@ __global__ void transpose_remove_padding(const T* src,
 
     const int dst_seq_id = bid;
 
+    const int src_offset_base = src_batch_id * seq_len * head_num * size_per_head + src_seq_id * size_per_head;
+    const int dst_offset_base = dst_seq_id * head_num * size_per_head;
+
+    
     for (int idx = threadIdx.x; idx < head_num * size_per_head; idx += blockDim.x) {
         const int head_id = idx / size_per_head;
         const int hidden_id = idx % size_per_head;
-        dst[dst_seq_id * head_num * size_per_head + idx] =
-            __ldg(&src[src_batch_id * head_num * seq_len * size_per_head + head_id * seq_len * size_per_head
-                       + src_seq_id * size_per_head + hidden_id]);
-    }
+        const T   src_elem  = ldg(&src[src_offset_base + head_id * seq_len * size_per_head + hidden_id]);
+        dst[dst_offset_base + idx] = src_elem;
+        }
 }
 
 template<typename T>
@@ -1061,33 +1937,349 @@ template void invokeTransposeAttentionOutRemovePadding(half* src,
                                                        const int* mask_offset,
                                                        cudaStream_t stream);
 
-template<typename T>
-__global__ void add_fusedQKV_bias_transpose_kernel(T* q_buf,
-                                                   T* k_buf,
-                                                   T* v_buf,
+template<typename T, typename U=T>
+__global__ void add_fusedQKV_bias_transpose_kernel(T* q_buf,
+                                                   T* k_buf,
+                                                   T* v_buf,
+                                                   const T* __restrict QKV,
+                                                   const U* __restrict qkv_bias,
+                                                   const int batch_size,
+                                                   const int seq_len,
+                                                   const int head_num,
+                                                   const int size_per_head)
+{
+    // QKV: [m, 3, n]
+    // qkv_bias: [3, n]
+    // q_buf, k_buf, v_buf: [batch, head_num, seq_len, size_per_head]
+
+    T* qkv_ptr[3] = {q_buf, k_buf, v_buf};
+    const int n = head_num * size_per_head;
+    for (int index = blockDim.x * blockIdx.x + threadIdx.x; index < batch_size * seq_len * 3 * n;
+        index += gridDim.x * blockDim.x) {
+        int bias_id = index % (3 * n);
+        T val = ldg(&QKV[index]) + (T)ldg(&qkv_bias[bias_id]);
+        int tmp_index = index;
+        const int target_batch_id = tmp_index / (seq_len * 3 * n);
+        tmp_index -= target_batch_id * seq_len * 3 * n;
+        const int seq_id = tmp_index / (3 * n);
+        tmp_index -= seq_id * 3 * n;
+        const int qkv_id = tmp_index / n;
+        tmp_index -= qkv_id * n;
+        const int head_id = tmp_index / size_per_head;
+        const int size_id = tmp_index - head_id * size_per_head;
+
+        qkv_ptr[qkv_id][target_batch_id * head_num * seq_len * size_per_head + head_id * seq_len * size_per_head
+                        + seq_id * size_per_head + size_id] = val;
+    }
+}
+
+template<typename T, typename U=T>
+__global__ void transposeQKV_kernel(T* q_buf,
+                                    T* k_buf,
+                                    T* v_buf,
+                                    const T* __restrict QKV,
+                                    const int batch_size,
+                                    const int seq_len,
+                                    const int head_num,
+                                    const int size_per_head)
+{
+    // QKV: [m, 3, n]
+    // qkv_bias: [3, n]
+    // q_buf, k_buf, v_buf: [batch, head_num, seq_len, size_per_head]
+
+    T* qkv_ptr[3] = {q_buf, k_buf, v_buf};
+    const int n = head_num * size_per_head;
+    for (int index = blockDim.x * blockIdx.x + threadIdx.x; index < batch_size * seq_len * 3 * n;
+        index += gridDim.x * blockDim.x) {
+        T val = ldg(&QKV[index]); 
+        int tmp_index = index;
+        const int target_batch_id = tmp_index / (seq_len * 3 * n);
+        tmp_index -= target_batch_id * seq_len * 3 * n;
+        const int seq_id = tmp_index / (3 * n);
+        tmp_index -= seq_id * 3 * n;
+        const int qkv_id = tmp_index / n;
+        tmp_index -= qkv_id * n;
+        const int head_id = tmp_index / size_per_head;
+        const int size_id = tmp_index - head_id * size_per_head;
+        // const int dst_id = target_batch_id * head_num * seq_len * size_per_head + head_id * seq_len * size_per_head
+        //                 + seq_id * size_per_head + size_id;
+         qkv_ptr[qkv_id][target_batch_id * head_num * seq_len * size_per_head + head_id * seq_len * size_per_head
+                        + seq_id * size_per_head + size_id] = val;
+    }
+}
+
+
+template<typename T, typename U=T>
+__global__ void add_fusedQKV_ZP_bias_transpose_kernel(T* q_buf,
+                                                   T* k_buf,
+                                                   T* v_buf,
+                                                   const T* __restrict QKV,
+                                                   const U* __restrict qkv_bias,
+                                                   const int batch_size,
+                                                   const int seq_len,
+                                                   const int head_num,
+                                                   const int size_per_head,
+                                                   const int token_num,
+                                                   int *mask_offset)
+{
+    // QKV: [m, 3, n]
+    // qkv_bias: [3, n]
+    // q_buf, k_buf, v_buf: [batch, head_num, seq_len, size_per_head]
+
+    T* qkv_ptr[3] = {q_buf, k_buf, v_buf};
+    const int n = head_num * size_per_head;
+    for (int index = blockDim.x * blockIdx.x + threadIdx.x; index < token_num * 3 * n;   
+         index += gridDim.x * blockDim.x) {
+        int bias_id = index % (3 * n); // 0 - 160
+        T val = ldg(&QKV[index]); 
+        if (qkv_bias != nullptr)
+            val += (T)ldg(&qkv_bias[bias_id]);
+        int tmp_index = index; //0 -160 * 3 * n
+        int token_id = tmp_index / (3 *n); 
+        int batch_id = (token_id + ldg(&mask_offset[token_id])) / seq_len;  
+        int seq_id = (token_id + ldg(&mask_offset[token_id])) % seq_len;   
+        tmp_index -= token_id * 3 * n; 
+        int qkv_id = tmp_index / n;
+        tmp_index -= qkv_id * n;
+        int head_id = tmp_index / size_per_head;
+        int size_id = tmp_index - head_id * size_per_head;
+        int dst_id = batch_id * head_num * seq_len * size_per_head + head_id * seq_len * size_per_head
+                        + seq_id * size_per_head + size_id;
+         qkv_ptr[qkv_id][dst_id] = val;
+    }
+}
+
+
+
+
+
+template<typename T>
+struct Vec_t {};
+template<>
+struct Vec_t<float> {
+    using Type = float2;
+};
+template<>
+struct Vec_t<half> {
+    using Type = uint32_t;
+};
+
+#ifdef ENABLE_BF16
+template<>
+struct Vec_t<__nv_bfloat16> {
+    using Type = __nv_bfloat162;
+};
+#endif
+
+template<typename T,typename U=T>
+__global__ void add_fusedQKV_bias_transpose_kernel(T* q_buf,
+                                                   T* k_buf,
+                                                   T* v_buf,
+                                                   const T* __restrict QKV,
+                                                   const U* __restrict qkv_bias,
+                                                   const int batch_size,
+                                                   const int seq_len,
+                                                   const int head_num,
+                                                   const int size_per_head,
+                                                   const int rotary_embedding_dim)
+{
+    using Vec_t = typename Vec_t<T>::Type;
+    const int batch_idx = blockIdx.z;
+    const int head_idx = blockIdx.y;
+    const int seq_idx = blockIdx.x;
+    const int tidx = threadIdx.x;
+    if (tidx * 2 >= size_per_head) {
+        return;
+    }
+
+    const int batch_time_idx = seq_len * batch_idx + seq_idx;
+    const int hidden_idx = head_idx * size_per_head + tidx * 2;
+    const int n = head_num * size_per_head;
+
+    // src QKV: [batch, time, 3, head, hidden]
+    const int q_idx = batch_time_idx * 3 * n + hidden_idx;
+    const int k_idx = batch_time_idx * 3 * n + hidden_idx + n;
+    const int v_idx = batch_time_idx * 3 * n + hidden_idx + 2 * n;
+
+    Vec_t q = *reinterpret_cast<const Vec_t*>(&QKV[q_idx]);
+    Vec_t k = *reinterpret_cast<const Vec_t*>(&QKV[k_idx]);
+    Vec_t v = *reinterpret_cast<const Vec_t*>(&QKV[v_idx]);
+
+    // qkv_bias: [3, head, hidden]
+    Vec_t q_bias = *reinterpret_cast<const Vec_t*>(&qkv_bias[hidden_idx]);
+    Vec_t k_bias = *reinterpret_cast<const Vec_t*>(&qkv_bias[hidden_idx + n]);
+    Vec_t v_bias = *reinterpret_cast<const Vec_t*>(&qkv_bias[hidden_idx + 2 * n]);
+
+    q = mmha::add(q, q_bias);
+    k = mmha::add(k, k_bias);
+    v = mmha::add(v, v_bias);
+
+    mmha::apply_rotary_embedding(q, k, tidx, rotary_embedding_dim, seq_idx);
+
+    // q_buf, k_buf, v_buf: [batch, head_num, seq_len, size_per_head]
+    const int dest_idx = size_per_head * seq_len * head_num * batch_idx + size_per_head * seq_len * head_idx
+                         + size_per_head * seq_idx + tidx * 2;
+
+    *reinterpret_cast<Vec_t*>(&q_buf[dest_idx]) = q;
+    *reinterpret_cast<Vec_t*>(&k_buf[dest_idx]) = k;
+    *reinterpret_cast<Vec_t*>(&v_buf[dest_idx]) = v;
+}
+
+template<typename T, typename U>
+void invokeAddFusedQKVBiasTranspose(T* q_buf,
+                                    T* k_buf,
+                                    T* v_buf,
+                                    T* QKV,
+                                    const U* qkv_bias,
+                                    const int batch_size,
+                                    const int seq_len,
+                                    const int head_num,
+                                    const int size_per_head,
+                                    const int rotary_embedding_dim,
+                                    cudaStream_t stream)
+{
+    if (qkv_bias != nullptr) {
+        if (rotary_embedding_dim == 0) {
+            const int m = batch_size * seq_len;
+            const int n = head_num * size_per_head;
+            dim3 block(384);
+            dim3 grid((int)(ceil(1.0 * m * n / 384)));
+            add_fusedQKV_bias_transpose_kernel<<<grid, block, 0, stream>>>(
+                q_buf, k_buf, v_buf, QKV, qkv_bias, batch_size, seq_len, head_num, size_per_head);
+        }
+        else {
+            // To implement rotary embeddings, each thread processes two QKV elems:
+            dim3 block((size_per_head / 2 + 31) / 32 * 32);
+            dim3 grid(seq_len, head_num, batch_size);
+            add_fusedQKV_bias_transpose_kernel<<<grid, block, 0, stream>>>(
+                q_buf, k_buf, v_buf, QKV, qkv_bias, batch_size, seq_len, head_num, size_per_head, rotary_embedding_dim);
+        }
+    } else {
+        const int m = batch_size * seq_len;
+        const int n = head_num * size_per_head;
+        dim3 block(384);
+        dim3 grid((int)(ceil(1.0 * m * n / 384)));
+        transposeQKV_kernel<<<grid, block, 0, stream>>>(
+            q_buf, k_buf, v_buf, QKV, batch_size, seq_len, head_num, size_per_head);
+    }
+}
+
+template<typename T, typename U>
+void invokeAddFusedZP_QKVBiasTranspose(T* q_buf,
+                                    T* k_buf,
+                                    T* v_buf,
+                                    T* QKV,
+                                    const U* qkv_bias,
+                                    const int batch_size,
+                                    const int seq_len,
+                                    const int head_num,
+                                    const int size_per_head,
+                                    const int h_token,
+                                    int *padding_mask,
+                                    cudaStream_t stream)
+{
+       
+       const int m = h_token;
+       const int n = head_num * size_per_head;
+       cudaMemsetAsync(q_buf, 0, batch_size * seq_len * n * sizeof(T), stream);
+       cudaMemsetAsync(k_buf, 0, batch_size * seq_len * n * sizeof(T), stream);
+       cudaMemsetAsync(v_buf, 0, batch_size * seq_len * n * sizeof(T), stream);
+       dim3 block(384);
+       dim3 grid((int)(ceil(1.0 * m * n / 384)));
+       add_fusedQKV_ZP_bias_transpose_kernel<<<grid, block, 0, stream>>>(
+            q_buf, k_buf, v_buf, QKV, qkv_bias, batch_size, seq_len, head_num, size_per_head,h_token, padding_mask);
+
+}
+
+
+template void invokeAddFusedZP_QKVBiasTranspose(float* q_buf,
+                                    float* k_buf,
+                                    float* v_buf,
+                                    float* QKV,
+                                    const float* qkv_bias,
+                                    const int batch_size,
+                                    const int seq_len,
+                                    const int head_num,
+                                    const int size_per_head,
+                                    const int h_token,
+                                    int *padding_mask,
+                                    cudaStream_t stream);
+
+template void invokeAddFusedZP_QKVBiasTranspose(half* q_buf,
+                                    half* k_buf,
+                                    half* v_buf,
+                                    half* QKV,
+                                    const half* qkv_bias,
+                                    const int batch_size,
+                                    const int seq_len,
+                                    const int head_num,
+                                    const int size_per_head,
+                                    const int h_token,
+                                    int *padding_mask,
+                                    cudaStream_t stream);
+
+
+
+
+
+
+template<typename T, typename U=T>
+__global__ void invokeCrossAddFusedQKVBiasTransposeQ(T* q_buf,                                            
+                                                   const T* __restrict QKV,
+                                                   const U* __restrict qkv_bias,
+                                                   const int batch_size,
+                                                   const int seq_len,
+                                                   const int head_num,
+                                                   const int size_per_head)
+{
+    // QKV: [m, 1, n]
+    // qkv_bias: [1, n]
+    // q_buf: [batch, head_num, seq_len, size_per_head]
+
+    T* qkv_ptr[1] = {q_buf};
+    const int n = head_num * size_per_head;
+    for (int index = blockDim.x * blockIdx.x + threadIdx.x; index < batch_size * seq_len * 1 * n;
+         index += gridDim.x * blockDim.x) {
+        int bias_id = index % (1 * n);
+        T val = ldg(&QKV[index]) + (T)ldg(&qkv_bias[bias_id]);
+
+        int tmp_index = index;
+        const int target_batch_id = tmp_index / (seq_len * 1 * n);
+        tmp_index -= target_batch_id * seq_len * 1 * n;
+        const int seq_id = tmp_index / (1 * n);
+        tmp_index -= seq_id * 1 * n;
+        const int qkv_id = tmp_index / n;
+        tmp_index -= qkv_id * n;
+        const int head_id = tmp_index / size_per_head;
+        const int size_id = tmp_index - head_id * size_per_head;
+
+        qkv_ptr[qkv_id][target_batch_id * head_num * seq_len * size_per_head + head_id * seq_len * size_per_head
+                        + seq_id * size_per_head + size_id] = val;
+    }
+}
+
+template<typename T, typename U=T>
+__global__ void invokeCrossTransposeQ(T* q_buf,                                            
                                                    const T* __restrict QKV,
-                                                   const T* __restrict qkv_bias,
                                                    const int batch_size,
                                                    const int seq_len,
                                                    const int head_num,
                                                    const int size_per_head)
 {
-    // QKV: [m, 3, n]
-    // qkv_bias: [3, n]
-    // q_buf, k_buf, v_buf: [batch, head_num, seq_len, size_per_head]
+    // QKV: [m, 1, n]
+    // q_buf: [batch, head_num, seq_len, size_per_head]
 
-    T* qkv_ptr[3] = {q_buf, k_buf, v_buf};
+    T* qkv_ptr[1] = {q_buf};
     const int n = head_num * size_per_head;
-    for (int index = blockDim.x * blockIdx.x + threadIdx.x; index < batch_size * seq_len * 3 * n;
+    for (int index = blockDim.x * blockIdx.x + threadIdx.x; index < batch_size * seq_len * 1 * n;
          index += gridDim.x * blockDim.x) {
-        int bias_id = index % (3 * n);
-        T val = ldg(&QKV[index]) + ldg(&qkv_bias[bias_id]);
+        T val = ldg(&QKV[index]);
 
         int tmp_index = index;
-        const int target_batch_id = tmp_index / (seq_len * 3 * n);
-        tmp_index -= target_batch_id * seq_len * 3 * n;
-        const int seq_id = tmp_index / (3 * n);
-        tmp_index -= seq_id * 3 * n;
+        const int target_batch_id = tmp_index / (seq_len * 1 * n);
+        tmp_index -= target_batch_id * seq_len * 1 * n;
+        const int seq_id = tmp_index / (1 * n);
+        tmp_index -= seq_id * 1 * n;
         const int qkv_id = tmp_index / n;
         tmp_index -= qkv_id * n;
         const int head_id = tmp_index / size_per_head;
@@ -1098,108 +2290,167 @@ __global__ void add_fusedQKV_bias_transpose_kernel(T* q_buf,
     }
 }
 
-template<typename T>
-struct Vec_t {};
-template<>
-struct Vec_t<float> {
-    using Type = float2;
-};
-template<>
-struct Vec_t<half> {
-    using Type = uint32_t;
-};
-
-#ifdef ENABLE_BF16
-template<>
-struct Vec_t<__nv_bfloat16> {
-    using Type = __nv_bfloat162;
-};
-#endif
-
-template<typename T>
-__global__ void add_fusedQKV_bias_transpose_kernel(T* q_buf,
-                                                   T* k_buf,
-                                                   T* v_buf,
+template<typename T, typename U=T>
+__global__ void invokeCrossAddFusedQKVBiasTransposeKV(T* k_buf, T* v_buf,                                            
                                                    const T* __restrict QKV,
-                                                   const T* __restrict qkv_bias,
+                                                   const U* __restrict qkv_bias,
                                                    const int batch_size,
                                                    const int seq_len,
                                                    const int head_num,
-                                                   const int size_per_head,
-                                                   const int rotary_embedding_dim)
+                                                   const int size_per_head)
 {
-    using Vec_t = typename Vec_t<T>::Type;
-    const int batch_idx = blockIdx.z;
-    const int head_idx = blockIdx.y;
-    const int seq_idx = blockIdx.x;
-    const int tidx = threadIdx.x;
-    if (tidx * 2 >= size_per_head) {
-        return;
-    }
+    // QKV: [m, 2, n]
+    // qkv_bias: [2, n]
+    // k_buf, v_buf: [batch, head_num, seq_len, size_per_head]
 
-    const int batch_time_idx = seq_len * batch_idx + seq_idx;
-    const int hidden_idx = head_idx * size_per_head + tidx * 2;
+    T* qkv_ptr[2] = {k_buf, v_buf};
     const int n = head_num * size_per_head;
+    for (int index = blockDim.x * blockIdx.x + threadIdx.x; index < batch_size * seq_len * 2 * n;
+         index += gridDim.x * blockDim.x) {
+        int bias_id = index % (2 * n);
+        T val = ldg(&QKV[index]) + (T)ldg(&qkv_bias[bias_id]);
 
-    // src QKV: [batch, time, 3, head, hidden]
-    const int q_idx = batch_time_idx * 3 * n + hidden_idx;
-    const int k_idx = batch_time_idx * 3 * n + hidden_idx + n;
-    const int v_idx = batch_time_idx * 3 * n + hidden_idx + 2 * n;
-
-    Vec_t q = *reinterpret_cast<const Vec_t*>(&QKV[q_idx]);
-    Vec_t k = *reinterpret_cast<const Vec_t*>(&QKV[k_idx]);
-    Vec_t v = *reinterpret_cast<const Vec_t*>(&QKV[v_idx]);
-
-    // qkv_bias: [3, head, hidden]
-    Vec_t q_bias = *reinterpret_cast<const Vec_t*>(&qkv_bias[hidden_idx]);
-    Vec_t k_bias = *reinterpret_cast<const Vec_t*>(&qkv_bias[hidden_idx + n]);
-    Vec_t v_bias = *reinterpret_cast<const Vec_t*>(&qkv_bias[hidden_idx + 2 * n]);
+        int tmp_index = index;
+        const int target_batch_id = tmp_index / (seq_len * 2 * n);
+        tmp_index -= target_batch_id * seq_len * 2 * n;
+        const int seq_id = tmp_index / (2 * n);
+        tmp_index -= seq_id * 2 * n;
+        const int qkv_id = tmp_index / n;
+        tmp_index -= qkv_id * n;
+        const int head_id = tmp_index / size_per_head;
+        const int size_id = tmp_index - head_id * size_per_head;
+        //printf("%d %d\n", head_id, size_id);
+        qkv_ptr[qkv_id][target_batch_id * head_num * seq_len * size_per_head + head_id * seq_len * size_per_head
+                        + seq_id * size_per_head + size_id] = val;
+    }
+}
 
-    q = mmha::add(q, q_bias);
-    k = mmha::add(k, k_bias);
-    v = mmha::add(v, v_bias);
 
-    mmha::apply_rotary_embedding(q, k, tidx, rotary_embedding_dim, seq_idx);
+template<typename T, typename U=T>
+__global__ void invokeCrossTransposeKV(T* k_buf, T* v_buf,                                            
+                                                   const T* __restrict QKV,
+                                                   const int batch_size,
+                                                   const int seq_len,
+                                                   const int head_num,
+                                                   const int size_per_head)
+{
+    // QKV: [m, 2, n]
+    // k_buf, v_buf: [batch, head_num, seq_len, size_per_head]
 
-    // q_buf, k_buf, v_buf: [batch, head_num, seq_len, size_per_head]
-    const int dest_idx = size_per_head * seq_len * head_num * batch_idx + size_per_head * seq_len * head_idx
-                         + size_per_head * seq_idx + tidx * 2;
+    T* qkv_ptr[2] = {k_buf, v_buf};
+    const int n = head_num * size_per_head;
+    for (int index = blockDim.x * blockIdx.x + threadIdx.x; index < batch_size * seq_len * 2 * n;
+         index += gridDim.x * blockDim.x) {
+        T val = ldg(&QKV[index]);
 
-    *reinterpret_cast<Vec_t*>(&q_buf[dest_idx]) = q;
-    *reinterpret_cast<Vec_t*>(&k_buf[dest_idx]) = k;
-    *reinterpret_cast<Vec_t*>(&v_buf[dest_idx]) = v;
+        int tmp_index = index;
+        const int target_batch_id = tmp_index / (seq_len * 2 * n);
+        tmp_index -= target_batch_id * seq_len * 2 * n;
+        const int seq_id = tmp_index / (2 * n);
+        tmp_index -= seq_id * 2 * n;
+        const int qkv_id = tmp_index / n;
+        tmp_index -= qkv_id * n;
+        const int head_id = tmp_index / size_per_head;
+        const int size_id = tmp_index - head_id * size_per_head;
+        //printf("%d %d\n", head_id, size_id);
+        qkv_ptr[qkv_id][target_batch_id * head_num * seq_len * size_per_head + head_id * seq_len * size_per_head
+                        + seq_id * size_per_head + size_id] = val;
+    }
 }
 
-template<typename T>
-void invokeAddFusedQKVBiasTranspose(T* q_buf,
+template<typename T, typename U>
+void invokeCrossAddFusedQKVBiasTranspose(T* q_buf,
                                     T* k_buf,
                                     T* v_buf,
                                     T* QKV,
-                                    const T* qkv_bias,
+                                    const U* qkv_bias,
                                     const int batch_size,
                                     const int seq_len,
+                                    const int tgt_seq_len,
                                     const int head_num,
                                     const int size_per_head,
-                                    const int rotary_embedding_dim,
                                     cudaStream_t stream)
 {
-    if (rotary_embedding_dim == 0) {
+    if (qkv_bias != nullptr) {
         const int m = batch_size * seq_len;
         const int n = head_num * size_per_head;
         dim3 block(384);
         dim3 grid((int)(ceil(1.0 * m * n / 384)));
-        add_fusedQKV_bias_transpose_kernel<<<grid, block, 0, stream>>>(
-            q_buf, k_buf, v_buf, QKV, qkv_bias, batch_size, seq_len, head_num, size_per_head);
-    }
-    else {
-        // To implement rotary embeddings, each thread processes two QKV elems:
-        dim3 block((size_per_head / 2 + 31) / 32 * 32);
-        dim3 grid(seq_len, head_num, batch_size);
-        add_fusedQKV_bias_transpose_kernel<<<grid, block, 0, stream>>>(
-            q_buf, k_buf, v_buf, QKV, qkv_bias, batch_size, seq_len, head_num, size_per_head, rotary_embedding_dim);
+        invokeCrossAddFusedQKVBiasTransposeQ<<<grid, block, 0, stream>>>(
+        q_buf, QKV, qkv_bias, batch_size, seq_len, head_num, size_per_head);
+        
+        const int m2 = batch_size * tgt_seq_len;
+        const int n2 = head_num * size_per_head;
+        dim3 block2(384);
+        dim3 grid2((int)(ceil(1.0 * m2 * n2 / 384)));
+        invokeCrossAddFusedQKVBiasTransposeKV<<<grid2, block2, 0, stream>>>(
+        k_buf, v_buf, QKV + m * n, qkv_bias + n2, batch_size, tgt_seq_len, head_num, size_per_head);
+    } else {
+        const int m = batch_size * seq_len;
+        const int n = head_num * size_per_head;
+        dim3 block(384);
+        dim3 grid((int)(ceil(1.0 * m * n / 384)));
+        invokeCrossTransposeQ<<<grid, block, 0, stream>>>(
+        q_buf, QKV, batch_size, seq_len, head_num, size_per_head);
+        
+        const int m2 = batch_size * tgt_seq_len;
+        const int n2 = head_num * size_per_head;
+        dim3 block2(384);
+        dim3 grid2((int)(ceil(1.0 * m2 * n2 / 384)));
+        invokeCrossTransposeKV<<<grid2, block2, 0, stream>>>(
+        k_buf, v_buf, QKV + m * n, batch_size, tgt_seq_len, head_num, size_per_head);
     }
+    
 }
 
+template void invokeCrossAddFusedQKVBiasTranspose(float* q_buf,
+                                             float* k_buf,
+                                             float* v_buf,
+                                             float* QKV,
+                                             const float* qkv_bias,
+                                             const int batch_size,
+                                             const int seq_len,
+                                             const int tgt_seq_len,
+                                             const int head_num,
+                                             const int size_per_head,
+                                             cudaStream_t stream);
+
+template void invokeCrossAddFusedQKVBiasTranspose(half* q_buf,
+                                             half* k_buf,
+                                             half* v_buf,
+                                             half* QKV,
+                                             const half* qkv_bias,
+                                             const int batch_size,
+                                             const int seq_len,
+                                             const int tgt_seq_len,
+                                             const int head_num,
+                                             const int size_per_head,
+                                             cudaStream_t stream);
+
+template void invokeCrossAddFusedQKVBiasTranspose(float* q_buf,
+                                             float* k_buf,
+                                             float* v_buf,
+                                             float* QKV,
+                                             const half* qkv_bias,
+                                             const int batch_size,
+                                             const int seq_len,
+                                             const int tgt_seq_len,
+                                             const int head_num,
+                                             const int size_per_head,
+                                             cudaStream_t stream);
+
+template void invokeCrossAddFusedQKVBiasTranspose(half* q_buf,
+                                             half* k_buf,
+                                             half* v_buf,
+                                             half* QKV,
+                                             const float* qkv_bias,
+                                             const int batch_size,
+                                             const int seq_len,
+                                             const int tgt_seq_len,
+                                             const int head_num,
+                                             const int size_per_head,
+                                             cudaStream_t stream);
+
 template void invokeAddFusedQKVBiasTranspose(float* q_buf,
                                              float* k_buf,
                                              float* v_buf,
@@ -1224,6 +2475,30 @@ template void invokeAddFusedQKVBiasTranspose(half* q_buf,
                                              const int rotary_embedding_dim,
                                              cudaStream_t stream);
 
+template void invokeAddFusedQKVBiasTranspose(float* q_buf,
+                                             float* k_buf,
+                                             float* v_buf,
+                                             float* QKV,
+                                             const half* qkv_bias,
+                                             const int batch_size,
+                                             const int seq_len,
+                                             const int head_num,
+                                             const int size_per_head,
+                                             const int rotary_embedding_dim,
+                                             cudaStream_t stream);
+
+template void invokeAddFusedQKVBiasTranspose(half* q_buf,
+                                             half* k_buf,
+                                             half* v_buf,
+                                             half* QKV,
+                                             const float* qkv_bias,
+                                             const int batch_size,
+                                             const int seq_len,
+                                             const int head_num,
+                                             const int size_per_head,
+                                             const int rotary_embedding_dim,
+                                             cudaStream_t stream);
+
 #ifdef ENABLE_BF16
 template void invokeAddFusedQKVBiasTranspose(__nv_bfloat16* q_buf,
                                              __nv_bfloat16* k_buf,
@@ -1236,6 +2511,19 @@ template void invokeAddFusedQKVBiasTranspose(__nv_bfloat16* q_buf,
                                              const int size_per_head,
                                              const int rotary_embedding_dim,
                                              cudaStream_t stream);
+
+template void invokeCrossAddFusedQKVBiasTranspose(__nv_bfloat16* q_buf,
+                                             __nv_bfloat16* k_buf,
+                                             __nv_bfloat16* v_buf,
+                                             __nv_bfloat16* QKV,
+                                             const __nv_bfloat16* qkv_bias,
+                                             const int batch_size,
+                                             const int seq_len,
+                                             const int tgt_seq_len,
+                                             const int head_num,
+                                             const int size_per_head,
+                                             cudaStream_t stream);
+
 #endif
 
 template<typename T>
@@ -1860,4 +3148,423 @@ template void invokeMaskedSoftMaxWithRelPosBias(half* qk_buf,
                                                 const float qk_scale,
                                                 cudaStream_t stream);
 
+
+
+template<typename T>
+__global__ void attention_kernel(T* query_buf,
+                                       const T* Q_bias,
+                                       T* key_cache,
+                                       const T* K_bias,
+                                       T* value_cache,
+                                       const T* V_bias,
+                                       const int* length_per_sample,
+                                       T* context_buf,
+                                       const bool* finished,
+                                       int batch_size,
+                                       int head_num,
+                                       int size_per_head,
+                                       int step,
+                                       const int seq_len,
+                                       const T scalar)
+{
+    if (finished != nullptr && finished[blockIdx.x / head_num] == true) {
+        return;
+    }
+    int tid = threadIdx.x;
+    int bid = blockIdx.x / head_num;
+    int head_id = blockIdx.x % head_num;
+
+    extern __shared__ __align__(sizeof(T)) unsigned s_buf[];
+    T* sq = reinterpret_cast<T*>(s_buf);
+    T* logits = reinterpret_cast<T*>(&sq[size_per_head]);
+
+    int length = __ldg(&length_per_sample[bid]);
+
+    int qkv_id = bid * head_num * size_per_head + head_id * size_per_head + tid;
+    int qkv_bias_id = head_id * size_per_head + tid;
+
+    if (tid < size_per_head) {
+        sq[tid] = query_buf[qkv_id] + Q_bias[qkv_bias_id];
+    }
+    __syncthreads();
+
+    for (int ite = 0; ite < length; ++ite) {
+        int key_id = bid * (seq_len * head_num * size_per_head) + ite * (head_num * size_per_head)
+                     + head_id * size_per_head + tid;
+
+        T key = tid < size_per_head ? key_cache[key_id] : (T)(0.0f);
+
+        // For the first step, we should add bias to key memory cache.
+        // The KV memory cache only need to be updated at the first step.
+        if (step == 1 && tid < size_per_head) {
+            key += K_bias[head_id * size_per_head + tid];
+            key_cache[key_id] = key;
+        }
+
+        T val = (tid < size_per_head) ? key * sq[tid] * scalar : (T)(0.0f);
+        T qk = blockReduceSum(val);
+        if (threadIdx.x == 0) {
+            logits[ite] = qk;
+        }
+        __syncthreads();  // try to remove
+    }
+    __syncthreads();
+
+    __shared__ float s_max_val, s_sum;
+
+    float local_i = tid < length ? (float)logits[tid] : -1e20f;
+    float max_val = blockReduceMax(local_i);
+    if (tid == 0) {
+        s_max_val = max_val;
+    }
+    __syncthreads();
+
+    local_i -= s_max_val;
+    float local_o = tid < length ? __expf(local_i) : 0.0f;
+    float val = blockReduceSum(local_o);
+
+    if (tid == 0) {
+        s_sum = val + 1e-6;
+    }
+    __syncthreads();
+    if (tid < length) {
+        logits[tid] = local_o / s_sum;
+    }
+    __syncthreads();
+
+    if (tid < size_per_head) {
+        T sum = (T)0.0f;
+        for (int ite = 0; ite < length; ++ite) {
+            int value_id = bid * seq_len * head_num * size_per_head + ite * head_num * size_per_head
+                           + head_id * size_per_head + tid;
+
+            T value = value_cache[value_id];
+
+            // for the first step, we should add bias to key memory cache
+            if (step == 1) {
+                value += V_bias[head_id * size_per_head + tid];
+                value_cache[value_id] = value;
+            }
+            sum += value * logits[ite];
+        }
+        context_buf[bid * head_num * size_per_head + head_id * size_per_head + tid] = sum;
+    }
+}
+
+template<typename T, int size_per_head, int block_sz>
+__global__ void attention_kernel_opt(
+    const T* __restrict qkv_buf,
+    const T* __restrict qkv_bias,
+    const T* __restrict attr_mask,
+    T* __restrict out_buf,
+    T* __restrict key_cache_output,
+    T* __restrict value_cache_output,
+    int batch_size,
+    int head_num,
+    const int seq_len,
+    const float scalar)
+{    
+    typedef Copy_t<T, size_per_head> copy_t;
+    const int elems_per_thread = size_per_head / WARP_SIZE;
+    union Access_t {
+        copy_t v;
+        T x[elems_per_thread];  // supported size 1,2,4
+    };
+    typedef struct Float_n_t {
+        float x[elems_per_thread];  // supported size 1,2,4
+    } float_n_t;
+
+    __shared__ float_n_t sq[block_sz];
+    extern __shared__ float logits[];  // use to store the logits from [0~step]
+
+    const int warp_id = threadIdx.x / WARP_SIZE;
+    const int warp_num = block_sz / WARP_SIZE;
+
+    typedef cub::BlockReduce<float, block_sz> MaxValBlockReduce;
+    typedef cub::BlockReduce<float, block_sz> BlockReduce;
+    __shared__ typename MaxValBlockReduce::TempStorage max_val_block_temp_storage;
+    __shared__ typename BlockReduce::TempStorage block_temp_storage;
+
+    __shared__ typename cub::WarpReduce<float>::TempStorage temp_storage[warp_num];
+
+    const int tid = threadIdx.x;
+    const int bid = blockIdx.x / head_num;
+    const int head_id = blockIdx.x % head_num;
+    int seq_id = blockIdx.y;
+    
+
+    int length = seq_len;
+    const int lane_id = tid % WARP_SIZE;
+
+    // QKV [m 3 n] shape
+    int qkv_id = bid * (3 * seq_len * head_num * size_per_head) + seq_id * (3 * head_num * size_per_head)
+                 + head_id * size_per_head;
+    int q_id = bid * (seq_len * head_num * size_per_head) + seq_id * (head_num * size_per_head)
+                 + head_id * size_per_head;
+    int qkv_bias_id = head_id * size_per_head;
+    int key_id = bid * (3 * seq_len * head_num * size_per_head) + head_num * size_per_head + head_id * size_per_head;
+    int value_id = bid * (3 * seq_len * head_num * size_per_head) + 2 * head_num * size_per_head + head_id * size_per_head;
+
+    int key_trn_id = bid * (seq_len * head_num * size_per_head) + head_id * (size_per_head * seq_len);
+    int value_trn_id = bid * (seq_len * head_num * size_per_head) + head_id * (size_per_head * seq_len);
+    int mask_offset = bid * (seq_len * seq_len) + seq_id * seq_len;
+ 
+    // get pointers 
+    const T* query_buf = qkv_buf + qkv_id;
+    const T* Q_bias = qkv_bias + qkv_bias_id;
+    T* context_buf = out_buf + q_id;
+
+    const T* key_cache = qkv_buf + key_id;
+    const T* K_bias = qkv_bias + head_num * size_per_head + qkv_bias_id;
+    T* key_cache_out = key_cache_output + key_trn_id;
+
+    const T* value_cache = qkv_buf + value_id;
+    const T* V_bias = qkv_bias + 2 * head_num * size_per_head + qkv_bias_id;
+    T* value_cache_out = value_cache_output + value_trn_id;
+
+    Access_t bias_r, key_val_r, query_buf_r;
+    // offset inside head
+    int minor_offset = lane_id; // offset in copy_t elements
+    // each warp will have its own copy of sq
+    query_buf_r.v = *((copy_t*)query_buf +  minor_offset);
+
+    bias_r.v = *((copy_t*)Q_bias + minor_offset);
+    float qb_r[elems_per_thread];
+#pragma unroll    
+    for (int i = 0; i < elems_per_thread; ++i) {
+        qb_r[i] = (float)query_buf_r.x[i] + (float)bias_r.x[i];
+    }
+
+    // offset for each step
+    int offset = 3 * head_num * size_per_head;   
+    bias_r.v = *((copy_t*)K_bias + minor_offset);
+    for (int ite = warp_id; ite < length; ite += warp_num) {
+        key_val_r.v = *((copy_t*)&key_cache[ite * offset] + minor_offset);       
+        
+       if (seq_id == 0) {
+            for (int i = 0; i < elems_per_thread; i++) {
+                key_val_r.x[i] = (float)key_val_r.x[i] + (float)bias_r.x[i];
+                key_cache_out[ite + seq_len * (minor_offset * elems_per_thread + i)] = key_val_r.x[i];
+            }
+        } else {
+            for (int i = 0; i < elems_per_thread; i++) {
+                key_val_r.x[i] = (float)key_val_r.x[i] + (float)bias_r.x[i];
+            }
+        }
+        float val = 0;
+        for (int i = 0; i < elems_per_thread; i++) {
+            val = val + (float)key_val_r.x[i] * qb_r[i];
+        }
+        float qk = cub::WarpReduce<float>(temp_storage[warp_id]).Sum(val);
+
+        if (lane_id == 0) {
+            T mask_val = attr_mask[mask_offset + ite];
+            mask_val =  (1.0f - mask_val) * -10000.0f;
+            logits[ite] = qk * scalar + mask_val;
+        }
+    }
+
+    __syncthreads();
+
+    __shared__ float s_max_val, s_sum;
+    float local_i = -1e20f;
+    for (int i = tid; i < length; i += blockDim.x) {
+        local_i = max(local_i, logits[i]);
+    }
+
+    float max_val = MaxValBlockReduce(max_val_block_temp_storage).Reduce(local_i, cub::Max());
+    if (tid == 0) {
+        s_max_val = max_val;
+    }
+    __syncthreads();
+
+    float local_o = 0.0f;
+    for (int i = tid; i < length; i += blockDim.x) {
+        logits[i] = __expf(logits[i] - s_max_val);
+        local_o += logits[i];
+    }
+    float val = BlockReduce(block_temp_storage).Sum(local_o);
+
+    if (tid == 0) {
+        s_sum = val + 1e-6;
+    }
+    __syncthreads();
+
+    float s_sum_inverse = __fdividef(1.0f, s_sum);
+    for (int i = tid; i < length; i += blockDim.x) {
+        logits[i] = logits[i] * s_sum_inverse;
+    }
+    __syncthreads();
+
+    // This optimization introduces discrepancy because of different order in FP32 summation
+    float sum_r[elems_per_thread] = {0.f};
+    bias_r.v = *((copy_t*)V_bias + minor_offset);
+    for (int ite = warp_id; ite < length; ite += warp_num) {
+        key_val_r.v = *((copy_t*)&value_cache[ite * offset] + minor_offset);
+#pragma unroll            
+        for (int i = 0; i < elems_per_thread; i++) {
+            key_val_r.x[i] = (float)key_val_r.x[i] + (float)bias_r.x[i];
+        }
+        if(seq_id == 0)
+            *((copy_t*)&value_cache_out[ite * size_per_head] + minor_offset) = key_val_r.v;
+#pragma unroll
+        for (int i = 0; i < elems_per_thread; ++i) {
+            sum_r[i] += (float)key_val_r.x[i] * logits[ite];
+        }
+    }
+    for (int i = 0; i < elems_per_thread; i++) {
+        sq[warp_id * WARP_SIZE + lane_id].x[i] = sum_r[i];
+    }
+    __syncthreads();
+    if (threadIdx.x < WARP_SIZE) {
+#pragma unroll
+        for (int j = 1; j < warp_num; j++) {
+            for (int i = 0; i < elems_per_thread; ++i) {
+                sum_r[i] = sum_r[i] + (float)sq[j * WARP_SIZE + threadIdx.x].x[i];
+            }
+        }
+    }
+    __syncthreads();
+#pragma unroll
+    for (int i = 0; i < elems_per_thread; i++) {
+        key_val_r.x[i] = sum_r[i];
+    }
+    if (threadIdx.x < WARP_SIZE) {
+        *((copy_t*)context_buf + minor_offset) = key_val_r.v;
+    }
+}
+
+template<typename T>
+void myAttnention(
+                const T* qkv_buf,
+                const T* qkv_bias,
+                const T* attr_mask,
+                T* context_buf,
+                T* key_cache_out,
+                T* value_cache_out,
+                const int inference_batch_size,
+                const int head_num,
+                const int size_per_head,
+                const int seq_len,
+                const float q_scaling,
+                cudaStream_t stream)
+{
+    const int block_sz = ATTENTION_BLOCK_SIZE; // blockDim.x
+    float scalar = 1.f / (sqrtf(size_per_head * 1.0f) * q_scaling);
+
+    dim3 grid(inference_batch_size * head_num, seq_len); // gridDim.x gridDim.y
+    int cond = size_per_head * ((ATTENION_OPT) ? 1 : 0);
+    switch (cond) {
+        case 32:
+            attention_kernel_opt<T, 32, block_sz>
+                <<<grid, block_sz, sizeof(float) * seq_len, stream>>>(qkv_buf,
+                                                                      qkv_bias,
+                                                                      attr_mask,
+                                                                      context_buf,
+                                                                      key_cache_out,
+                                                                      value_cache_out,
+                                                                      inference_batch_size,
+                                                                      head_num,
+                                                                      seq_len,
+                                                                      scalar);
+            break;
+        case 64:
+            attention_kernel_opt<T, 64, block_sz>
+                <<<grid, block_sz, sizeof(float) * seq_len, stream>>>(qkv_buf,
+                                                                      qkv_bias,
+                                                                      attr_mask,
+                                                                      context_buf,
+                                                                      key_cache_out,
+                                                                      value_cache_out,
+                                                                      inference_batch_size,
+                                                                      head_num,
+                                                                      seq_len,
+                                                                      scalar);
+            break;
+        case 128:
+            attention_kernel_opt<T, 128, block_sz>
+                <<<grid, block_sz, sizeof(float) * seq_len, stream>>>(qkv_buf,
+                                                                      qkv_bias,
+                                                                      attr_mask,
+                                                                      context_buf,
+                                                                      key_cache_out,
+                                                                      value_cache_out,
+                                                                      inference_batch_size,
+                                                                      head_num,
+                                                                      seq_len,
+                                                                      scalar);
+            break;
+        default:
+            ;
+            // default path
+
+            // int block_size = 128;
+
+            // if (seq_len <= 64) {
+            //     block_size = 64;
+            // }
+            // else if (seq_len <= 128 && seq_len > size_per_head) {
+            //     block_size = 128;
+            // }
+            // else if (seq_len > 128 && seq_len <= 256) {
+            //     block_size = 256;
+            // }
+            // else if (seq_len > 256 && seq_len <= 512) {
+            //     block_size = 512;
+            // }
+            // else {
+            //     block_size = 1024;
+            // }
+
+            // if (block_size < size_per_head) {
+            //     block_size = size_per_head;
+            // }
+
+            // assert(block_size <= 1024);
+            // dim3 block(block_size);
+
+            // int shared_size = sizeof(T) * (size_per_head + seq_len);
+            // attention_kernel<T><<<grid, block, shared_size, stream>>>(query_buf,
+            //                                                           Q_bias,
+            //                                                           key_cache,
+            //                                                           K_bias,
+            //                                                           value_cache,
+            //                                                           V_bias,
+            //                                                           length,
+            //                                                           context_buf,
+            //                                                           finished,
+            //                                                           max_batch_size,
+            //                                                           head_num,
+            //                                                           size_per_head,
+            //                                                           step,
+            //                                                           seq_len,
+            //                                                           scalar);
+    }
+}
+
+template void myAttnention(const float* qkv_buf,
+                           const float* qkv_bias,
+                           const float *attr_mask,
+                           float* context_buf,
+                           float* key_cache_out,
+                           float* value_cache_out,
+                           const int inference_batch_size,
+                           const int head_num,
+                           const int size_per_head,
+                           const int seq_len,
+                           const float q_scaling,
+                           cudaStream_t stream);
+
+// template void myAttnention(const half* qkv_buf,
+//                            const half* qkv_bias,
+//                            const half *attr_mask,
+//                            half* context_buf,
+//                            half* key_cache_out,
+//                            half* value_cache_out,
+//                            const int inference_batch_size,
+//                            const int head_num,
+//                            const int size_per_head,
+//                            const int seq_len,
+//                            const float q_scaling,
+//                            cudaStream_t stream);
 }  // namespace fastertransformer
diff --git a/src/fastertransformer/kernels/unfused_attention_kernels.h b/src/fastertransformer/kernels/unfused_attention_kernels.h
index be8b178..e9b4310 100644
--- a/src/fastertransformer/kernels/unfused_attention_kernels.h
+++ b/src/fastertransformer/kernels/unfused_attention_kernels.h
@@ -42,6 +42,37 @@ void invokeMaskedSoftMax(T* buffer,
                          const int head_num,
                          const T scalar,
                          cudaStream_t stream);
+template<typename T, typename T_IN>
+void invokeCrossMaskedSoftMax(T* buffer,
+                         const T_IN* buffer_src,
+                         const T* attr_mask,
+                         const int batch_size,
+                         const int seq_len,
+                         const int tgt_seq_len,
+                         const int head_num,
+                         const T scalar,
+                         cudaStream_t stream);
+
+template<typename T, typename T_M>
+void invokeMixMaskedSoftMax(T* io_buffer,
+                         const T_M* attr_mask,
+                         const int batch_size,
+                         const int seq_len,
+                         const int tgt_seq_len,
+                         const int head_num,
+                         const T scalar,
+                         cudaStream_t stream);
+
+template<typename T, typename T_M>
+void invokeMixMaskedSoftMax(T* io_buffer,
+                         const T_M* attr_mask,
+                         const T* position_bias,
+                         const int batch_size,
+                         const int seq_len,
+                         const int tgt_seq_len,
+                         const int head_num,
+                         const T scalar,
+                         cudaStream_t stream);
 
 template<typename T>
 void invokeTransposeQKV(T* dst,
@@ -81,12 +112,12 @@ void invokeTransposeAttentionOutRemovePadding(T* src,
                                               const int* mask_offset,
                                               cudaStream_t stream);
 
-template<typename T>
+template<typename T, typename U=T>
 void invokeAddFusedQKVBiasTranspose(T* q_buf,
                                     T* k_buf,
                                     T* v_buf,
                                     T* QKV,
-                                    const T* qkv_bias,
+                                    const U* qkv_bias,
                                     const int batch_size,
                                     const int seq_len,
                                     const int head_num,
@@ -97,12 +128,45 @@ void invokeAddFusedQKVBiasTranspose(T* q_buf,
         q_buf, k_buf, v_buf, QKV, qkv_bias, batch_size, seq_len, head_num, size_per_head, 0, stream);
 }
 
-template<typename T>
+template<typename T, typename U>
+void invokeAddFusedZP_QKVBiasTranspose(T* q_buf,
+                                    T* k_buf,
+                                    T* v_buf,
+                                    T* QKV,
+                                    const U* qkv_bias,
+                                    const int batch_size,
+                                    const int seq_len,
+                                    const int head_num,
+                                    const int size_per_head,
+                                    const int h_token,
+                                    int *padding_mask,
+                                    cudaStream_t stream);
+
+
+
+template<typename T, typename U=T>
+void invokeCrossAddFusedQKVBiasTranspose(T* q_buf,
+                                    T* k_buf,
+                                    T* v_buf,
+                                    T* QKV,
+                                    const U* qkv_bias,
+                                    const int batch_size,
+                                    const int seq_len,
+                                    const int tgt_seq_len,
+                                    const int head_num,
+                                    const int size_per_head,
+                                    cudaStream_t stream);
+// {
+//     invokeCrossAddFusedQKVBiasTranspose(
+//         q_buf, k_buf, v_buf, QKV, qkv_bias, batch_size, seq_len, tgt_seq_len, head_num, size_per_head, stream);
+// }
+
+template<typename T, typename U>
 void invokeAddFusedQKVBiasTranspose(T* q_buf,
                                     T* k_buf,
                                     T* v_buf,
                                     T* QKV,
-                                    const T* qkv_bias,
+                                    const U* qkv_bias,
                                     const int batch_size,
                                     const int seq_len,
                                     const int head_num,
@@ -166,4 +230,21 @@ void invokeMaskedSoftMaxWithRelPosBias(T* qk_buf,
                                        const float qk_scale,
                                        cudaStream_t stream);
 
+
+template<typename T>
+void myAttnention(const T* qkv_buf,
+                  const T* qkv_bias,
+                  const T *attr_mask,
+                  T* context_buf,
+                  T* key_cache_out,
+                  T* value_cache_out,
+                  const int inference_batch_size,
+                  const int head_num,
+                  const int size_per_head,
+                  const int seq_len,
+                  const float q_scaling,
+                  cudaStream_t stream);
+
+
+
 }  // namespace fastertransformer
diff --git a/src/fastertransformer/layers/CMakeLists.txt b/src/fastertransformer/layers/CMakeLists.txt
index cbaf4fa..00a46d4 100644
--- a/src/fastertransformer/layers/CMakeLists.txt
+++ b/src/fastertransformer/layers/CMakeLists.txt
@@ -14,6 +14,7 @@
 
 cmake_minimum_required(VERSION 3.8)
 
+add_subdirectory(encoder_layers)
 add_subdirectory(attention_layers)
 add_subdirectory(attention_layers_int8)
 add_subdirectory(xlnet_attention_layers)
@@ -30,15 +31,18 @@ set_property(TARGET FfnLayerINT8 PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET FfnLayerINT8 PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
 target_link_libraries(FfnLayerINT8 PUBLIC -lcublasLt -lcublas -lcudart cublasMMWrapper cublasINT8MMWrapper activation_int8_kernels memory_utils)
 
+if(EXAMPLES)
 add_library(TensorParallelGeluFfnLayer STATIC TensorParallelGeluFfnLayer.cc)
 set_property(TARGET TensorParallelGeluFfnLayer PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET TensorParallelGeluFfnLayer PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
 target_link_libraries(TensorParallelGeluFfnLayer PUBLIC -lcudart FfnLayer nccl_utils)
 
+
 add_library(TensorParallelReluFfnLayer STATIC TensorParallelReluFfnLayer.cc)
 set_property(TARGET TensorParallelReluFfnLayer PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET TensorParallelReluFfnLayer PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
 target_link_libraries(TensorParallelReluFfnLayer PUBLIC -lcudart FfnLayer nccl_utils)
+endif()
 
 add_library(DynamicDecodeLayer STATIC DynamicDecodeLayer.cc)
 set_property(TARGET DynamicDecodeLayer PROPERTY POSITION_INDEPENDENT_CODE  ON)
diff --git a/src/fastertransformer/layers/DenseWeight.h b/src/fastertransformer/layers/DenseWeight.h
index 5a5eb6a..c95b97c 100644
--- a/src/fastertransformer/layers/DenseWeight.h
+++ b/src/fastertransformer/layers/DenseWeight.h
@@ -28,4 +28,5 @@ struct DenseWeight {
     const float* scale = nullptr;
 };
 
+
 }  // namespace fastertransformer
\ No newline at end of file
diff --git a/src/fastertransformer/layers/attention_layers/BaseAttentionLayer.h b/src/fastertransformer/layers/attention_layers/BaseAttentionLayer.h
index b21e3a7..746cb71 100644
--- a/src/fastertransformer/layers/attention_layers/BaseAttentionLayer.h
+++ b/src/fastertransformer/layers/attention_layers/BaseAttentionLayer.h
@@ -62,13 +62,13 @@ AttentionType getAttentionTypeINT8(
     }
 }
 
-template<typename T>
+template<typename T, typename U=T, typename S=T>
 class BaseAttentionLayer: public BaseLayer {
 
 public:
     virtual void forward(std::vector<fastertransformer::Tensor>* output_tensors,
                          const std::vector<fastertransformer::Tensor>* input_tensors,
-                         const AttentionWeight<T>* attention_weights) = 0;
+                         const AttentionWeight<U>* attention_weights) = 0;
     BaseAttentionLayer(cudaStream_t stream,
                        cublasMMWrapper* cublas_wrapper,
                        IAllocator* allocator,
diff --git a/src/fastertransformer/layers/attention_layers/CMakeLists.txt b/src/fastertransformer/layers/attention_layers/CMakeLists.txt
index 9cef315..f9c9cde 100644
--- a/src/fastertransformer/layers/attention_layers/CMakeLists.txt
+++ b/src/fastertransformer/layers/attention_layers/CMakeLists.txt
@@ -42,8 +42,8 @@ target_link_libraries(DecoderSelfAttentionLayer PUBLIC -lcublas -lcudart cublasM
 add_library(GptContextAttentionLayer STATIC GptContextAttentionLayer.cc)
 set_property(TARGET GptContextAttentionLayer PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET GptContextAttentionLayer PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
-target_link_libraries(GptContextAttentionLayer PUBLIC -lcublas -lcudart cublasMMWrapper memory_utils unfused_attention_kernels)
-
+target_link_libraries(GptContextAttentionLayer PUBLIC -lcublas -lcudart cublasMMWrapper memory_utils unfused_attention_kernels activation_kernels EncoderLayer)
+if(EXAMPLES)
 add_library(TensorParallelDecoderSelfAttentionLayer STATIC TensorParallelDecoderSelfAttentionLayer.cc)
 set_property(TARGET TensorParallelDecoderSelfAttentionLayer PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET TensorParallelDecoderSelfAttentionLayer PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
@@ -63,6 +63,7 @@ add_library(TensorParallelUnfusedAttentionLayer STATIC TensorParallelUnfusedAtte
 set_property(TARGET TensorParallelUnfusedAttentionLayer PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET TensorParallelUnfusedAttentionLayer PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
 target_link_libraries(TensorParallelUnfusedAttentionLayer PUBLIC -lcudart UnfusedAttentionLayer nccl_utils)
+endif()
 
 add_library(WindowAttention STATIC WindowAttention.cc)
 set_property(TARGET WindowAttention PROPERTY POSITION_INDEPENDENT_CODE  ON)
diff --git a/src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.cc b/src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.cc
old mode 100644
new mode 100755
index bada640..3dca224
--- a/src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.cc
+++ b/src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.cc
@@ -16,10 +16,39 @@
  */
 
 #include "src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.h"
+#include "src/fastertransformer/kernels/activation_kernels.h"
 #include "src/fastertransformer/kernels/unfused_attention_kernels.h"
 
 namespace fastertransformer {
 
+template<typename T>
+cublasComputeType_t getCublasComputeType()
+{
+    if (std::is_same<T, int>::value)
+        return CUBLAS_COMPUTE_16F;
+
+    else
+        return CUBLAS_COMPUTE_32F_FAST_TF32;
+}
+template<typename T>
+static void printTensor(char* str, T* input, int size)
+{
+    printf("%s ", str);
+    T* input_device = input;
+    T* input_host = (T*)malloc(size * sizeof(T));
+
+    fastertransformer::cudaD2Hcpy(input_host, input_device, size);
+
+    for (int k = 0; k < (int)size; k++) {
+        std::cout << input_host[k] << ",";
+        if (k % 10 == 0)
+            std::cout << std::endl;
+    }
+
+    std::cout << std::endl;
+
+    free(input_host);
+}
 template<typename T>
 void GptContextAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor>* output_tensors,
                                           const std::vector<fastertransformer::Tensor>* input_tensors,
@@ -34,7 +63,6 @@ void GptContextAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor>
     //      attention_out [batch_size * seq_len, hidden_dimension]
     //      key_cache [batch, local_head_num, size_per_head // x, max_seq_len, x]
     //      value_cache [batch, local_head_num, max_seq_len, size_per_head]
-
     FT_CHECK(input_tensors->size() == 3);
     FT_CHECK(output_tensors->size() == 3);
     FT_CHECK(output_tensors->at(1).shape.size() == 5);
@@ -49,7 +77,7 @@ void GptContextAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor>
     T* attention_out = (T*)output_tensors->at(0).data;
     const T* attention_input = (const T*)input_tensors->at(0).data;
     const T* attention_mask = (const T*)input_tensors->at(1).data;
-    const bool is_final = *((bool*)(input_tensors->at(2).data));
+    const bool is_final = false;  // *((bool*)(input_tensors->at(2).data));
 
     const int m = input_tensors->at(0).shape[0];
 
@@ -134,7 +162,7 @@ void GptContextAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor>
                                                 request_seq_len,
                                                 request_seq_len * request_seq_len,
                                                 request_batch_size * local_head_num_,
-                                                CUDA_R_32F);
+                                                getCublasComputeType<T>());
             sync_check_cuda_error();
             T scalar = 1 / sqrtf(size_per_head_ * 1.0f);
             invokeMaskedSoftMax(qk_buf_,
@@ -428,4 +456,148 @@ template class GptContextAttentionLayer<half>;
 template class GptContextAttentionLayer<__nv_bfloat16>;
 #endif
 
+// HAIM Playground MS-MHA
+
+template<typename T, typename U, typename S>
+MSMHALayer<T, U, S>::MSMHALayer(size_t max_batch_size,
+                                size_t max_src_seq_len,
+                                size_t max_tgt_seq_len,
+                                size_t head_num,
+                                size_t size_per_head,
+                                cudaStream_t stream,
+                                cublasMMWrapper* cublas_wrapper,
+                                IAllocator* allocator,
+                                bool is_free_buffer_after_forward,
+                                bool is_qk_buf_float,
+                                bool is_cross,
+                                bool sparse,
+                                bool is_position_bias):
+    BaseAttentionLayer<T, U, S>(stream, cublas_wrapper, allocator, is_free_buffer_after_forward, sparse)
+{
+cublasHandle_t cublas_handle;
+    cublasCreate(&cublas_handle);
+    cublasSetStream(cublas_handle, stream);
+
+    params_.batch_size = max_batch_size;
+    params_.src_seq_len = max_src_seq_len;
+    params_.tgt_seq_len = max_tgt_seq_len;
+    params_.head_num = head_num;
+    params_.head_size = size_per_head;
+    params_.hidden_size = head_num * size_per_head;
+    params_.cublas_handle = cublas_handle;
+    params_.stream = stream;
+    // ctrls
+    params_.in_idx = 0;
+    params_.qkv_bias = !is_position_bias;
+    params_.projection_bias = !is_position_bias;
+    params_.is_cross = is_cross;
+    params_.position_bias = is_position_bias;
+    params_.algo = CUBLAS_GEMM_DEFAULT_TENSOR_OP;
+}
+
+template<typename T, typename U, typename S>
+void MSMHALayer<T, U, S>::allocateBuffer()
+{
+    if (buf_ == nullptr) {
+        size_t buff_size = GetAttnWorkspaceSize<T>(&params_);
+        buf_ = reinterpret_cast<T*>(allocator_->reMalloc(buf_, buff_size, true));
+    }
+}
+
+template<typename T, typename U, typename S>
+void MSMHALayer<T, U, S>::forward(std::vector<fastertransformer::Tensor>* output_tensors,
+                                  const std::vector<fastertransformer::Tensor>* input_tensors,
+                                  const AttentionWeight<U>* attention_weights)
+{
+    // input_tensors: use 1 gemm -- multi head attention
+    //      input_query [batch_size * seq_len, hidden_dimension]
+    //      attention_mask [batch_size, 1, seq_len, seq_len]
+
+    // input_tensors: use 2 gemm -- cross attention
+    //      input_query [batch_size * seq_len, hidden_dimension]
+    //      enc_output [batch_size * tgt_len, hidden_dimension]
+    //      attention_mask [batch_size, 1, seq_len, seq_len]
+
+    // output_tensors:
+    //      attention_out [batch_size * seq_len, hidden_dimension]
+    //      key_cache [batch, local_head_num, size_per_head // x, max_seq_len, x]
+    //      value_cache [batch, local_head_num, max_seq_len, size_per_head]
+
+    int in_tensor_number = input_tensors->size();
+    allocateBuffer();  // only once
+    if (params_.position_bias)
+        if (params_.is_cross) {
+            void* outputs[] = {(void*)output_tensors->at(0).data};
+            void* inputs[] = {(void*)input_tensors->at(0).data,
+                              (void*)input_tensors->at(1).data,
+                              (void*)attention_weights->query_weight.kernel,
+                              (void*)attention_weights->key_weight.kernel,
+                              (void*)input_tensors->at(2).data,
+                              (void*)input_tensors->at(3).data,
+                              (void*)attention_weights->attention_output_weight.kernel};
+
+            forward_attn<T>((T**)inputs, 7, (T**)outputs, 1, &params_, (void*)buf_);
+        }
+        else {
+            void* outputs[] = {(void*)output_tensors->at(0).data};
+            void* inputs[] = {
+                (void*)input_tensors->at(0).data,
+                (void*)attention_weights->query_weight.kernel,
+                (void*)input_tensors->at(1).data,
+                (void*)input_tensors->at(2).data,
+                (void*)attention_weights->attention_output_weight.kernel
+            };
+            forward_attn<T>((T**)inputs, 5, (T**)outputs, 1, &params_, (void*)buf_);
+        }
+    else {
+        if (params_.is_cross) {
+            void* outputs[] = {(void*)output_tensors->at(0).data};
+            void* inputs[] = {(void*)input_tensors->at(0).data,
+                              (void*)input_tensors->at(1).data,
+                              (void*)attention_weights->query_weight.kernel,
+                              (void*)attention_weights->key_weight.kernel,
+                              (void*)attention_weights->query_weight.bias,
+                              (void*)input_tensors->at(2).data,
+                              (void*)attention_weights->attention_output_weight.kernel,
+                              (void*)attention_weights->attention_output_weight.bias
+                              };
+            forward_attn<T>((T**)inputs, 8, (T**)outputs, 1, &params_, (void*)buf_);
+        } else {
+            void* outputs[] = {(void*)output_tensors->at(0).data};
+            void* inputs[] = {(void*)input_tensors->at(0).data,
+                              (void*)attention_weights->query_weight.kernel,
+                              (void*)attention_weights->query_weight.bias,
+                              (void*)input_tensors->at(1).data,
+                              (void*)attention_weights->attention_output_weight.kernel,
+                              (void*)attention_weights->attention_output_weight.bias};                
+            forward_attn<T>((T**)inputs, 6, (T**)outputs, 1, &params_, (void*)buf_);
+        }
+    }
+}
+
+template<typename T, typename U, typename S>
+MSMHALayer<T, U, S>::~MSMHALayer()
+{
+    cublas_wrapper_ = nullptr;
+    freeBuffer();
+}
+
+template<typename T, typename U, typename S>
+void MSMHALayer<T, U, S>::freeBuffer()
+{
+    if (buf_ != nullptr) {
+        allocator_->free(buf_);
+        buf_ = nullptr;
+    }
+}
+
+template class MSMHALayer<float, float, float>;
+template class MSMHALayer<float, float, half>;
+template class MSMHALayer<float, half, float>;
+template class MSMHALayer<float, half, half>;
+template class MSMHALayer<half, float, float>;
+template class MSMHALayer<half, float, half>;
+template class MSMHALayer<half, half, float>;
+template class MSMHALayer<half, half, half>;
+
 }  // namespace fastertransformer
diff --git a/src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.h b/src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.h
old mode 100644
new mode 100755
index 92e2175..f7fa5ca
--- a/src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.h
+++ b/src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.h
@@ -18,7 +18,7 @@
 #pragma once
 
 #include "src/fastertransformer/layers/attention_layers/BaseAttentionLayer.h"
-
+#include "src/fastertransformer/layers/encoder_layers/encoder.h"
 namespace fastertransformer {
 
 template<typename T>
@@ -107,4 +107,44 @@ public:
                  const AttentionWeight<T>* attention_weights) override;
 };
 
+
+// TODO(haim): Add template according to "mix" compute type (fp32, fp16)
+template<typename T,typename U=T, typename S=T>
+class MSMHALayer: public BaseAttentionLayer<T,U,S> {
+private:
+    void allocateBuffer() override;
+    void freeBuffer() override;
+
+    using BaseAttentionLayer<T,U,S>::is_free_buffer_after_forward_;
+    using BaseAttentionLayer<T,U,S>::is_allocate_buffer_;
+    using BaseAttentionLayer<T,U,S>::cublas_wrapper_;
+    using BaseAttentionLayer<T,U,S>::allocator_;
+
+protected:
+    using BaseAttentionLayer<T,U,S>::stream_;
+    using BaseAttentionLayer<T,U,S>::sparse_;
+    T* buf_ = nullptr;
+    encoderParamT params_;
+
+public:
+    MSMHALayer(size_t batch_size,
+               size_t src_seq_len,
+               size_t tgt_seq_len,
+               size_t head_num,
+               size_t size_per_head,
+               cudaStream_t stream,
+               cublasMMWrapper* cublas_wrapper,
+               IAllocator* allocator,
+               bool is_free_buffer_after_forward,
+               bool is_qk_buf_float,
+               bool is_cross,
+               bool sparse = false,
+               bool is_position_bias=false);
+    MSMHALayer(MSMHALayer<T> const& attention_layer);
+    virtual ~MSMHALayer();
+    void forward(std::vector<fastertransformer::Tensor>* output_tensors,
+                 const std::vector<fastertransformer::Tensor>* input_tensors,
+                 const AttentionWeight<U>* attention_weights) override;
+};
+
 }  // namespace fastertransformer
diff --git a/src/fastertransformer/layers/encoder_layers/BaseEncoderLayer.h b/src/fastertransformer/layers/encoder_layers/BaseEncoderLayer.h
new file mode 100644
index 0000000..3b43391
--- /dev/null
+++ b/src/fastertransformer/layers/encoder_layers/BaseEncoderLayer.h
@@ -0,0 +1,76 @@
+/*
+ * Copyright (c) 2019-2021, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <assert.h>
+#include <vector>
+
+#include "3rdparty/trt_fused_multihead_attention/fused_multihead_attention_common.h"
+#include "src/fastertransformer/layers/BaseLayer.h"
+#include "src/fastertransformer/layers/encoder_layers/EncoderLayerWeight.h"
+#include "src/fastertransformer/utils/Tensor.h"
+#include "src/fastertransformer/utils/allocator.h"
+#include "src/fastertransformer/utils/cublasMMWrapper.h"
+#include "src/fastertransformer/utils/memory_utils.h"
+
+namespace fastertransformer {
+
+enum class EncoderLayerType {
+    UNFUSED_ENCODER_LAYER,
+    FUSED_ENCODER_LAYER
+};
+
+template<typename T>
+EncoderLayerType getEncoderLayerType(size_t size_per_head, const int sm, const bool remove_padding, 
+                                        const int max_seq_len, const bool is_fuse = true) {
+    if (std::is_same<T, half>::value && (sm == kSM_70 || sm == kSM_86 || sm == kSM_80 || sm == kSM_75 || sm == kSM_72)
+        && size_per_head == 64 && max_seq_len <= 384 && is_fuse == true) {
+        return remove_padding ? EncoderLayerType::FUSED_ENCODER_LAYER : EncoderLayerType::FUSED_ENCODER_LAYER;
+    } else {
+        return remove_padding ? EncoderLayerType::FUSED_ENCODER_LAYER : EncoderLayerType::FUSED_ENCODER_LAYER;
+    }
+}
+
+template<typename T>
+EncoderLayerType getEncoderLayerTypeINT8(size_t size_per_head, const int sm, const bool remove_padding, 
+                                            const int max_seq_len, const int int8_mode) {
+    if ((int8_mode == 1 || int8_mode == 2) && (sm == kSM_86 || sm == kSM_80 || sm == kSM_75) && size_per_head == 64
+        && max_seq_len <= 384) {
+        return remove_padding ? EncoderLayerType::FUSED_ENCODER_LAYER : EncoderLayerType::FUSED_ENCODER_LAYER;
+    } else {
+        return remove_padding ? EncoderLayerType::FUSED_ENCODER_LAYER : EncoderLayerType::FUSED_ENCODER_LAYER;
+    }
+}
+
+template<typename T, typename U = T, typename S = T>
+class BaseEncoderLayer: public BaseLayer {
+
+public:
+    virtual void forward(std::vector<fastertransformer::Tensor>* output_tensors,
+                         const std::vector<fastertransformer::Tensor>* input_tensors,
+                         const EncoderLayerWeight<U>* encoder_layer_weights) = 0;
+    BaseEncoderLayer(cudaStream_t stream,
+                     cublasMMWrapper* cublas_wrapper,
+                     IAllocator* allocator,
+                     bool is_free_buffer_after_forward,
+                     bool sparse = false):
+        BaseLayer(stream, cublas_wrapper, allocator, is_free_buffer_after_forward, nullptr, sparse)
+    {
+    }
+    virtual ~BaseEncoderLayer() = default;
+};
+}  // namespace fastertransformer
diff --git a/src/fastertransformer/layers/encoder_layers/CMakeLists.txt b/src/fastertransformer/layers/encoder_layers/CMakeLists.txt
new file mode 100644
index 0000000..1a3af85
--- /dev/null
+++ b/src/fastertransformer/layers/encoder_layers/CMakeLists.txt
@@ -0,0 +1,21 @@
+# Copyright (c) 2019-2022, NVIDIA CORPORATION.  All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+cmake_minimum_required(VERSION 3.8)
+
+add_library(EncoderLayer STATIC encoder.cc MSEncoderLayer.cc)
+set_property(TARGET EncoderLayer PROPERTY POSITION_INDEPENDENT_CODE  ON)
+set_property(TARGET EncoderLayer PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
+target_link_libraries(EncoderLayer PUBLIC -lcublas -lcudart unfused_attention_kernels activation_kernels 
+                      layernorm_kernels add_residual_kernels bert_preprocess_kernels)
diff --git a/src/fastertransformer/layers/encoder_layers/EncoderLayerWeight.h b/src/fastertransformer/layers/encoder_layers/EncoderLayerWeight.h
new file mode 100644
index 0000000..c441b23
--- /dev/null
+++ b/src/fastertransformer/layers/encoder_layers/EncoderLayerWeight.h
@@ -0,0 +1,33 @@
+/*
+ * Copyright (c) 2019-2021, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "src/fastertransformer/layers/DenseWeight.h"
+#include "src/fastertransformer/kernels/layernorm_kernels.h"
+namespace fastertransformer {
+
+template<typename T>
+struct EncoderLayerWeight {
+    DenseWeight<T> qkv_weight;
+    DenseWeight<T> attention_layer_output_weight;
+    DenseWeight<T> encoder_output_mapping;
+    DenseWeight<T> encoder_output_projection;
+    LayerNormWeight<T> layernorm1;
+    LayerNormWeight<T> layernorm2;
+};
+
+}  // namespace fastertransformer
diff --git a/src/fastertransformer/layers/encoder_layers/MSEncoderLayer.cc b/src/fastertransformer/layers/encoder_layers/MSEncoderLayer.cc
new file mode 100644
index 0000000..a3442da
--- /dev/null
+++ b/src/fastertransformer/layers/encoder_layers/MSEncoderLayer.cc
@@ -0,0 +1,164 @@
+/*
+ * Copyright (c) 2021-2022, NVIDIA CORPORATION.  All rights reserved.
+ * Copyright (c) 2021, NAVER Corp.  Authored by CLOVA.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "src/fastertransformer/layers/encoder_layers/MSEncoderLayer.h"
+#include "src/fastertransformer/kernels/activation_kernels.h"
+
+namespace fastertransformer {
+template<typename T>
+void printTensor(char* str, T* input, int size)
+{
+    printf("%s ", str);
+    T* input_device = input;
+    T* input_host = (T*)malloc(size * sizeof(T));
+
+    fastertransformer::cudaD2Hcpy(input_host, input_device, size);
+
+    for (int k = 0; k < (int)size; k++) {
+
+        std::cout << input_host[k] << ",";
+        if (k % 10 == 0)
+            std::cout << std::endl;
+    }
+
+    std::cout << std::endl;
+
+    free(input_host);
+}
+template<typename T, typename U, typename S>
+MSELayer<T, U, S>::MSELayer(size_t max_batch_size,
+                            size_t max_src_seq_len,
+                            size_t max_tgt_seq_len,
+                            size_t head_num,
+                            size_t size_per_head,
+                            size_t ffn_hidden_size,
+                            float eps1,
+                            float eps2,
+                            bool post_layernorm,
+                            cudaStream_t stream,
+                            cublasMMWrapper* cublas_wrapper,
+                            cublasHandle_t* cublas_handle,
+                            IAllocator* allocator,
+                            bool is_free_buffer_after_forward,
+                            bool is_qk_buf_float,
+                            bool sparse):
+
+    BaseEncoderLayer<T, U, S>(stream, cublas_wrapper, allocator, is_free_buffer_after_forward, sparse), buf_(nullptr)
+{
+    params_.batch_size = max_batch_size;
+    params_.src_seq_len = max_src_seq_len;
+    params_.tgt_seq_len = max_tgt_seq_len;
+    params_.head_num = head_num;
+    params_.head_size = size_per_head;
+    params_.hidden_size = head_num * size_per_head;
+    params_.ffn_hidden_size = ffn_hidden_size;
+    params_.eps1 = eps1;
+    params_.eps2 = eps2;
+    params_.layernorm_post = post_layernorm;
+    // handle
+    params_.cublas_handle = *cublas_handle;
+    params_.stream = stream;
+    params_.ffn_fp16 = true;
+    // ctrls
+    params_.in_idx = 0;
+    params_.qkv_bias = true;
+    params_.projection_bias = true;
+    params_.is_cross = false;
+    params_.position_bias = false;
+    params_.algo = CUBLAS_GEMM_DEFAULT_TENSOR_OP;
+}
+
+template<typename T, typename U, typename S>
+void MSELayer<T, U, S>::allocateBuffer()
+{
+    if (buf_ == nullptr) {
+        size_t buff_size = GetEncoderLayerWorkspaceSize<T>(&params_);
+        buf_ = reinterpret_cast<T*>(allocator_->reMalloc(buf_, sizeof(T) * buff_size, true));
+    }
+}
+
+template<typename T, typename U, typename S>
+void MSELayer<T, U, S>::freeBuffer()
+{
+    if (buf_ != nullptr) {
+        allocator_->free(buf_);
+        buf_ = nullptr;
+    }
+}
+
+template<typename T, typename U, typename S>
+MSELayer<T, U, S>::~MSELayer()
+{
+    cublas_wrapper_ = nullptr;
+    freeBuffer();
+}
+
+template<typename T, typename U, typename S>
+void MSELayer<T, U, S>::forward(std::vector<fastertransformer::Tensor>* output_tensors,
+                                const std::vector<fastertransformer::Tensor>* input_tensors,
+                                const EncoderLayerWeight<U>* encoder_weights)
+{
+    allocateBuffer();  // only once
+    void* outputs[] = {(void*)output_tensors->at(0).data};
+    if (!params_.layernorm_post) {
+        void* inputs[] = {(void*)input_tensors->at(0).data,
+                          (void*)encoder_weights->layernorm1.gamma,
+                          (void*)encoder_weights->layernorm1.beta,
+                          (void*)encoder_weights->qkv_weight.kernel,
+                          (void*)encoder_weights->qkv_weight.bias,
+                          (void*)input_tensors->at(1).data,
+                          (void*)encoder_weights->attention_layer_output_weight.kernel,
+                          (void*)encoder_weights->attention_layer_output_weight.bias,
+                          (void*)encoder_weights->layernorm2.gamma,
+                          (void*)encoder_weights->layernorm2.beta,
+                          (void*)encoder_weights->encoder_output_mapping.kernel,
+                          (void*)encoder_weights->encoder_output_mapping.bias,
+                          (void*)encoder_weights->encoder_output_projection.kernel,
+                          (void*)encoder_weights->encoder_output_projection.bias};
+        forwardEncoder<T>(inputs, 14, outputs, 1, &params_, buf_);
+    }
+    else {
+        void* inputs[] = {(void*)input_tensors->at(0).data,
+                          (void*)encoder_weights->qkv_weight.kernel,
+                          (void*)encoder_weights->qkv_weight.bias,
+                          (void*)input_tensors->at(1).data,
+                          (void*)encoder_weights->attention_layer_output_weight.kernel,
+                          (void*)encoder_weights->attention_layer_output_weight.bias,
+                          (void*)encoder_weights->layernorm1.gamma,
+                          (void*)encoder_weights->layernorm1.beta,
+                          (void*)encoder_weights->encoder_output_mapping.kernel,
+                          (void*)encoder_weights->encoder_output_mapping.bias,
+                          (void*)encoder_weights->encoder_output_projection.kernel,
+                          (void*)encoder_weights->encoder_output_projection.bias,
+                          (void*)encoder_weights->layernorm2.gamma,
+                          (void*)encoder_weights->layernorm2.beta};
+        forwardEncoder<T>(inputs, 3, outputs, 1, &params_, buf_);
+    }
+
+    return;
+}
+
+template class MSELayer<float, float, float>;
+template class MSELayer<float, float, half>;
+template class MSELayer<float, half, float>;
+template class MSELayer<float, half, half>;
+template class MSELayer<half, float, float>;
+template class MSELayer<half, float, half>;
+template class MSELayer<half, half, float>;
+template class MSELayer<half, half, half>;
+
+}  // namespace fastertransformer
diff --git a/src/fastertransformer/layers/encoder_layers/MSEncoderLayer.h b/src/fastertransformer/layers/encoder_layers/MSEncoderLayer.h
new file mode 100644
index 0000000..afc6a5a
--- /dev/null
+++ b/src/fastertransformer/layers/encoder_layers/MSEncoderLayer.h
@@ -0,0 +1,69 @@
+/*
+ * Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
+ * Copyright (c) 2021, NAVER Corp.  Authored by CLOVA.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "src/fastertransformer/layers/encoder_layers/BaseEncoderLayer.h"
+#include "src/fastertransformer/layers/encoder_layers/encoder.h"
+
+namespace fastertransformer {
+
+// TODO(haim): Add template according to "mix" compute type (fp32, fp16)
+template<typename T, typename U = T, typename S = T>
+class MSELayer: public BaseEncoderLayer<T, U, S> {
+private:
+    encoderParamT params_;
+    void allocateBuffer() override;
+    void freeBuffer() override;
+    void* buf_;
+    using BaseEncoderLayer<T, U, S>::is_free_buffer_after_forward_;
+    using BaseEncoderLayer<T, U, S>::is_allocate_buffer_;
+    using BaseEncoderLayer<T, U, S>::cublas_wrapper_;
+    using BaseEncoderLayer<T, U, S>::allocator_;
+
+protected:
+    using BaseEncoderLayer<T, U, S>::stream_;
+    using BaseEncoderLayer<T, U, S>::sparse_;
+
+public:
+    MSELayer(size_t max_batch_size,
+                            size_t max_src_seq_len,
+                            size_t max_tgt_seq_len,
+                            size_t head_num,
+                            size_t size_per_head,
+                            size_t ffn_hidden_size,
+                            float eps1,
+                            float eps2,
+                            bool post_layernorm,
+                            cudaStream_t stream,
+                            cublasMMWrapper* cublas_wrapper,
+                            cublasHandle_t* cublas_handle,
+                            IAllocator* allocator,
+                            bool is_free_buffer_after_forward,
+                            bool is_qk_buf_float,
+                            bool sparse);
+
+    MSELayer(MSELayer<T> const& encoder_layer);
+
+    virtual ~MSELayer();
+
+    void forward(std::vector<fastertransformer::Tensor>* output_tensors,
+                 const std::vector<fastertransformer::Tensor>* input_tensors,
+                 const EncoderLayerWeight<U>* encoder_weights) override;
+};
+
+}  // namespace fastertransformer
diff --git a/src/fastertransformer/layers/encoder_layers/encoder.cc b/src/fastertransformer/layers/encoder_layers/encoder.cc
new file mode 100644
index 0000000..004718e
--- /dev/null
+++ b/src/fastertransformer/layers/encoder_layers/encoder.cc
@@ -0,0 +1,814 @@
+
+#include "src/fastertransformer/layers/encoder_layers/encoder.h"
+#include "src/fastertransformer/kernels/activation_kernels.h"
+#include "src/fastertransformer/kernels/add_residual_kernels.h"
+#include "src/fastertransformer/kernels/bert_preprocess_kernels.h"
+#include "src/fastertransformer/kernels/layernorm_kernels.h"
+#include "src/fastertransformer/kernels/unfused_attention_kernels.h"
+#include <iostream>
+
+namespace fastertransformer {
+
+#define UP_DIV(x, y) (((x) + (y) - (1)) / (y))
+#define ALIGN(x, y) (UP_DIV(x, y) * (y))
+#define ALIGN_SIZE 16
+
+template<typename T>
+void printTensor(const std::string& str, T* input, int size)
+{
+    std::cout << str;
+    T* input_device = input;
+    auto input_host = std::make_unique<T[]>(size);
+    cudaD2Hcpy(input_host.get(), input_device, size);
+    for (int k = 0, index = 0; k < size; k++) {
+        if (index != 0)
+            std::cout << ',';
+        std::cout << input_host[k];
+        index++;
+        if (index == 10) {
+            std::cout << std::endl;
+            index = 0;
+        }
+    }
+    std::cout << std::endl;
+}
+
+template<typename T>
+void isNan(char* str, T* input, int size)
+{
+    std::cout << str << " "
+              << " size is " << size;
+    T* input_device = input;
+    T* input_host = (T*)malloc(size * sizeof(T));
+    cudaD2Hcpy(input_host, input_device, size);
+    for (int k = 0; k < (int)size; k++) {
+        if (std::isnan((float)input_host[k]) || std ::isinf((float)input_host[k])) {
+            std::cout << "found NAN or INF";
+            break;
+        }
+    }
+    std::cout << std::endl;
+    free(input_host);
+}
+template<typename T>
+T checksum(const T* tensor, int size)
+{
+    if constexpr (std::is_floating_point<T>()) {
+        auto tensor_host = std::make_unique<T[]>(size);
+        double sum = 0.;
+        T* ptr = tensor_host.get();
+        cudaD2Hcpy(ptr, tensor, size);
+        for (int i = 0; i < size; i++) {
+            //  sum += (double)ptr[i]*i;
+            sum += ptr[i];
+        }
+        return static_cast<T>(sum);
+    }
+    else
+        return static_cast<T>(0.f);
+}
+
+template<typename T>
+T checksumGrid(const T* tensor, const encoderParamT* param, bool zp = false, bool cross = false, bool ffn = false)
+{
+    if constexpr (std::is_floating_point<T>()) {
+        int hidden_size;
+        if (ffn) {
+            hidden_size = param->ffn_hidden_size;
+        }
+        else {
+            hidden_size = param->hidden_size;
+        }
+        const int size = param->batch_size * param->src_seq_len * hidden_size;
+        int head_size = hidden_size / param->head_num;
+        auto tensor_host = std::make_unique<T[]>(size);
+        double sum = 0.;
+        T* ptr = tensor_host.get();
+        try {
+            cudaD2Hcpy(ptr, tensor, size);
+        }
+        catch (...) {
+            std::cout << "copy tensor failed" << std::endl;
+            return static_cast<T>(0.f);
+        }
+        bool compressed = param->eft && zp;
+        if (!compressed) {
+            if (cross) {
+                std::cout << "cross sum:" << std::endl;
+                for (int i = 0; i < param->batch_size; i++) {
+                    for (int j = 0; j < param->head_num; j++) {
+                        for (int k = 0; k < param->src_seq_len / 2; k++) {
+                            for (int l = 0; l < head_size; l++) {
+                                sum += ptr[(((i * param->head_num) + j) * param->src_seq_len + k) * head_size + l];
+                            }
+                        }
+                    }
+                }
+            }
+            else {
+                std::cout << "grid sum:" << std::endl;
+                for (int i = 0; i < param->batch_size; i++) {
+                    for (int j = 0; j < param->src_seq_len / 2; j++) {
+                        for (int k = 0; k < hidden_size; k++) {
+                            sum += ptr[((i * param->src_seq_len) + j) * hidden_size + k];
+                        }
+                    }
+                }
+            }
+        }
+        else {
+            std::cout << "compress sum:" << std::endl;
+            for (int i = 0; i < param->h_token_num * hidden_size; i++) {
+                sum += ptr[i];
+            }
+        }
+        return static_cast<T>(sum);
+    }
+    else {
+        return static_cast<T>(0.f);
+    }
+}
+
+template<typename T>
+void saveTensor(const std::string& name, T* tensor, int size)
+{
+    auto tensor_host = std::make_unique<T[]>(size);
+    T* ptr = tensor_host.get();
+    cudaD2Hcpy(ptr, tensor, size);
+    std::ofstream wf(name + ".bin", std::ofstream::out | std::ofstream::binary);
+    wf.write(reinterpret_cast<char*>(ptr), size * sizeof(T));
+    wf.close();
+}
+
+void CublasGemmWrapper(const void* a_addr,
+                       const void* b_addr,
+                       void* c_addr,
+                       const int* params,
+                       const int* lds,
+                       const cublasOperation_t* operations,
+                       const cudaDataType* data_types,
+                       void* alpha,
+                       void* beta,
+                       cublasHandle_t cublas_handle,
+                       cublasGemmAlgo_t algo)
+{
+    const int m = params[0];
+    const int n = params[1];
+    const int k = params[2];
+    cublasOperation_t trans_a = operations[0];
+    cublasOperation_t trans_b = operations[1];
+    const int lda = lds[0];
+    const int ldb = lds[1];
+    const int ldc = lds[2];
+    cudaDataType type_a = data_types[0];
+    cudaDataType type_b = data_types[1];
+    cudaDataType type_c = data_types[2];
+    cublasComputeType_t compute_type = CUBLAS_COMPUTE_32F_FAST_TF32;
+    if ((type_a == CUDA_R_16F) && (type_b == CUDA_R_16F) && (type_c == CUDA_R_16F)) {
+        compute_type = CUBLAS_COMPUTE_16F;
+    }
+    cublasGemmEx(cublas_handle,
+                 trans_a,
+                 trans_b,
+                 m,
+                 n,
+                 k,
+                 alpha,
+                 a_addr,
+                 type_a,
+                 lda,
+                 b_addr,
+                 type_b,
+                 ldb,
+                 beta,
+                 c_addr,
+                 type_c,
+                 ldc,
+                 compute_type,
+                 algo);
+}
+
+void CublasGemmStridedBatchedWrapper(const void* a_addr,
+                                     const void* b_addr,
+                                     void* c_addr,
+                                     const int* params,
+                                     const int* lds,
+                                     const cublasOperation_t* operations,
+                                     const int* strides,
+                                     const cudaDataType* data_types,
+                                     void* alpha,
+                                     void* beta,
+                                     int batch,
+                                     cublasHandle_t cublas_handle,
+                                     cublasGemmAlgo_t algo)
+{
+    const int m = params[0];
+    const int n = params[1];
+    const int k = params[2];
+    cublasOperation_t trans_a = operations[0];
+    cublasOperation_t trans_b = operations[1];
+    const int lda = lds[0];
+    const int ldb = lds[1];
+    const int ldc = lds[2];
+    cudaDataType type_a = data_types[0];
+    cudaDataType type_b = data_types[1];
+    cudaDataType type_c = data_types[2];
+    cublasComputeType_t compute_type = CUBLAS_COMPUTE_32F_FAST_TF32;
+    // cublasComputeType_t compute_type = CUBLAS_COMPUTE_32F_FAST_16F;
+
+    if ((type_a == CUDA_R_16F) && (type_b == CUDA_R_16F) && (type_c == CUDA_R_16F)) {
+        compute_type = CUBLAS_COMPUTE_16F;
+    }
+    const int stride_a = strides[0];
+    const int stride_b = strides[1];
+    const int stride_c = strides[2];
+    cublasGemmStridedBatchedEx(cublas_handle,
+                               trans_a,
+                               trans_b,
+                               m,
+                               n,
+                               k,
+                               alpha,
+                               a_addr,
+                               type_a,
+                               lda,
+                               stride_a,
+                               b_addr,
+                               type_b,
+                               ldb,
+                               stride_b,
+                               beta,
+                               c_addr,
+                               type_c,
+                               ldc,
+                               stride_c,
+                               batch,
+                               compute_type,
+                               algo);
+}
+
+template<typename T>
+size_t GetAttnWorkspaceSize(encoderParamT* param)
+{
+    size_t size_q = ALIGN((param->batch_size * param->src_seq_len * param->hidden_size), ALIGN_SIZE);
+    size_t size_k = ALIGN((param->batch_size * param->tgt_seq_len * param->hidden_size), ALIGN_SIZE);
+    size_t size_v = size_k;
+    size_t qkv_len = size_q + size_k + size_v;
+    size_t qk_buf_len =
+        ALIGN(param->batch_size * param->head_num * param->src_seq_len * param->tgt_seq_len, ALIGN_SIZE);
+    size_t qkv_buf_2_len = ALIGN(param->batch_size * param->src_seq_len * param->hidden_size, ALIGN_SIZE);
+    size_t attn_out_size =
+        ALIGN(param->batch_size * param->head_num * param->head_size * param->tgt_seq_len, ALIGN_SIZE);
+    return (qkv_buf_2_len + 2 * attn_out_size + std::max(qkv_len, qk_buf_len)) * sizeof(T);
+}
+
+template size_t GetAttnWorkspaceSize<float>(encoderParamT* param);
+template size_t GetAttnWorkspaceSize<half>(encoderParamT* param);
+template<typename T>
+size_t GetEncoderLayerWorkspaceSize(encoderParamT* param)
+{
+    size_t max_hidden = ALIGN(std::max(param->hidden_size, param->ffn_hidden_size),ALIGN_SIZE);
+    size_t compress_buffer_len = ALIGN(param->batch_size * param->src_seq_len * max_hidden,ALIGN_SIZE);
+    size_t padding_len = ALIGN(param->batch_size * param->src_seq_len,ALIGN_SIZE);
+    size_t offset_len = ALIGN(param->batch_size,ALIGN_SIZE);
+    size_t d_token_len = ALIGN(1,ALIGN_SIZE);
+    size_t eft_size = compress_buffer_len * sizeof(T) + (padding_len + offset_len) * sizeof(int) + d_token_len * sizeof(size_t);
+    size_t attn_out = ALIGN(param->batch_size * param->src_seq_len * param->hidden_size, ALIGN_SIZE);
+    size_t ffn = ALIGN(param->batch_size * param->src_seq_len * param->ffn_hidden_size, ALIGN_SIZE);
+    return (std::max(GetAttnWorkspaceSize<T>(param), ffn * sizeof(T)) + (attn_out * 3) * sizeof(T)) + eft_size;
+}
+
+template size_t GetEncoderLayerWorkspaceSize<float>(encoderParamT* param);
+template size_t GetEncoderLayerWorkspaceSize<half>(encoderParamT* param);
+
+template<typename T, typename S = T>
+void forward_ffn(T* inputs[], int in_len, T* output[], int out_len, encoderParamT* param, void* ws)
+{
+    size_t inter_size = param->ffn_hidden_size;
+    size_t h_token_num = param->h_token_num;
+    cublasOperation_t gemm_ops[] = {CUBLAS_OP_N, CUBLAS_OP_N};
+    cudaDataType gemm_data_types[] = {CUDA_R_32F, CUDA_R_32F, CUDA_R_32F};
+    if ((std::is_same<T, half>::value) || (std::is_same<S, half>::value)) {
+        gemm_data_types[0] = CUDA_R_16F;
+        gemm_data_types[1] = CUDA_R_16F;
+        gemm_data_types[2] = CUDA_R_16F;
+    }
+    S alpha = 1.0f;
+    S beta = 0.0f;
+
+    int gemm_dims[] = {(int)inter_size, (int)h_token_num, (int)param->hidden_size};
+    int gemm_lds[] = {(int)inter_size, (int)param->hidden_size, (int)inter_size};
+    T* normed_attn_out = reinterpret_cast<T*>(inputs[param->in_idx++]);
+    CublasGemmWrapper(inputs[param->in_idx++],
+                      normed_attn_out,
+                      ws,
+                      gemm_dims,
+                      gemm_lds,
+                      gemm_ops,
+                      gemm_data_types,
+                      &alpha,
+                      &beta,
+                      param->cublas_handle,
+                      param->algo);
+    invokeAddBiasGelu(reinterpret_cast<S*>(ws),
+                      reinterpret_cast<S*>(inputs[param->in_idx++]),
+                      h_token_num,
+                      inter_size,
+                      param->stream);
+    gemm_dims[0] = param->hidden_size;
+    gemm_dims[1] = h_token_num;
+    gemm_dims[2] = inter_size;
+    gemm_lds[0] = param->hidden_size;
+    gemm_lds[1] = inter_size;
+    gemm_lds[2] = param->hidden_size;
+    CublasGemmWrapper(inputs[param->in_idx++],
+                      ws,
+                      output[0],
+                      gemm_dims,
+                      gemm_lds,
+                      gemm_ops,
+                      gemm_data_types,
+                      &alpha,
+                      &beta,
+                      param->cublas_handle,
+                      param->algo);
+}
+
+template<typename T>
+void forwardEncoder(void* inputs[], int in_len, void* output[], int out_len, encoderParamT* param, void* ws)
+{
+    param->in_idx = 0;
+    size_t h_token_num = param->batch_size * param->src_seq_len;
+    param->h_token_num = h_token_num;
+    param->padding_offset = nullptr;
+    int* d_sequence_lengths = nullptr;
+    T* input_tensor = reinterpret_cast<T*>(inputs[param->in_idx++]);
+    T* from_tensor = input_tensor;
+    T* compress_buffer;
+    compress_buffer = reinterpret_cast<T*>(ws);
+    ws = reinterpret_cast<void*>(reinterpret_cast<T*>(ws) + ALIGN(h_token_num * param->hidden_size,ALIGN_SIZE));
+    int* padding_offset = reinterpret_cast<int*>(ws);
+    ws = reinterpret_cast<void*>(reinterpret_cast<int*>(ws) + ALIGN(param->batch_size * param->src_seq_len,ALIGN_SIZE));
+    d_sequence_lengths = reinterpret_cast<int*>(ws);
+    param->d_sequence_length = d_sequence_lengths;
+    ws = reinterpret_cast<void*>(reinterpret_cast<int*>(ws) + ALIGN(param->batch_size,ALIGN_SIZE));
+    size_t* d_token_num = reinterpret_cast<size_t*>(ws);
+    ws = reinterpret_cast<void*>(reinterpret_cast<size_t*>(ws) + ALIGN(1,ALIGN_SIZE));
+    invokeBuildSequnceLength(
+        from_tensor, param->batch_size, d_sequence_lengths, param->src_seq_len, param->hidden_size, param->stream);
+   // printTensor("seq_len=",d_sequence_lengths,param->batch_size);
+    invokeGetPaddingOffset(&h_token_num,
+                           d_token_num,
+                           padding_offset,
+                           d_sequence_lengths,
+                           param->batch_size,
+                           param->src_seq_len,
+                           param->stream);
+    // std::cout << "token=" << h_token_num << "m=" << param->batch_size * param->src_seq_len << std::endl;
+    if (h_token_num * 2 <= param->batch_size * param->src_seq_len) {
+        param->eft = true;
+        invokeRemovePadding(compress_buffer,
+                            (const T*)from_tensor,
+                            padding_offset,
+                            h_token_num,
+                            param->head_num * param->head_size,
+                            param->stream);
+        param->h_token_num = h_token_num;
+        param->padding_offset = padding_offset;
+        from_tensor = compress_buffer;
+    }
+    h_token_num = param->h_token_num;
+    T* attn_out = reinterpret_cast<T*>(ws);
+    T* normed_from_tensor =
+        reinterpret_cast<T*>(ws) + ALIGN(param->batch_size * param->src_seq_len * param->hidden_size, ALIGN_SIZE);
+    T* attn_ws_offset = (param->layernorm_post) ? reinterpret_cast<T*>(ws) : reinterpret_cast<T*>(normed_from_tensor);
+    T* attn_ws = attn_ws_offset + ALIGN(param->batch_size * param->src_seq_len * param->hidden_size, ALIGN_SIZE);
+    T* normed_attn_out = normed_from_tensor;
+    T* ffn_ws = normed_attn_out + ALIGN(param->batch_size * param->src_seq_len * param->hidden_size, ALIGN_SIZE);
+
+    T* tmp_out = reinterpret_cast<T*>(output[0]);
+    if (param->padding_offset != nullptr || (std::is_same<T, float>::value && param->ffn_fp16 == true)) {
+        tmp_out = ffn_ws + ALIGN(param->batch_size * param->src_seq_len * param->ffn_hidden_size, ALIGN_SIZE);
+    }
+    T* tmp_out1 = reinterpret_cast<T*>(output[0]);
+    T* out_buf = tmp_out;
+    if (param->padding_offset != nullptr) {
+        tmp_out1 = compress_buffer;
+    }
+    if (param->layernorm_post == false) {
+        T* gamma1 = reinterpret_cast<T*>(inputs[param->in_idx++]);
+        T* beta1 = reinterpret_cast<T*>(inputs[param->in_idx++]);
+
+        invokeGeneralLayerNorm(normed_from_tensor,
+                               reinterpret_cast<T*>(from_tensor),  // from tensor
+                               gamma1,                             // Gamma
+                               beta1,                              // Beta
+                               h_token_num,
+                               param->hidden_size,
+                               param->stream,
+                               param->eps1);
+    }
+    else {
+        normed_from_tensor = from_tensor;
+    }
+    inputs[--param->in_idx] = normed_from_tensor;
+    // if attention is embedded inside an encoder - fuse the bias to next layer normalization
+    bool projection_bias = param->projection_bias;
+    param->projection_bias = false;
+    int in_idx = param->in_idx;
+    forward_attn(reinterpret_cast<T**>(&inputs[param->in_idx]), in_len, &attn_out, 1, param, attn_ws);
+    param->in_idx += in_idx;
+    param->projection_bias = projection_bias;
+    if (param->projection_bias) {
+        T* projection_bias = reinterpret_cast<T*>(inputs[param->in_idx++]);
+        T* gamma2 = reinterpret_cast<T*>(inputs[param->in_idx++]);
+        T* beta2 = reinterpret_cast<T*>(inputs[param->in_idx++]);
+        if (param->layernorm_post == false) {
+            if (std::is_same<T, half>::value || param->ffn_fp16 == false) {
+                invokeGeneralAddBiasResidualPreLayerNorm(attn_out,
+                                                         normed_attn_out,
+                                                         from_tensor,
+                                                         gamma2,  // gamma
+                                                         beta2,   // beta
+                                                         projection_bias,
+                                                         h_token_num,
+                                                         param->hidden_size,
+                                                         param->stream,
+                                                         param->eps2);
+            }
+            else {
+                invokeGeneralAddBiasResidualPreLayerNormCast<T, half>(attn_out,
+                                                                      reinterpret_cast<half*>(normed_attn_out),
+                                                                      from_tensor,
+                                                                      gamma2,  // gamma
+                                                                      beta2,   // beta
+                                                                      projection_bias,
+                                                                      h_token_num,
+                                                                      param->hidden_size,
+                                                                      param->stream,
+                                                                      param->eps2);
+            }
+        }
+        else {
+            if (std::is_same<T, half>::value || param->ffn_fp16 == false) {
+                invokeAddBiasResidualLayerNorm(attn_out,
+                                               from_tensor,
+                                               projection_bias,
+                                               gamma2,  // gamma
+                                               beta2,   // beta
+                                               h_token_num,
+                                               param->hidden_size,
+                                               param->stream,
+                                               param->eps1);
+                normed_attn_out = attn_out;
+            }
+            else {
+                invokeAddBiasResidualLayerNormCast<T, float, half>(reinterpret_cast<float*>(attn_out),
+                                                                   reinterpret_cast<half*>(normed_attn_out),
+                                                                   reinterpret_cast<float*>(from_tensor),
+                                                                   projection_bias,
+                                                                   gamma2,  // gamma
+                                                                   beta2,   // beta
+                                                                   h_token_num,
+                                                                   param->hidden_size,
+                                                                   param->stream,
+                                                                   param->eps1);
+                // isNan<half>((char*)"LN 1 model", (half*)attn_out, h_token_num * param->hidden_size);
+            }
+        }
+    }
+    else {
+        // without projection bias
+    }
+    // forward ffn
+    // simulate attention inputs
+    inputs[--param->in_idx] = normed_attn_out;
+    if (param->ffn_fp16 == false) {
+        forward_ffn<T, T>(reinterpret_cast<T**>(inputs), in_len, &tmp_out, 1, param, ffn_ws);
+    }
+    else {
+        forward_ffn<T, half>(reinterpret_cast<T**>(inputs), in_len, &tmp_out, 1, param, ffn_ws);
+    }
+    if (param->layernorm_post == true) {
+        if (std::is_same<T, half>::value || param->ffn_fp16 == false) {
+            invokeAddBiasResidualLayerNorm(reinterpret_cast<T*>(tmp_out),
+                                           attn_out,
+                                           reinterpret_cast<T*>(inputs[param->in_idx++]),  // FFN bias,
+                                           reinterpret_cast<T*>(inputs[param->in_idx++]),  // Gamma
+                                           reinterpret_cast<T*>(inputs[param->in_idx++]),  // Beta
+                                           h_token_num,
+                                           param->hidden_size,
+                                           param->stream,
+                                           param->eps2);
+        }
+        else {
+            invokeAddBiasResidualLayerNormCast<T, half, float>(
+                reinterpret_cast<half*>(tmp_out),
+                reinterpret_cast<float*>(tmp_out1),
+                reinterpret_cast<half*>(normed_attn_out),
+                reinterpret_cast<T*>(inputs[param->in_idx++]),  // FFN bias,
+                reinterpret_cast<T*>(inputs[param->in_idx++]),  // Gamma
+                reinterpret_cast<T*>(inputs[param->in_idx++]),  // Beta
+                h_token_num,
+                param->hidden_size,
+                param->stream,
+                param->eps2);
+            out_buf = tmp_out1;
+        }
+    }
+    else {
+        if (std::is_same<T, half>::value || param->ffn_fp16 == false) {
+            invokeAddBiasResidual(reinterpret_cast<T*>(tmp_out),
+                                  attn_out,
+                                  reinterpret_cast<T*>(inputs[param->in_idx++]),  // FFN bias
+                                  h_token_num,
+                                  param->hidden_size,
+                                  param->stream);
+        }
+        else {
+            invokeAddBiasResidualCast<T, T, half>(reinterpret_cast<half*>(tmp_out),
+                                                  reinterpret_cast<T*>(attn_out),
+                                                  reinterpret_cast<T*>(tmp_out1),
+                                                  reinterpret_cast<T*>(inputs[param->in_idx++]),  // FFN bias
+                                                  h_token_num,
+                                                  param->hidden_size,
+                                                  param->stream);
+        }
+    }
+    if (param->padding_offset != nullptr) {
+        cudaMemsetAsync(output[0],
+                        0,
+                        param->batch_size * param->src_seq_len * param->head_size * param->head_num * sizeof(T),
+                        param->stream);
+        invokeRebuildPadding(
+            (T*)output[0], out_buf, param->padding_offset, h_token_num, param->hidden_size, param->stream);
+    }
+    return;
+}
+
+template void
+forwardEncoder<float>(void* inputs[], int in_len, void* output[], int out_len, encoderParamT* param, void* ws);
+template void
+forwardEncoder<half>(void* inputs[], int in_len, void* output[], int out_len, encoderParamT* param, void* ws);
+
+template<typename T>
+void forward_attn(T* inputs[], int in_len, T* output[], int out_len, encoderParamT* param, void* ws)
+{
+    param->in_idx = 0;
+    auto extra_tmp_size =
+        ALIGN(param->batch_size * param->head_num * param->head_size * param->tgt_seq_len, ALIGN_SIZE);
+    size_t size_q = ALIGN(param->batch_size * param->src_seq_len * param->hidden_size, ALIGN_SIZE);
+    size_t q_buf_2_len = size_q;
+    size_t qk_buf_len =
+        ALIGN(param->batch_size * param->head_num * param->src_seq_len * param->tgt_seq_len, ALIGN_SIZE);
+    size_t qkv_buf_2_len = ALIGN(param->batch_size * param->src_seq_len * param->hidden_size, ALIGN_SIZE);
+    T* q_buf_2 = (T*)ws;
+    T* output1 = static_cast<T*>(ws) + q_buf_2_len;
+    T* output2 = static_cast<T*>(output1) + extra_tmp_size;
+    T* qkv_buf = static_cast<T*>(output2) + extra_tmp_size;
+    T* qk_buf = qkv_buf;
+    T* qkv_buf_2 = q_buf_2;
+    T* qkv_buf_3 = qk_buf;
+    int gemm_dims[] = {3 * (int)param->hidden_size, (int)param->h_token_num, (int)param->hidden_size};
+    int gemm_lds[] = {3 * (int)param->hidden_size, (int)param->hidden_size, 3 * (int)param->hidden_size};
+    T* from_tensor = reinterpret_cast<T*>(inputs[param->in_idx++]);
+    cublasOperation_t gemm_ops[] = {CUBLAS_OP_N, CUBLAS_OP_N};
+    cudaDataType gemm_data_types[] = {CUDA_R_32F, CUDA_R_32F, CUDA_R_32F};
+    if (std::is_same<T, half>::value) {
+        gemm_data_types[0] = CUDA_R_16F;
+        gemm_data_types[1] = CUDA_R_16F;
+        gemm_data_types[2] = CUDA_R_16F;
+    }
+    T alpha = 1.0f;
+    T beta = 0.0f;
+
+    if (param->is_cross) {
+        gemm_dims[0] = param->hidden_size;
+        gemm_dims[1] = param->batch_size * param->src_seq_len;
+        gemm_dims[2] = param->hidden_size;
+        gemm_lds[0] = param->hidden_size;
+        gemm_lds[1] = param->hidden_size;
+        gemm_lds[2] = param->hidden_size;
+        T* encoder_output = reinterpret_cast<T*>(inputs[param->in_idx++]);
+        T* weight_q = reinterpret_cast<T*>(inputs[param->in_idx++]);
+
+        CublasGemmWrapper(weight_q,
+                          from_tensor,
+                          qkv_buf,
+                          gemm_dims,
+                          gemm_lds,
+                          gemm_ops,
+                          gemm_data_types,
+                          &alpha,
+                          &beta,
+                          param->cublas_handle,
+                          param->algo);
+        gemm_dims[0] = 2 * param->hidden_size;
+        gemm_dims[1] = param->batch_size * param->tgt_seq_len;
+        gemm_lds[0] = 2 * param->hidden_size;
+        gemm_lds[2] = 2 * param->hidden_size;
+        T* weight_kv = reinterpret_cast<T*>(inputs[param->in_idx++]);
+
+        CublasGemmWrapper(weight_kv,
+                          encoder_output,
+                          qkv_buf + (param->batch_size * param->src_seq_len) * param->hidden_size,
+                          gemm_dims,
+                          gemm_lds,
+                          gemm_ops,
+                          gemm_data_types,
+                          &alpha,
+                          &beta,
+                          param->cublas_handle,
+                          param->algo);
+
+        T* bias_qkv = (param->qkv_bias) ? reinterpret_cast<T*>(inputs[param->in_idx++]) : nullptr;
+        invokeCrossAddFusedQKVBiasTranspose(q_buf_2,
+                                            output1,
+                                            output2,
+                                            qkv_buf,
+                                            bias_qkv,
+                                            param->batch_size,
+                                            param->src_seq_len,
+                                            param->tgt_seq_len,
+                                            param->head_num,
+                                            param->head_size,
+                                            param->stream);
+    }
+    else {
+        T* weight_qkv = reinterpret_cast<T*>(inputs[param->in_idx++]);
+        CublasGemmWrapper(weight_qkv,
+                          from_tensor,
+                          qkv_buf,
+                          gemm_dims,
+                          gemm_lds,
+                          gemm_ops,
+                          const_cast<const cudaDataType*>(gemm_data_types),
+                          &alpha,
+                          &beta,
+                          param->cublas_handle,
+                          param->algo);
+
+        T* bias_qkv = (param->qkv_bias) ? reinterpret_cast<T*>(inputs[param->in_idx++]) : nullptr;
+        if (param->padding_offset == nullptr) {
+            invokeAddFusedQKVBiasTranspose(static_cast<T*>(q_buf_2),
+                                           static_cast<T*>(output1),
+                                           static_cast<T*>(output2),
+                                           static_cast<T*>(qkv_buf),
+                                           bias_qkv,
+                                           param->batch_size,
+                                           param->src_seq_len,
+                                           param->head_num,
+                                           param->head_size,
+                                           0,
+                                           param->stream);
+        }
+        else {
+            invokeAddFusedZP_QKVBiasTranspose(static_cast<T*>(q_buf_2),
+                                              static_cast<T*>(output1),
+                                              static_cast<T*>(output2),
+                                              static_cast<T*>(qkv_buf),
+                                              bias_qkv,
+                                              param->batch_size,
+                                              param->src_seq_len,
+                                              param->head_num,
+                                              param->head_size,
+                                              param->h_token_num,
+                                              param->padding_offset,
+                                              param->stream);
+        }
+    }
+    gemm_ops[0] = CUBLAS_OP_T;
+    gemm_ops[1] = CUBLAS_OP_N;
+    gemm_dims[0] = param->tgt_seq_len;
+    gemm_dims[1] = param->src_seq_len;
+    gemm_dims[2] = param->head_size;
+
+    gemm_lds[0] = param->head_size;
+    gemm_lds[1] = param->head_size;
+    gemm_lds[2] = param->tgt_seq_len;
+
+    int gemm_strides[] = {(int)(param->tgt_seq_len * param->head_size),
+                          (int)(param->src_seq_len * param->head_size),
+                          (int)(param->src_seq_len * param->tgt_seq_len)};
+
+    CublasGemmStridedBatchedWrapper(output1,
+                                    q_buf_2,
+                                    qk_buf,
+                                    gemm_dims,
+                                    gemm_lds,
+                                    gemm_ops,
+                                    gemm_strides,
+                                    const_cast<const cudaDataType*>(gemm_data_types),
+                                    &alpha,
+                                    &beta,
+                                    param->batch_size * param->head_num,
+                                    param->cublas_handle,
+                                    param->algo);
+
+    T* attention_mask = reinterpret_cast<T*>(inputs[param->in_idx++]);
+    if (param->padding_offset != nullptr)
+        invokeBuildEncoderAttentionMask(
+            attention_mask, param->d_sequence_length, param->batch_size, param->src_seq_len, param->stream);
+    T* position_bias = nullptr;
+    if (param->position_bias) {
+        position_bias = reinterpret_cast<T*>(inputs[param->in_idx++]);
+    }
+    T scalar = static_cast<T>(1.0f / sqrtf(param->head_size * 1.0f));
+    invokeMixMaskedSoftMax(static_cast<T*>(qk_buf),
+                           attention_mask,
+                           position_bias,
+                           param->batch_size,
+                           param->src_seq_len,
+                           param->tgt_seq_len,
+                           param->head_num,
+                           scalar,
+                           param->stream);
+
+    gemm_ops[0] = CUBLAS_OP_N;
+    gemm_ops[1] = CUBLAS_OP_N;
+    gemm_dims[0] = param->head_size;
+    gemm_dims[1] = param->src_seq_len;
+    gemm_dims[2] = param->tgt_seq_len;
+
+    gemm_lds[0] = param->head_size;
+    gemm_lds[1] = param->tgt_seq_len;
+    gemm_lds[2] = param->head_size;
+
+    gemm_strides[0] = param->tgt_seq_len * param->head_size;
+    gemm_strides[1] = param->src_seq_len * param->tgt_seq_len;
+    gemm_strides[2] = param->src_seq_len * param->head_size;
+
+    CublasGemmStridedBatchedWrapper(output2,
+                                    qk_buf,
+                                    qkv_buf_2,
+                                    gemm_dims,
+                                    gemm_lds,
+                                    gemm_ops,
+                                    gemm_strides,
+                                    const_cast<const cudaDataType*>(gemm_data_types),
+                                    &alpha,
+                                    &beta,
+                                    param->batch_size * param->head_num,
+                                    param->cublas_handle,
+                                    param->algo);
+
+    if (param->padding_offset == nullptr) {
+        invokeTransposeQKV(static_cast<T*>(qkv_buf_3),
+                           static_cast<T*>(qkv_buf_2),
+                           param->batch_size,
+                           param->src_seq_len,
+                           param->head_num,
+                           param->head_size,
+                           param->stream);
+    }
+    else {
+        invokeTransposeAttentionOutRemovePadding(qkv_buf_2,
+                                                 qkv_buf_3,
+                                                 param->h_token_num,
+                                                 param->batch_size,
+                                                 param->src_seq_len,
+                                                 param->head_num,
+                                                 param->head_size,
+                                                 param->padding_offset,
+                                                 param->stream);
+    }
+    gemm_ops[0] = CUBLAS_OP_N;
+    gemm_ops[1] = CUBLAS_OP_N;
+    gemm_dims[0] = param->hidden_size;
+    gemm_dims[1] = param->h_token_num;
+    gemm_dims[2] = param->hidden_size;
+
+    gemm_lds[0] = param->hidden_size;
+    gemm_lds[1] = param->hidden_size;
+    gemm_lds[2] = param->hidden_size;
+    CublasGemmWrapper(reinterpret_cast<T*>(inputs[param->in_idx++]),
+                      qkv_buf_3,
+                      static_cast<T*>(output[0]),
+                      gemm_dims,
+                      gemm_lds,
+                      gemm_ops,
+                      const_cast<const cudaDataType*>(gemm_data_types),
+                      &alpha,
+                      &beta,
+                      param->cublas_handle,
+                      param->algo);
+    if (param->projection_bias) {
+        int len = param->h_token_num;
+        invokeAddBias(
+            static_cast<T*>(output[0]), (const T*)(inputs[param->in_idx++]), len, param->hidden_size, param->stream);
+    }
+    return;
+}
+
+template void
+forward_attn<float>(float* inputs[], int in_len, float* output[], int out_len, encoderParamT* param, void* ws);
+template void
+forward_attn<half>(half* inputs[], int in_len, half* output[], int out_len, encoderParamT* param, void* ws);
+
+template void
+forward_ffn<float, half>(float* inputs[], int in_len, float* output[], int out_len, encoderParamT* param, void* ws);
+template void
+forward_ffn<half>(half* inputs[], int in_len, half* output[], int out_len, encoderParamT* param, void* ws);
+template void
+forward_ffn<float>(float* inputs[], int in_len, float* output[], int out_len, encoderParamT* param, void* ws);
+}  // namespace fastertransformer
diff --git a/src/fastertransformer/layers/encoder_layers/encoder.h b/src/fastertransformer/layers/encoder_layers/encoder.h
new file mode 100644
index 0000000..ffba081
--- /dev/null
+++ b/src/fastertransformer/layers/encoder_layers/encoder.h
@@ -0,0 +1,49 @@
+#pragma once
+
+#include "src/fastertransformer/kernels/activation_kernels.h"
+#include "src/fastertransformer/layers/encoder_layers/BaseEncoderLayer.h"
+#include <cublas_v2.h>
+#include <cuda.h>
+
+namespace fastertransformer {
+
+typedef struct {
+    size_t batch_size;
+    size_t src_seq_len;
+    size_t tgt_seq_len;
+    size_t head_num;
+    size_t head_size;
+    size_t hidden_size;
+    size_t h_token_num;
+    size_t ffn_hidden_size;  // 4 * param->hidden_size;
+    bool ffn_fp16;
+    float eps1;
+    float eps2;
+    // handle
+    cublasHandle_t cublas_handle;
+    cudaStream_t stream;
+    cublasGemmAlgo_t algo;
+    // ctrls
+    int in_idx;
+    bool qkv_bias;         // true
+    bool projection_bias;  // true
+    bool is_cross;         // false
+    bool position_bias;    // false
+    bool layernorm_post;   // dont care
+    bool eft;              // false - effective fast trn
+    int *padding_offset;
+    int *d_sequence_length;
+} encoderParamT;
+
+template<typename T>
+size_t GetEncoderLayerWorkspaceSize(encoderParamT* param);
+
+template<typename T>
+size_t GetAttnWorkspaceSize(encoderParamT* param);
+template<typename T>
+void forward_attn(T* inputs[], int in_len, T* output[], int out_len, encoderParamT* param, void* ws);
+template<typename T>
+void forwardEncoder(void* inputs[], int in_len, void* output[], int out_len, encoderParamT* param, void* ws);
+// void forwardEncoder(std::vector<fastertransformer::Tensor, std::allocator<fastertransformer::Tensor> > const*
+// inputs);
+}  // namespace fastertransformer
diff --git a/src/fastertransformer/models/CMakeLists.txt b/src/fastertransformer/models/CMakeLists.txt
index af33e76..97fc471 100644
--- a/src/fastertransformer/models/CMakeLists.txt
+++ b/src/fastertransformer/models/CMakeLists.txt
@@ -21,8 +21,11 @@ add_subdirectory(xlnet)
 
 add_subdirectory(t5)
 add_subdirectory(gptj)
-add_subdirectory(multi_gpu_gpt)
+if(EXAMPLES)
+    add_subdirectory(multi_gpu_gpt)
+endif()
 add_subdirectory(swin)
 add_subdirectory(swin_int8)
 add_subdirectory(vit)
-add_subdirectory(vit_int8)
\ No newline at end of file
+add_subdirectory(vit_int8)
+add_subdirectory(ms)
\ No newline at end of file
diff --git a/src/fastertransformer/models/gptj/CMakeLists.txt b/src/fastertransformer/models/gptj/CMakeLists.txt
index d7d9d3e..e69a988 100644
--- a/src/fastertransformer/models/gptj/CMakeLists.txt
+++ b/src/fastertransformer/models/gptj/CMakeLists.txt
@@ -19,6 +19,7 @@ set_property(TARGET GptJDecoderLayerWeight PROPERTY POSITION_INDEPENDENT_CODE  O
 set_property(TARGET GptJDecoderLayerWeight PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
 target_link_libraries(GptJDecoderLayerWeight PUBLIC memory_utils)
 
+if(off)
 add_library(GptJDecoder STATIC GptJDecoder.cc)
 set_property(TARGET GptJDecoder PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET GptJDecoder PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
@@ -40,12 +41,14 @@ target_link_libraries(GptJContextDecoder PUBLIC -lcudart cublasMMWrapper
                       add_residual_kernels
                       gpt_kernels
                       nccl_utils)
+endif()
 
 add_library(GptJWeight STATIC GptJWeight.cc)
 set_property(TARGET GptJWeight PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET GptJWeight PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
 target_link_libraries(GptJWeight PUBLIC GptJDecoderLayerWeight)
 
+if(off)
 add_library(GptJ STATIC GptJ.cc)
 set_property(TARGET GptJ PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET GptJ PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
@@ -58,3 +61,4 @@ target_link_libraries(GptJ PUBLIC -lcudart
                       BaseBeamSearchLayer
                       bert_preprocess_kernels
                       GptJWeight)
+endif()
\ No newline at end of file
diff --git a/src/fastertransformer/models/gptj/GptJ.cc b/src/fastertransformer/models/gptj/GptJ.cc
index 0829e0d..fe41d4b 100644
--- a/src/fastertransformer/models/gptj/GptJ.cc
+++ b/src/fastertransformer/models/gptj/GptJ.cc
@@ -665,7 +665,7 @@ void GptJ<T>::forward(std::unordered_map<std::string, Tensor>* output_tensors,
                                           logits_buf_ + vocab_size_units_offset,
                                           CUDA_R_32F,
                                           vocab_size_padded_, /* n */
-                                          CUDA_R_32F,
+                                          CUBLAS_COMPUTE_32F_FAST_TF32,
                                           cublasGemmAlgo_t(-1));
                 }
                 else {
@@ -691,7 +691,7 @@ void GptJ<T>::forward(std::unordered_map<std::string, Tensor>* output_tensors,
                                               + tensor_para_.rank_ * local_batch_size * beam_width * local_vocab_size,
                                           CUDA_R_32F,
                                           local_vocab_size, /* n */
-                                          CUDA_R_32F,
+                                          CUBLAS_COMPUTE_32F_FAST_TF32,
                                           cublasGemmAlgo_t(-1));
                     ftNcclAllGather(nccl_logits_buf_ + vocab_size_units_offset,
                                     nccl_logits_buf_ + vocab_size_units_offset,
@@ -928,7 +928,7 @@ void GptJ<T>::forward(std::unordered_map<std::string, Tensor>* output_tensors,
             if (output_tensors->count("output_log_probs") > 0
                 && output_tensors->at("output_log_probs").data != nullptr) {
                 ftNcclSend(output_tensors->at("output_log_probs").getPtr<float>(),
-                           batch_size * beam_width * input_tensors->at("max_output_seq_len").getVal<int>(),
+                           output_tensors->at("output_log_probs").size(),
                            0,
                            pipeline_para_,
                            stream_);
@@ -958,7 +958,7 @@ void GptJ<T>::forward(std::unordered_map<std::string, Tensor>* output_tensors,
             if (output_tensors->count("output_log_probs") > 0
                 && output_tensors->at("output_log_probs").data != nullptr) {
                 ftNcclRecv(output_tensors->at("output_log_probs").getPtr<float>(),
-                           batch_size * beam_width * input_tensors->at("max_output_seq_len").getVal<int>(),
+                           output_tensors->at("output_log_probs").size(),
                            pipeline_para_.world_size_ - 1,
                            pipeline_para_,
                            stream_);
diff --git a/src/fastertransformer/models/ms/CMakeLists.txt b/src/fastertransformer/models/ms/CMakeLists.txt
new file mode 100644
index 0000000..8a99ce4
--- /dev/null
+++ b/src/fastertransformer/models/ms/CMakeLists.txt
@@ -0,0 +1,19 @@
+# Copyright (c) 2019-2022, NVIDIA CORPORATION.  All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+cmake_minimum_required(VERSION 3.8)
+
+add_executable(ms_gemm main.cc)
+# target_link_libraries(ms_gemm PUBLIC -lcudart encoder_gemm_func encoder_igemm_func memory_utils)
+target_link_libraries(ms_gemm PUBLIC -lcudart ms_gemm_func memory_utils)
diff --git a/src/fastertransformer/models/ms/main.cc b/src/fastertransformer/models/ms/main.cc
new file mode 100644
index 0000000..cd5844f
--- /dev/null
+++ b/src/fastertransformer/models/ms/main.cc
@@ -0,0 +1,179 @@
+/*
+ * Copyright (c) 2020-2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "src/fastertransformer/utils/gemm_test/ms_gemm_func.h"
+#include "src/fastertransformer/utils/memory_utils.h"
+
+namespace ft = fastertransformer;
+
+struct ms_opt_arg {
+        size_t batch_size;
+        size_t num_layers;
+        size_t seq_len;     // source seq len
+        size_t tgt_seq_len;
+        size_t head_num;
+        size_t hidden_size;
+        size_t size_per_head;
+        bool is_remove_padding;
+        int m;
+        int n;
+        int k;
+        std::string model_name;
+        std::string compute_type;
+        std::string w_compute_type;
+        std::string s_compute_type;
+};
+
+void usage() {
+  std::cout << "Usage: ms_benchmark -b<batch_size> -l <num_layers> -t <trg_seq_len>"
+            << "-s <src_seq_len> -H <head_num> -S <hidden_size> -p <is_remove_padding>"
+            << "-T <compute_type> -W <weights_compute_type> -F <softmax_compute_type>"
+            << "-m <model_name> -c <is_cross> -M <gemm m value> -N <gemm n value> -K <gemm k value>\n";
+}
+
+bool read_args(int argc, char* argv[], ms_opt_arg* opt_a) {
+    int opt;
+    while ((opt = getopt(argc, argv, "b:l:s:t:H:S:p:m:T:W:F:i:w:M:N:K:")) != -1) {
+      switch (opt) {
+        case 'b':
+            opt_a->batch_size = atoi(optarg);
+            break;
+        case 'l':
+            opt_a->num_layers = atoi(optarg);
+            break;
+        case 's':
+            opt_a->seq_len = atoi(optarg);
+            break;
+        case 't':
+            opt_a->tgt_seq_len = atoi(optarg);
+            break;
+        case 'H':
+            opt_a->head_num = atoi(optarg);
+            break;
+        case 'S':
+            opt_a->hidden_size = atoi(optarg);
+            break;
+        case 'p':
+            opt_a->is_remove_padding = static_cast<bool>(atoi(optarg));
+            break;
+        case 'm':
+            opt_a->model_name = std::string(optarg);
+            break;
+        case 'T':
+            opt_a->compute_type = std::string(optarg);
+            break;
+        case 'W':
+            opt_a->w_compute_type = std::string(optarg);
+            break;
+        case 'F':
+            opt_a->s_compute_type = std::string(optarg);
+            break;
+        case 'M':
+            opt_a->m = atoi(optarg);
+            break;
+        case 'N':
+            opt_a->n = atoi(optarg);
+            break;
+        case 'K':
+            opt_a->k = atoi(optarg);
+            break;
+        case 'i':
+        case 'w':    
+            break;    
+        case 'h':
+        default:
+            usage();
+            return false;
+      }
+    }
+    opt_a->size_per_head = opt_a->hidden_size / opt_a->head_num;
+    opt_a->tgt_seq_len = (opt_a->tgt_seq_len == -1) ? opt_a->seq_len : opt_a->tgt_seq_len;
+    return true;
+}
+
+int main(int argc, char* argv[])
+{
+    ms_opt_arg opt_a;
+    opt_a.batch_size = 1;
+    opt_a.num_layers = 1;
+    opt_a.seq_len = 1;
+    opt_a.tgt_seq_len = -1;
+    opt_a.head_num = 1;
+    opt_a.hidden_size = 1;
+    opt_a.size_per_head = 1;
+    opt_a.is_remove_padding = false;
+    opt_a.m = 1;
+    opt_a.n = 1;
+    opt_a.k = 1;
+    opt_a.model_name = "";
+    opt_a.compute_type = "fp32";
+    opt_a.w_compute_type = "fp32";
+    opt_a.s_compute_type = "fp32";
+
+    if (!read_args(argc, argv, &opt_a)) {
+      printf("[ERROR] Failed to read arguments. \n");
+      usage();
+      return 0;
+    }
+
+    bool c_type_fp32 =  (opt_a.compute_type.compare("fp32") == 0);
+    std::cout << "[INFO] arguments: " << std::endl;
+    std::cout << "  batch_size: " << opt_a.batch_size << std::endl;
+    std::cout << "  num of layers: " << opt_a.num_layers << std::endl;
+    std::cout << "  seq len:" << opt_a.seq_len << std::endl;
+    std::cout << "  target seq len: " << opt_a.tgt_seq_len << std::endl;
+    std::cout << "  head_num: " << opt_a.head_num << std::endl;
+    std::cout << "  size_per_head: " << opt_a.size_per_head << std::endl;
+    // std::cout << "  compute_type: " << c_type_fp32 << std::endl;
+
+    std::cout << std::endl;
+
+    const int inter_size = 4 * opt_a.head_num * opt_a.size_per_head;
+    const ft::CublasDataType data_type = static_cast<ft::CublasDataType>(0); // 0 FP32, 1 FP16, 2 BF 16
+    void* gemm_test_buf;
+    size_t buf_size_in_byte = ft::calGemmTestBufSizeInByte(opt_a.batch_size,
+                                                              opt_a.seq_len,
+                                                              opt_a.head_num,
+                                                              opt_a.size_per_head,
+                                                              inter_size,
+                                                              0,  // default
+                                                              0,  // default
+                                                              data_type);
+                                                        
+    size_t total, free;
+    ft::check_cuda_error(cudaMemGetInfo(&free, &total));
+    if (free < buf_size_in_byte + 10 * 1024 * 1024) {
+        printf("[ERROR] There is no enough device memory for gemm test!\n"
+               " %ld Bytes is needed, but only %ld Bytes is free.\n",
+               buf_size_in_byte,
+               free);
+        gemm_test_buf = NULL;
+        return -1;
+    } else {
+        ft::deviceMalloc(reinterpret_cast<char**>(&gemm_test_buf), buf_size_in_byte, false);
+    }
+    // int fast_algo = 0;
+    if (data_type == ft::FLOAT_DATATYPE) {
+        ft::generate_ms_gemm_config<float>(opt_a.batch_size, opt_a.seq_len, opt_a.tgt_seq_len, opt_a.head_num, opt_a.size_per_head, gemm_test_buf,
+                                            false);
+    } else {
+        printf("[ERROR] data type only supports fp32(0). \n");
+        return -1;
+    }
+    // std::cout << "main fast algo: " << fast_algo << std::endl;
+    ft::check_cuda_error(cudaFree(gemm_test_buf));
+    return 0;
+}
\ No newline at end of file
diff --git a/src/fastertransformer/models/multi_gpu_gpt/CMakeLists.txt b/src/fastertransformer/models/multi_gpu_gpt/CMakeLists.txt
index 10b9e0b..86d733f 100644
--- a/src/fastertransformer/models/multi_gpu_gpt/CMakeLists.txt
+++ b/src/fastertransformer/models/multi_gpu_gpt/CMakeLists.txt
@@ -37,7 +37,7 @@ set_property(TARGET ParallelGptDecoder PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
 target_link_libraries(ParallelGptDecoder PUBLIC -lcudart TensorParallelGeluFfnLayer
                                                 TensorParallelDecoderSelfAttentionLayer layernorm_kernels
                                                 add_residual_kernels nccl_utils)
-
+                                              
 add_library(ParallelGpt STATIC ParallelGpt.cc)
 set_property(TARGET ParallelGpt PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET ParallelGpt PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
diff --git a/src/fastertransformer/models/multi_gpu_gpt/ParallelGpt.cc b/src/fastertransformer/models/multi_gpu_gpt/ParallelGpt.cc
index 17f9099..d171b4b 100644
--- a/src/fastertransformer/models/multi_gpu_gpt/ParallelGpt.cc
+++ b/src/fastertransformer/models/multi_gpu_gpt/ParallelGpt.cc
@@ -345,7 +345,7 @@ void ParallelGpt<T>::computeContextCumLogProbs(float* cum_log_probs,
                                   lp_logits_buf_,
                                   CUDA_R_32F,
                                   vocab_size_padded_, /* n */
-                                  CUDA_R_32F,
+                                  CUBLAS_COMPUTE_32F_FAST_TF32,
                                   cublasGemmAlgo_t(-1));
             sync_check_cuda_error();
         }
@@ -370,7 +370,7 @@ void ParallelGpt<T>::computeContextCumLogProbs(float* cum_log_probs,
                                   lp_nccl_logits_buf_ + tensor_para_.rank_ * n_hidden_states * local_vocab_size,
                                   CUDA_R_32F,
                                   local_vocab_size, /* n */
-                                  CUDA_R_32F,
+                                  CUBLAS_COMPUTE_32F_FAST_TF32,
                                   cublasGemmAlgo_t(-1));
             sync_check_cuda_error();
             ftNcclAllGather(lp_nccl_logits_buf_,
@@ -803,7 +803,7 @@ void ParallelGpt<T>::forward(std::unordered_map<std::string, Tensor>* output_ten
                                           logits_buf_ + vocab_size_units_offset,
                                           CUDA_R_32F,
                                           vocab_size_padded_, /* n */
-                                          CUDA_R_32F,
+                                          CUBLAS_COMPUTE_32F_FAST_TF32,
                                           cublasGemmAlgo_t(-1));
                 }
                 else {
@@ -829,7 +829,7 @@ void ParallelGpt<T>::forward(std::unordered_map<std::string, Tensor>* output_ten
                                               + tensor_para_.rank_ * local_batch_size * beam_width * local_vocab_size,
                                           CUDA_R_32F,
                                           local_vocab_size, /* n */
-                                          CUDA_R_32F,
+                                          CUBLAS_COMPUTE_32F_FAST_TF32,
                                           cublasGemmAlgo_t(-1));
                     ftNcclAllGather(nccl_logits_buf_ + vocab_size_units_offset,
                                     nccl_logits_buf_ + vocab_size_units_offset,
@@ -1057,7 +1057,7 @@ void ParallelGpt<T>::forward(std::unordered_map<std::string, Tensor>* output_ten
             if (output_tensors->count("output_log_probs") > 0
                 && output_tensors->at("output_log_probs").data != nullptr) {
                 ftNcclSend(output_tensors->at("output_log_probs").getPtr<float>(),
-                           batch_size * beam_width * input_tensors->at("max_output_seq_len").getVal<int>(),
+                           output_tensors->at("output_log_probs").size(),
                            0,
                            pipeline_para_,
                            stream_);
@@ -1087,7 +1087,7 @@ void ParallelGpt<T>::forward(std::unordered_map<std::string, Tensor>* output_ten
             if (output_tensors->count("output_log_probs") > 0
                 && output_tensors->at("output_log_probs").data != nullptr) {
                 ftNcclRecv(output_tensors->at("output_log_probs").getPtr<float>(),
-                           batch_size * beam_width * input_tensors->at("max_output_seq_len").getVal<int>(),
+                           output_tensors->at("output_log_probs").size(),
                            pipeline_para_.world_size_ - 1,
                            pipeline_para_,
                            stream_);
diff --git a/src/fastertransformer/models/t5/CMakeLists.txt b/src/fastertransformer/models/t5/CMakeLists.txt
index 9f3455d..e75bbbd 100644
--- a/src/fastertransformer/models/t5/CMakeLists.txt
+++ b/src/fastertransformer/models/t5/CMakeLists.txt
@@ -14,6 +14,7 @@
 
 cmake_minimum_required(VERSION 3.8)
 
+if(False)
 add_library(T5Decoder STATIC T5Decoder.cc T5DecoderLayerWeight.cc)
 set_property(TARGET T5Decoder PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET T5Decoder PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
@@ -21,6 +22,7 @@ target_link_libraries(T5Decoder PUBLIC -lcudart cublasMMWrapper TensorParallelDe
                     TensorParallelDecoderCrossAttentionLayer TensorParallelReluFfnLayer 
                     layernorm_kernels add_residual_kernels nccl_utils memory_utils)
 
+
 add_library(T5Decoding STATIC T5Decoding.cc T5DecodingWeight.cc)
 set_property(TARGET T5Decoding PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET T5Decoding PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
@@ -28,6 +30,8 @@ target_link_libraries(T5Decoding PUBLIC -lcudart cublasMMWrapper T5Decoder bert_
                                         decoding_kernels DynamicDecodeLayer BaseBeamSearchLayer 
                                         beam_search_topk_kernels gpt_kernels)
 
+
+
 add_library(T5Encoder STATIC T5Encoder.cc T5EncoderWeight.cc T5EncoderLayerWeight.cc)
 set_property(TARGET T5Encoder PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET T5Encoder PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
@@ -36,4 +40,5 @@ target_link_libraries(T5Encoder PUBLIC -lcudart bert_preprocess_kernels cublasMM
                         TensorParallelGeluFfnLayer layernorm_kernels add_residual_kernels nccl_utils)
 
 add_executable(t5_gemm t5_gemm.cc)
-target_link_libraries(t5_gemm PUBLIC -lcudart t5_gemm_func memory_utils)
\ No newline at end of file
+target_link_libraries(t5_gemm PUBLIC -lcudart t5_gemm_func memory_utils)
+endif()
\ No newline at end of file
diff --git a/src/fastertransformer/models/t5/T5Encoder.cc b/src/fastertransformer/models/t5/T5Encoder.cc
index 698e3d6..db989ff 100644
--- a/src/fastertransformer/models/t5/T5Encoder.cc
+++ b/src/fastertransformer/models/t5/T5Encoder.cc
@@ -380,7 +380,7 @@ void T5Encoder<T>::forward(std::unordered_map<std::string, Tensor>* output_tenso
                                                      request_seq_len,
                                                      request_seq_len,
                                                      local_batch_size,
-                                                     hidden_units_,
+                                                     d_model_,
                                                      stream_);
         }
         else {
diff --git a/src/fastertransformer/utils/CMakeLists.txt b/src/fastertransformer/utils/CMakeLists.txt
index 3d0f28a..3d2efbd 100644
--- a/src/fastertransformer/utils/CMakeLists.txt
+++ b/src/fastertransformer/utils/CMakeLists.txt
@@ -44,10 +44,12 @@ set_property(TARGET memory_utils PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET memory_utils PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
 target_link_libraries(memory_utils PUBLIC -lnvToolsExt)
 
+if(EXAMPLES)
 add_library(nccl_utils STATIC nccl_utils.cc)
 set_property(TARGET nccl_utils PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET nccl_utils PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
 target_link_libraries(nccl_utils PUBLIC -lnccl)
+endif()
 
 add_library(cublasINT8MMWrapper STATIC cublasINT8MMWrapper.cc)
 set_property(TARGET cublasINT8MMWrapper PROPERTY POSITION_INDEPENDENT_CODE  ON)
diff --git a/src/fastertransformer/utils/cublasMMWrapper.cc b/src/fastertransformer/utils/cublasMMWrapper.cc
index e291151..e0c6d20 100644
--- a/src/fastertransformer/utils/cublasMMWrapper.cc
+++ b/src/fastertransformer/utils/cublasMMWrapper.cc
@@ -99,7 +99,7 @@ void cublasMMWrapper::Gemm(cublasOperation_t transa,
                            void* C,
                            cudaDataType_t Ctype,
                            int ldc,
-                           cudaDataType_t computeType,
+                           cublasComputeType_t computeType,
                            cublasGemmAlgo_t algo)
 {
     mu_->lock();
@@ -160,7 +160,7 @@ void cublasMMWrapper::Gemm(cublasOperation_t transa,
 
     mu_->lock();
     // TODO: default cublas libs
-    int is_fp16_computeType = computeType_ == CUDA_R_16F ? 1 : 0;
+    int is_fp16_computeType = computeType_ == CUBLAS_COMPUTE_16F ? 1 : 0;
     bool using_cublasLt = (Atype_ == CUDA_R_16F) ? true : false;
     int batch_count = 1;
     // fp32 use cublas as default
@@ -187,14 +187,14 @@ void cublasMMWrapper::Gemm(cublasOperation_t transa,
 #if (CUDART_VERSION >= 11000)
         cublasComputeType_t computeType;
 #else
-        cudaDataType_t computeType;
+        cublasComputeType_t computeType;
 #endif
 
         if (is_fp16_computeType) {
 #if (CUDART_VERSION >= 11000)
             computeType = CUBLAS_COMPUTE_16F;
 #else
-            computeType = CUDA_R_16F;
+            computeType = CUBLAS_COMPUTE_16F;
 #endif
             scaleType = CUDA_R_16F;
         }
@@ -302,7 +302,7 @@ void cublasMMWrapper::setFP32GemmConfig()
     Atype_ = CUDA_R_32F;
     Btype_ = CUDA_R_32F;
     Ctype_ = CUDA_R_32F;
-    computeType_ = CUDA_R_32F;
+    computeType_ = CUBLAS_COMPUTE_32F_FAST_TF32;
 }
 
 void cublasMMWrapper::setFP16GemmConfig()
@@ -310,7 +310,23 @@ void cublasMMWrapper::setFP16GemmConfig()
     Atype_ = CUDA_R_16F;
     Btype_ = CUDA_R_16F;
     Ctype_ = CUDA_R_16F;
-    computeType_ = CUDA_R_32F;
+    computeType_ = CUBLAS_COMPUTE_16F;
+}
+
+void cublasMMWrapper::setFP32MixedGemmConfig()
+{
+    Atype_ = CUDA_R_32F;
+    Btype_ = CUDA_R_16F;
+    Ctype_ = CUDA_R_32F;
+    computeType_ = CUBLAS_COMPUTE_32F_FAST_TF32;
+}
+
+void cublasMMWrapper::setFP16MixedGemmConfig()
+{
+    Atype_ = CUDA_R_16F;
+    Btype_ = CUDA_R_32F;
+    Ctype_ = CUDA_R_32F;
+    computeType_ = CUBLAS_COMPUTE_32F_FAST_TF32;
 }
 
 #ifdef ENABLE_BF16
@@ -319,14 +335,14 @@ void cublasMMWrapper::setBF16GemmConfig()
     Atype_ = CUDA_R_16BF;
     Btype_ = CUDA_R_16BF;
     Ctype_ = CUDA_R_16BF;
-    computeType_ = CUDA_R_32F;
+    computeType_ = CUBLAS_COMPUTE_16F;
 }
 #endif
 
 void cublasMMWrapper::setGemmConfig(cudaDataType_t aType,
                                     cudaDataType_t bType,
                                     cudaDataType_t cType,
-                                    cudaDataType_t computeType)
+                                    cublasComputeType_t computeType)
 {
     Atype_ = aType;
     Btype_ = bType;
@@ -451,7 +467,7 @@ void cublasMMWrapper::stridedBatchedGemm(cublasOperation_t transa,
     half h_beta = (half)f_beta;
 
     mu_->lock();
-    int is_fp16_computeType = computeType_ == CUDA_R_16F ? 1 : 0;
+    int is_fp16_computeType = computeType_ == CUBLAS_COMPUTE_16F ? 1 : 0;
     const void* alpha =
         is_fp16_computeType ? reinterpret_cast<void*>(&h_alpha) : reinterpret_cast<const void*>(&f_alpha);
     const void* beta = is_fp16_computeType ? reinterpret_cast<void*>(&h_beta) : reinterpret_cast<const void*>(&f_beta);
@@ -504,13 +520,13 @@ void cublasMMWrapper::stridedBatchedGemm(cublasOperation_t transa,
                                          const int ldc,
                                          const int64_t strideC,
                                          const int batch_count,
-                                         cudaDataType_t computeType)
+                                         cublasComputeType_t computeType)
 {
     half h_alpha = (half)f_alpha;
     half h_beta = (half)f_beta;
 
     mu_->lock();
-    int is_fp16_computeType = computeType == CUDA_R_16F ? 1 : 0;
+    int is_fp16_computeType = computeType == CUBLAS_COMPUTE_16F ? 1 : 0;
     const void* alpha =
         is_fp16_computeType ? reinterpret_cast<void*>(&h_alpha) : reinterpret_cast<const void*>(&f_alpha);
     const void* beta = is_fp16_computeType ? reinterpret_cast<void*>(&h_beta) : reinterpret_cast<const void*>(&f_beta);
@@ -563,7 +579,7 @@ void cublasMMWrapper::batchedGemm(cublasOperation_t transa,
     half h_beta = (half)0.0f;
 
     mu_->lock();
-    int is_fp16_computeType = computeType_ == CUDA_R_16F ? 1 : 0;
+    int is_fp16_computeType = computeType_ == CUBLAS_COMPUTE_16F ? 1 : 0;
     const void* alpha = is_fp16_computeType ? reinterpret_cast<void*>(&h_alpha) : reinterpret_cast<void*>(&f_alpha);
     const void* beta = is_fp16_computeType ? reinterpret_cast<void*>(&h_beta) : reinterpret_cast<void*>(&f_beta);
     cublasLtMatmulAlgo_info info = cublas_algo_map_->getAlgo(batch_count, m, n, k, getCublasDataType(Atype_));
diff --git a/src/fastertransformer/utils/cublasMMWrapper.h b/src/fastertransformer/utils/cublasMMWrapper.h
index 6f410ab..21a8ea8 100644
--- a/src/fastertransformer/utils/cublasMMWrapper.h
+++ b/src/fastertransformer/utils/cublasMMWrapper.h
@@ -41,7 +41,7 @@ private:
     cudaDataType_t Atype_;
     cudaDataType_t Btype_;
     cudaDataType_t Ctype_;
-    cudaDataType_t computeType_;
+    cublasComputeType_t computeType_;
 
     cudaStream_t stream_;
     cublasAlgoMap* cublas_algo_map_;
@@ -90,7 +90,7 @@ public:
               void* C,
               cudaDataType_t Ctype,
               int ldc,
-              cudaDataType_t computeType,
+              cublasComputeType_t computeType,
               cublasGemmAlgo_t algo);
 
     void Gemm(cublasOperation_t transa,
@@ -121,12 +121,14 @@ public:
 
     void setFP32GemmConfig();
     void setFP16GemmConfig();
+    void setFP32MixedGemmConfig();
+    void setFP16MixedGemmConfig();
 #ifdef ENABLE_BF16
     void setBF16GemmConfig();
 #endif
     void setStream(cudaStream_t stream);
 
-    void setGemmConfig(cudaDataType_t aType, cudaDataType_t bType, cudaDataType_t cType, cudaDataType_t computeType);
+    void setGemmConfig(cudaDataType_t aType, cudaDataType_t bType, cudaDataType_t cType, cublasComputeType_t computeType);
 
     CublasDataType getCublasDataType(cudaDataType_t data_type);
 
@@ -183,7 +185,7 @@ public:
                             const int ldc,
                             const int64_t strideC,
                             const int batch_count,
-                            cudaDataType_t computeType);
+                            cublasComputeType_t computeType);
 
     void batchedGemm(cublasOperation_t transa,
                      cublasOperation_t transb,
diff --git a/src/fastertransformer/utils/cuda_utils.h b/src/fastertransformer/utils/cuda_utils.h
index 5d73c87..aef6ab9 100644
--- a/src/fastertransformer/utils/cuda_utils.h
+++ b/src/fastertransformer/utils/cuda_utils.h
@@ -382,7 +382,7 @@ public:
 
 static double diffTime(timeval start, timeval end)
 {
-    return (end.tv_sec - start.tv_sec) * 1000 + (end.tv_usec - start.tv_usec) * 0.001;
+    return (end.tv_sec - start.tv_sec) * 1000000 + (end.tv_usec - start.tv_usec);
 }
 
 /* ***************************** common utils ****************************** */
diff --git a/src/fastertransformer/utils/custom_ar_comm.cc b/src/fastertransformer/utils/custom_ar_comm.cc
index ded1e58..159faaf 100644
--- a/src/fastertransformer/utils/custom_ar_comm.cc
+++ b/src/fastertransformer/utils/custom_ar_comm.cc
@@ -54,6 +54,7 @@ void CustomAllReduceComm<T>::customAllReduce(size_t elts, cudaStream_t stream)
     output_tensor_->at(0).data = (const void*)tmp_tensor_data_;
 }
 
+
 template<typename T>
 void CustomAllReduceComm<T>::allocateAndExchangePeerAccessPointer(
     std::vector<std::shared_ptr<AbstractCustomComm>>* custom_all_reduce_comms)
diff --git a/src/fastertransformer/utils/gemm_test/CMakeLists.txt b/src/fastertransformer/utils/gemm_test/CMakeLists.txt
index 223b85d..ab48356 100644
--- a/src/fastertransformer/utils/gemm_test/CMakeLists.txt
+++ b/src/fastertransformer/utils/gemm_test/CMakeLists.txt
@@ -49,6 +49,10 @@ set(swin_gemm_func_files
   swin_gemm_func.cc
 )
 
+set(ms_gemm_func_files
+  ms_gemm_func.cc
+)
+
 add_library(gemm_func STATIC ${gemm_func_files})
 target_link_libraries(gemm_func PUBLIC -lcublas -lcublasLt -lcudart)
 set_property(TARGET gemm_func PROPERTY POSITION_INDEPENDENT_CODE ON)
@@ -109,3 +113,12 @@ add_library(swin_gemm_func STATIC ${swin_gemm_func_files})
 target_link_libraries(swin_gemm_func PUBLIC -lcublas -lcublasLt -lcudart gemm_func)
 set_property(TARGET swin_gemm_func PROPERTY POSITION_INDEPENDENT_CODE ON)
 set_property(TARGET swin_gemm_func PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)
+
+add_library(ms_gemm_func STATIC ${ms_gemm_func_files})
+if (SPARSITY_SUPPORT)
+target_link_libraries(ms_gemm_func PUBLIC -lcublas -lcublasLt -lcudart gemm_func -lcusparse -lcusparseLt)
+else()
+target_link_libraries(ms_gemm_func PUBLIC -lcublas -lcublasLt -lcudart gemm_func)
+endif()
+set_property(TARGET ms_gemm_func PROPERTY POSITION_INDEPENDENT_CODE ON)
+set_property(TARGET ms_gemm_func PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)
\ No newline at end of file
diff --git a/src/fastertransformer/utils/gemm_test/ms_gemm_func.cc b/src/fastertransformer/utils/gemm_test/ms_gemm_func.cc
new file mode 100644
index 0000000..e8f88fe
--- /dev/null
+++ b/src/fastertransformer/utils/gemm_test/ms_gemm_func.cc
@@ -0,0 +1,364 @@
+/*
+ * Copyright (c) 2020-2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "src/fastertransformer/utils/gemm_test/ms_gemm_func.h"
+
+namespace fastertransformer {
+
+template<typename T>
+void generate_ms_gemm_config(
+    int batch_size, int seq_len, int tgt_seq_len, int head_num, int size_per_head, void* buffer_in, bool isAppend)
+{
+    void* cublas_workspace;
+    void* buffer;
+    int workSpaceSize;
+
+#ifdef ENABLE_BF16
+    if (std::is_same<T, half>::value || std::is_same<T, __nv_bfloat16>::value) {
+#else
+    if (std::is_same<T, half>::value) {
+#endif  // ENABLE_BF16
+        // cublas_workspace_ should be the start pointer of cudaMalloc()
+        // to ensure 16B alignemnet
+        cublas_workspace = buffer_in;
+        buffer = (void*)((char*)cublas_workspace + CUBLAS_WORKSPACE_SIZE);
+        workSpaceSize = CUBLAS_WORKSPACE_SIZE;
+    }
+    else {
+        cublas_workspace = nullptr;
+        buffer = buffer_in;
+        workSpaceSize = 0;
+    }
+
+    struct cudaDeviceProp prop;
+    check_cuda_error(cudaGetDeviceProperties(&prop, 0));
+    printf("Device %s\n", prop.name);
+
+    // check config
+    FILE* fd;
+    int line_count = 0;
+    if (!isAppend) {
+        fd = fopen(GEMM_CONFIG, "w+");
+    }
+    else {
+        fd = fopen(GEMM_CONFIG, "a+");
+        std::vector<std::string> config;
+        char line[1024];
+        while (fgets(line, 1024, fd) != NULL) {
+            config.push_back(std::string(line));
+        }
+        line_count = config.size();
+        if (config.size() >= (MAX_CONFIG_NUM * GEMM_NUM + 1))  // 6 cublas/cublasLt, first row is not included
+        {
+            int startIdx = config.size() - ((MAX_CONFIG_NUM - 1) * GEMM_NUM);
+            fclose(fd);
+            fd = fopen(GEMM_CONFIG, "w+");
+            fprintf(fd, "%s", config[0].c_str());
+            for (uint i = startIdx; i < config.size(); i++) {
+                fprintf(fd, "%s", config[i].c_str());
+            }
+            line_count = config.size() - (GEMM_NUM + 3);
+        }
+    }
+
+    const int gemm_num = 4;
+    int M[gemm_num];
+    int N[gemm_num];
+    int K[gemm_num];
+    int batchCount[gemm_num] = {1, 1, 1, 1};
+    char mess[gemm_num][256];
+    float exec_times[gemm_num];
+    int gemm_lds[gemm_num][3]; // = {3 * hidden_size, hidden_size, 3 * hidden_size};
+    cublasOperation_t gemm_ops[gemm_num][2]; // = {CUBLAS_OP_N, CUBLAS_OP_N};
+    int gemm_strides[2][3];
+
+    // gemm1
+    // int gemm_dims[] = {3 * hidden_size, request_batch_size * request_src_seq_len, hidden_size};
+    int hidden_size = head_num * size_per_head;
+    M[0] = 3 * hidden_size;
+    N[0] = batch_size * seq_len;
+    K[0] = hidden_size;
+    gemm_lds[0][0] = 3 * hidden_size;
+    gemm_lds[0][1] = hidden_size;
+    gemm_lds[0][2] = 3 * hidden_size;
+    gemm_ops[0][0] = CUBLAS_OP_N;
+    gemm_ops[0][1] = CUBLAS_OP_N;
+    strcpy(mess[0], "cublasGemmEx ");
+
+    // gemm2
+    M[1] = tgt_seq_len;
+    N[1] = seq_len;
+    K[1] = size_per_head;
+    gemm_ops[1][0] = CUBLAS_OP_T;
+    gemm_ops[1][1] = CUBLAS_OP_N;
+
+    gemm_lds[1][0] = size_per_head;
+    gemm_lds[1][1] = size_per_head;
+    gemm_lds[1][2] = tgt_seq_len;
+    
+    gemm_strides[0][0] = tgt_seq_len * size_per_head;
+    gemm_strides[0][1] = seq_len * size_per_head;
+    gemm_strides[0][2] =  seq_len * tgt_seq_len;
+    strcpy(mess[1], "cublasGemmStridedBatchedEx");
+
+    // gemm3
+    M[2] = size_per_head;
+    N[2] = seq_len;
+    K[2] = tgt_seq_len;
+    gemm_ops[2][0] = CUBLAS_OP_N;
+    gemm_ops[2][1] = CUBLAS_OP_N;
+
+    gemm_lds[2][0] = size_per_head;
+    gemm_lds[2][1] = tgt_seq_len;
+    gemm_lds[2][2] = size_per_head;
+
+    gemm_strides[1][0] = tgt_seq_len * size_per_head;
+    gemm_strides[1][1] = seq_len * tgt_seq_len;
+    gemm_strides[1][2] = seq_len * size_per_head;
+    strcpy(mess[2], "cublasGemmStridedBatchedEx");
+
+    // gemm4
+    M[3] = hidden_size;
+    N[3] = batch_size * seq_len;
+    K[3] = hidden_size;
+    gemm_ops[3][0] = CUBLAS_OP_N;
+    gemm_ops[3][1] = CUBLAS_OP_N;
+
+    gemm_lds[3][0] = hidden_size;
+    gemm_lds[3][1] = hidden_size;
+    gemm_lds[3][2] = hidden_size;
+    strcpy(mess[3], "cublasGemmEx");
+
+    cublasHandle_t cublas_handle;
+    check_cuda_error(cublasCreate(&cublas_handle));
+    cublasLtHandle_t ltHandle;
+    check_cuda_error(cublasLtCreate(&ltHandle));
+
+    cudaDataType_t AType;
+    cudaDataType_t BType;
+    cudaDataType_t CType;
+    cublasComputeType_t computeType;
+    int startAlgo, endAlgo;
+    const int ites = 10000;
+    const int warmup_ites = 10000;
+    struct timeval start, end;
+
+    CublasDataType data_type;
+    if (std::is_same<T, float>::value) {
+        data_type = FLOAT_DATATYPE;
+        AType = CUDA_R_32F;
+        BType = CUDA_R_32F;
+        CType = CUDA_R_32F;
+        computeType = CUBLAS_COMPUTE_32F_FAST_TF32;
+        startAlgo = (int)CUBLAS_GEMM_DEFAULT_TENSOR_OP;
+        endAlgo = (int)CUBLAS_GEMM_ALGO15_TENSOR_OP;
+    }
+    else if (std::is_same<T, half>::value) {
+        data_type = HALF_DATATYPE;
+        AType = CUDA_R_16F;
+        BType = CUDA_R_16F;
+        CType = CUDA_R_16F;
+        computeType = CUBLAS_COMPUTE_16F;
+        startAlgo = (int)CUBLAS_GEMM_DEFAULT_TENSOR_OP;
+        endAlgo = (int)CUBLAS_GEMM_ALGO15_TENSOR_OP;
+    }
+#ifdef ENABLE_BF16
+    else if (std::is_same<T, __nv_bfloat16>::value) {
+        data_type = BFLOAT16_DATATYPE;
+        AType = CUDA_R_16BF;
+        BType = CUDA_R_16BF;
+        CType = CUDA_R_16BF;
+        computeType = CUBLAS_COMPUTE_32F;
+        startAlgo = (int)CUBLAS_GEMM_DEFAULT_TENSOR_OP;
+        endAlgo = (int)CUBLAS_GEMM_ALGO15_TENSOR_OP;
+    }
+#endif
+    using scaleT = typename ScaleTypeConverter<T, false>::Type;
+
+    scaleT alpha = (scaleT)1.0f;
+    scaleT beta = (scaleT)0.0f;
+
+    printf("***Encoder Gemm Testing Begin***\n");
+    printf("***Cublas Gemm Testing Begin***\n");
+    if (line_count == 0) {
+        fprintf(fd,
+                "batch_size, seq_len, head_num, size_per_head dataType ### batchCount, n, m, k, algoId, "
+                "customOption, tile, numSplitsK, swizzle, reductionScheme, workspaceSize, stages, exec_time\n");
+    }
+    for (int i = 0; i < gemm_num; ++i) {
+        // if(i != 0 && i != 5) continue;
+
+        int m = M[i], n = N[i], k = K[i];
+        printf("\n-----------------------------\n");
+        printf("GEMM test %d: [M: %d, K: %d, N: %d] %s\n", i, m, k, n, mess[i]);
+        // printf("GEMM test %d: [M: %d, K: %d, N: %d] \n", i, m, k, n);
+        T* d_A = (T*)buffer;
+        T* d_B = d_A + m * k * batchCount[i];
+        T* d_C = d_B + k * n * batchCount[i];
+
+        // array of pointer for batchedGemm
+        T* harray[12];
+        harray[0] = (T*)buffer;
+        harray[1] = (T*)((char*)buffer + sizeof(T) * m * k);
+        harray[2] = (T*)((char*)buffer + 2 * sizeof(T) * m * k);
+        harray[4] = (T*)((char*)buffer + 3 * sizeof(T) * m * k);
+        harray[5] = (T*)((char*)buffer + 3 * sizeof(T) * m * k + sizeof(T) * k * n);
+        harray[6] = (T*)((char*)buffer + 3 * sizeof(T) * m * k + 2 * sizeof(T) * k * n);
+        harray[8] = (T*)((char*)buffer + 3 * sizeof(T) * m * k + 3 * sizeof(T) * k * n);
+        harray[9] = (T*)((char*)buffer + 3 * sizeof(T) * m * k + 3 * sizeof(T) * k * n + sizeof(T) * m * n);
+        harray[10] = (T*)((char*)buffer + 3 * sizeof(T) * m * k + 3 * sizeof(T) * k * n + 2 * sizeof(T) * m * n);
+
+        T** darray = 0;
+        check_cuda_error(cudaMalloc((void**)&darray, sizeof(T*) * 12));
+        cudaMemcpy((void*)darray, (void*)harray, sizeof(T*) * 12, cudaMemcpyHostToDevice);
+        T** dAarray = darray;
+        T** dBarray = darray + 4;
+        T** dCarray = darray + 8;
+
+        float exec_time = 99999.0f;
+        int fast_algo = 0;
+        
+        // warmup
+        // for (int j = 0; j < ites*10; j++) {
+        //     cublasGemmEx(cublas_handle, gemm_ops[i][0], gemm_ops[i][1], m, n, k, &alpha, d_A, AType, gemm_lds[i][0], d_B, BType,
+        //                          gemm_lds[i][1], &beta, d_C, CType, gemm_lds[i][2], computeType, static_cast<cublasGemmAlgo_t>(0));
+        // }
+
+        for (int algo = startAlgo; algo <= endAlgo; algo++) {
+            cublasStatus_t status;
+            //warmup
+            for (int ite = 0; ite < warmup_ites; ++ite) {
+                if ((i == 0) || (i == 3)) {
+                    status = cublasGemmEx(cublas_handle, gemm_ops[i][0], gemm_ops[i][1], m, n, k, &alpha, d_A, AType, gemm_lds[i][0], d_B, BType,
+                                 gemm_lds[i][1], &beta, d_C, CType, gemm_lds[i][2], computeType, static_cast<cublasGemmAlgo_t>(algo));
+                } else {
+                    status = cublasGemmStridedBatchedEx(cublas_handle, gemm_ops[i][0], gemm_ops[i][1], m, n, k, &alpha, d_A, AType, gemm_lds[i][0],
+                                               gemm_strides[i-1][0], d_B, BType, gemm_lds[i][1], gemm_strides[i-1][1], &beta, d_C, CType, 
+                                               gemm_lds[i][2], gemm_strides[i-1][2], batch_size, computeType, static_cast<cublasGemmAlgo_t>(algo));
+                }
+            }
+            cudaDeviceSynchronize();
+            gettimeofday(&start, NULL);
+            if ((i == 0) || (i == 3)) {
+                for (int ite = 0; ite < ites; ++ite) {
+                    status = cublasGemmEx(cublas_handle, gemm_ops[i][0], gemm_ops[i][1], m, n, k, &alpha, d_A, AType, gemm_lds[i][0], d_B, BType,
+                                 gemm_lds[i][1], &beta, d_C, CType, gemm_lds[i][2], computeType, static_cast<cublasGemmAlgo_t>(algo));
+                }
+            } else {
+                for (int ite = 0; ite < ites; ++ite) {
+                    status = cublasGemmStridedBatchedEx(cublas_handle, gemm_ops[i][0], gemm_ops[i][1], m, n, k, &alpha, d_A, AType, gemm_lds[i][0],
+                                               gemm_strides[i-1][0], d_B, BType, gemm_lds[i][1], gemm_strides[i-1][1], &beta, d_C, CType, 
+                                               gemm_lds[i][2], gemm_strides[i-1][2], batch_size, computeType, static_cast<cublasGemmAlgo_t>(algo));
+                }
+            }
+
+                if (status != CUBLAS_STATUS_SUCCESS) {
+                    break;
+                }
+            // }
+            cudaDeviceSynchronize();
+            gettimeofday(&end, NULL);
+            if (status == CUBLAS_STATUS_SUCCESS) {
+                printf("algo_%d costs %.6fms \n", algo, diffTime(start, end) / ites);
+                if (diffTime(start, end) / ites < exec_time) {
+                    exec_time = diffTime(start, end) / ites;
+                    fast_algo = algo;
+                }
+            }
+        }
+        printf("fast_algo %d costs %.6f ms \n", fast_algo, exec_time);
+
+        // for fp16 and bf16, we compare cublasLt
+        if (i < 3 && data_type != FLOAT_DATATYPE) {
+            printf("***cublasLt Gemm Testing Beign***\n");
+            // Let try a fixed number of combinations
+            int ALGO_COMBINATIONS = 5000;
+            customMatmulPerf_t perfResults[ALGO_COMBINATIONS];
+            LtHgemmCustomFind<T, scaleT>(ltHandle,
+                                         batch_size,
+                                         seq_len,
+                                         head_num,
+                                         size_per_head,
+                                         n,
+                                         m,
+                                         k,
+                                         &alpha,
+                                         d_B,
+                                         d_A,
+                                         &beta,
+                                         d_C,
+                                         cublas_workspace,
+                                         workSpaceSize,
+                                         fd,
+                                         perfResults,
+                                         ALGO_COMBINATIONS);
+            if (perfResults[0].time < exec_time) {
+                printPerfStructure(
+                    batch_size, seq_len, head_num, size_per_head, n, m, k, perfResults[0], fd, data_type, 0);
+                exec_time = perfResults[0].time;
+            }
+            else {
+                fprintf(fd,
+                        "%d %d %d %d %d ### %d %d %d %d %d -1 -1 -1 -1 -1 -1 -1 %f\n",
+                        batch_size,
+                        seq_len,
+                        head_num,
+                        size_per_head,
+                        data_type,
+                        batchCount[i],
+                        n,
+                        m,
+                        k,
+                        fast_algo,
+                        exec_time);
+            }
+            printf("***cublasLt Gemm Testing End***\n");
+        }
+        else {
+            fprintf(fd,
+                    "%d %d %d %d %d ### %d %d %d %d %d -1 -1 -1 -1 -1 -1 -1 %f\n",
+                    batch_size,
+                    seq_len,
+                    head_num,
+                    size_per_head,
+                    data_type,
+                    batchCount[i],
+                    n,
+                    m,
+                    k,
+                    fast_algo,
+                    exec_time);
+        }
+        exec_times[i] = exec_time;
+        cudaFree(darray);
+    }
+    printf("***cublas Gemm Testing End***\n\n");
+    fclose(fd);
+    printf("***Encoder Gemm Testing End***\n");
+
+    return;
+}
+
+template void generate_ms_gemm_config<float>(
+    int batch_size, int seq_len, int tgt_seq_len, int head_num, int size_per_head, void* buffer, bool isAppend);
+template void generate_ms_gemm_config<half>(
+    int batch_size, int seq_len, int tgt_seq_len, int head_num, int size_per_head, void* buffer, bool isAppend);
+#ifdef ENABLE_BF16
+template void generate_ms_gemm_config<__nv_bfloat16>(
+    int batch_size, int seq_len, int tgt_seq_len, int head_num, int size_per_head, void* buffer, bool isAppend);
+#endif
+
+}  // namespace fastertransformer
diff --git a/src/fastertransformer/utils/gemm_test/ms_gemm_func.h b/src/fastertransformer/utils/gemm_test/ms_gemm_func.h
new file mode 100644
index 0000000..c6f68ca
--- /dev/null
+++ b/src/fastertransformer/utils/gemm_test/ms_gemm_func.h
@@ -0,0 +1,40 @@
+/*
+ * Copyright (c) 2020-2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "src/fastertransformer/utils/cublasAlgoMap.h"
+#include "src/fastertransformer/utils/cuda_bf16_wrapper.h"
+#include "src/fastertransformer/utils/cuda_utils.h"
+#include "src/fastertransformer/utils/gemm_test/gemm_func.h"
+
+#include <cstdio>
+#include <cstdlib>
+#include <ctime>
+#include <cuda_fp16.h>
+#include <cuda_profiler_api.h>
+#include <map>
+#include <sys/time.h>
+#include <unistd.h>
+#include <vector>
+
+namespace fastertransformer {
+
+template<typename T>
+void generate_ms_gemm_config(
+    int batch_size, int seq_len, int tgt_seq_len, int head_num, int size_per_head, void* buffer, bool isAppend = true);
+
+}  // namespace fastertransformer
diff --git a/src/fastertransformer/utils/logger.h b/src/fastertransformer/utils/logger.h
index bcdf8fa..e3e7007 100644
--- a/src/fastertransformer/utils/logger.h
+++ b/src/fastertransformer/utils/logger.h
@@ -65,7 +65,7 @@ private:
 #else
     const Level DEFAULT_LOG_LEVEL = INFO;
 #endif
-    Level level_ = DEFAULT_LOG_LEVEL;
+    Level level_ = ERROR; // DEFAULT_LOG_LEVEL;
 
     Logger()
     {
diff --git a/tests/unittests/test_gemm.cu b/tests/unittests/test_gemm.cu
index 13719f7..4ecf0bd 100644
--- a/tests/unittests/test_gemm.cu
+++ b/tests/unittests/test_gemm.cu
@@ -157,7 +157,7 @@ void computeReference(GemmOp transa,
     cudaDataType_t atype = (A.type == TYPE_FP16) ? CUDA_R_16F : CUDA_R_32F;
     cudaDataType_t btype = (B.type == TYPE_FP16) ? CUDA_R_16F : CUDA_R_32F;
     cudaDataType_t ctype = (C.type == TYPE_FP16) ? CUDA_R_16F : CUDA_R_32F;
-    cudaDataType_t compute_type = (computeType == TYPE_FP16) ? CUDA_R_16F : CUDA_R_32F;
+    cublasComputeType_t compute_type = (computeType == TYPE_FP16) ? CUBLAS_COMPUTE_16F : CUBLAS_COMPUTE_32F_FAST_TF32;
 
     cublasHandle_t cublas_handle;
     check_cuda_error(cublasCreate(&cublas_handle));
@@ -391,7 +391,11 @@ void testGemmConsistencyMatmul(size_t m, size_t n, size_t k) {
 
     cudaDataType_t cuda_dtype = std::is_same<float, T>::value ? CUDA_R_32F : CUDA_R_16F;
     cudaDataType_t cuda_ctype = (DataType::TYPE_FP32 == computeType) ? CUDA_R_32F : CUDA_R_16F;
-    cublas_wrapper.setGemmConfig(cuda_dtype, cuda_dtype, cuda_dtype, cuda_ctype);
+    // add culab type
+    cublasComputeType_t cublasComputeType = (DataType::TYPE_FP32 == computeType) ? CUBLAS_COMPUTE_32F_FAST_TF32 : CUBLAS_COMPUTE_16F;
+    cublas_wrapper.setGemmConfig(cuda_dtype, cuda_dtype, cuda_dtype, cublasComputeType);
+    //before change
+    // cublas_wrapper.setGemmConfig(cuda_dtype, cuda_dtype, cuda_dtype, cuda_ctype);
 
     std::shared_ptr<Gemm> gemm = createGemm(&allocator, stream, false, false);
     gemm->setTypes(a_tensor.type, b_tensor.type, c_tensor.type, computeType);
@@ -506,8 +510,12 @@ void testGemmConsistencyBatchedMatmul(size_t m, size_t n, size_t k) {
                                    &allocator);
 
     cudaDataType_t dtype = std::is_same<float, T>::value ? CUDA_R_32F : CUDA_R_16F;
-    cudaDataType_t ctype = (computeType == DataType::TYPE_FP32) ? CUDA_R_32F : CUDA_R_16F;
+    // cudaDataType_t ctype = (computeType == DataType::TYPE_FP32) ? CUDA_R_32F : CUDA_R_16F;
+    // add culab type
+    cublasComputeType_t ctype = (computeType == DataType::TYPE_FP32) ? CUBLAS_COMPUTE_32F_FAST_TF32 : CUBLAS_COMPUTE_16F;
     cublas_wrapper.setGemmConfig(dtype, dtype, dtype, ctype);
+    //before change
+    // cublas_wrapper.setGemmConfig(dtype, dtype, dtype, ctype);
 
     std::shared_ptr<Gemm> gemm = createGemm(&allocator, stream, false, false);
     gemm->setTypes(a_type, b_type, c_type, computeType);
@@ -606,8 +614,12 @@ void testGemmConsistencyStridedBatchedMatmul(size_t batch_size, size_t m, size_t
                                    &allocator);
 
     cudaDataType_t dtype = std::is_same<float, T>::value ? CUDA_R_32F : CUDA_R_16F;
-    cudaDataType_t ctype = (computeType == DataType::TYPE_FP32) ? CUDA_R_32F : CUDA_R_16F;
+      // add culab type
+    cublasComputeType_t ctype = (computeType == DataType::TYPE_FP32) ? CUBLAS_COMPUTE_32F_FAST_TF32 : CUBLAS_COMPUTE_16F;
     cublas_wrapper.setGemmConfig(dtype, dtype, dtype, ctype);
+    //before change
+    // cudaDataType_t ctype = (computeType == DataType::TYPE_FP32) ? CUDA_R_32F : CUDA_R_16F;
+    // cublas_wrapper.setGemmConfig(dtype, dtype, dtype, ctype);
 
     std::shared_ptr<Gemm> gemm = createGemm(&allocator, stream, false, false);
     gemm->setTypes(a_tensor.type, b_tensor.type, c_tensor.type, computeType);
@@ -647,7 +659,7 @@ void testGemmConsistencyStridedBatchedMatmul(size_t batch_size, size_t m, size_t
                                           ldc,
                                           stridec,
                                           batch_size,
-                                          getCublasDataType(computeType));
+                                          getCublasComputeType(computeType));
 
         c_tensor.setInvalidValues();  // to guarantee C has invalid data
         gemm->stridedBatchedGemm(op_pair.transa, op_pair.transb, m, n, k,
